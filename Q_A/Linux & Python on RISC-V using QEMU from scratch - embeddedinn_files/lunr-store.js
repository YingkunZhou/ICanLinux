var store = [{
        "title": "PIC microcontrollers",
        "excerpt":"  NOTE: The IDE used in this article has been phased out by Microchip. However, basic techniques and architecture described herein holds good.   The first instance of this article originated at my google site page. It was further migrated to wordpress and then this page.   INTRODUCTION TO MICRO-CONTROLLERS          Table of contents     INTRODUCTION TO MICRO-CONTROLLERS   Getting Started   Architecture – A general Introduction   PIC Architecture   PIC memory map   Execution Cycle   Ports   Instruction Set   MPLAB IDE   Hello World           Problem Statement       Code       Compilation (make)       Simulation           Simulator Tweaks           Animation       Simulator Logic Analyzer                   Problem Statement           Code           Result                       Breakpoints and the Run button…                   Problem statement           Code                                A micro-controller, in simple words, is a miniature computer with a central processing unit and some peripherals integrated into a single integrated circuit package.   The central processing unit can can execute some instructions resulting in some outcomes. This instructions define the architecture of the controllers central processor in a macro scale.This gives rise to the a major classifications in processor architecture as      Reduced Instruction Set Computer (RISC)   Complex Instruction Set Computer (CISC)   To learn about controllers, processors and architectures in a general and abstract manner is tedious, time consuming and at-times dry. So here we are considering a simple microcontroller – the PIC 16F877a as an example to begin with.   PIC 16F877a is a mid range microcontroller from microchip inc. It is a CMOS FLASH-based 8-bit microcontroller with a RISC architecture that can handle 35 instructions.   When studying any electronic device or part, the bible is its datasheet. The data sheet describes in detail the architecture, capabilities and requirements of the part. PIC16F877a ‘s datasheet can be found here.   Download it and keep it for further reference throughout the tutorial. A printout of section 15 of the datasheet (only 6 pages) will be a great help during the programming exercises.   Getting Started   As said in the introduction, PIC micro controller, like any other micro controller executes the instructions one at a time in a sequential order as stored in its program memory and it is the skill of the developer to use these instructions (35 in this case) to create magics (like an intelligent robot). The programs written using these basic instructions are called assembly language programs and is the most primitive (not exactly, but close [:D]) and optimized form of programming.   An assembly language program will look something like the snippet given below   \tmovlw 0xfa movwf 0x20 movlw 0xdf addwf ox20,1   The above snippet is to add two numbers and does what the algorithm below does.   \tx=250; x=x+223;   The second form is easier to understand and manipulate from a programmers point of view. But to learn the architecture and functionality of the micro controller, we have to deal with the assembly language programming. Since it gives a clear cut idea as to what is happening inside the device – i.e. the data flow within the device and which internal modules are involved, we can also optimize our code for best performance.   Now, to get started with, we need two things.   A software that can simulate the internal working of the PIC micro controller and the datasheet of the device.   MPLAB IDE from microchip to simulate the device. MPLAB Integrated Development Environment (IDE) is a free, integrated toolset for the development of embedded applications employing Microchip’s PIC® and dsPIC® microcontrollers. The latest version can be downloaded for free here.   We will be using this software to simulate instruction flow within the PIC microcontroller and there by understand its architecture.   The data sheet is the document in which the device vendor release with the product. It will have all the device details and specifications for end users. Once we are familiar with the basic concepts of microcontrollers, we can explore the data sheet on our own and discover newer tricks. The datasheet of PIC 16f877a can be downloaded from here.   Architecture – A general Introduction                                               General Processor Architecture         Shown above is a simplified processor architecture. ( A more proper ‘Processor Architecture’ is to indicate the memory connectors as buses since most processors maintain an external program and data memory, while controllers have them in-built along with other peripherals. But, I want to keep the picture simple so that explanation will be easier :smile: ).                                           The firmware (program) resides in the program memory. Once the processor is reset and ready to go, the program counter, which is simply a counter that acts as a pointer to the program instructions points to the initial location of the program memory.   The execution unit fetches the program instruction in this first location. This will be one of the 35 instructions that the PIC can handle in our case. These instructions are stored in the program memory in an encoded fashion. It will be a binary number that has encoded information relevant to the instruction.   For example, the instruction movlw 0xff will be encoded as 11 0000 11111111 when stored in the PIC 16f877a program memory. This 14 bit encoded binary contains the instruction, the scratch pad memory location to be used and the literal value 0xff. A complete list of instructions and their encoding is given in page 160 of the datasheet.   The task of the execution unit, in simple words, is to fetch the instructions pointed to by the program counter (PC), understand it (Decode) and execute it.   Execution of a command can include a wide variety of tasks like moving some data from one RAM location to another, or storing it in a non-volatile  EEPROM location, or communicating with an external device like a PC. These tasks vary from micro controller to micro controller. A user side view of these tasks can be obtained by analyzing the instruction set of the specific device we are planning to use.   RAM is the volatile memory integrated within the controller package. It provides working space for the data manipulation during the command execution. The amount of RAM available is an important metric as the speed of operation and instruction set for a micro controller.   Scratch pad memory registers are high speed memory registers which are integral to the processing center architecture. The concept is from processor architecture, since the external memory access which will be much slower can be a bottleneck to the high speed operations within the processor. Microcontrollers usually have one or two such registers only.   PIC Architecture                                               PIC16f877a Architecture         Based on the memory organization, processor architectures can be divided into two as      Von Neumann Architecture and   Havard Architecture   Our point of interest here is that the Von Neumann architecture has a common bus for program memory and data memory (RAM), where as the Havard architecture maintains separate buses.   PIC 16F877a has the havard architecture, as it can be noticed from the architecture diagram above.   We will analyze the architecture in light of the general introduction in the previous section. The blocks are identified below.      Section marked 1 (blue) is the program memory.   Section marked 2 (green) is the Data Memory (RAM).   Section marked 3 (red) is the Execution Unit.   Section marked 4 (yellow) is the ALU.   The instructions are encoded and stored in the non-volatile Flash Program memory. Upon reset, the program counter points to memory location 0x00. This point is the reset vector and contains the first instruction of the steps that are to be done once the controller is reset. Details of the actual reset mechanism and other details will be dealt with later on.   Under regular circumstances, the program counter increments by one every execution cycle (explained later, as of now, consider it as each clock). This new location is used by the execution unit to fetch the next instruction.   When the execution unit receive jump or loop instructions, it stores the current program counter value to the stack and loads the new program location to go to into the PC. Thus these instructions take two execution cycles to complete. (A complete listing of the execution times can be found in page 160 of the data sheet).   The stack has 8 levels. i.e the PIC can perform upto 8 jump instructions after which it can return to the original location without errors in execution.   A detailed explanation if the instruction set will follow later on in the tutorial, but for easy understanding of many of the concepts, it is advised to thoroughly go through the instruction set summary given in section 15 of the datasheet (6pages) before proceeding further.   Let us consider an example to make this clear.   00    movlw 0xf0              ; moving value 0xf0 to location 0x22 through w register 01    movwf 0x22 02    call rtn1               ; a jump instruction to label rtn1, pushes 03 (next PC)                                            ; to stack and loads 05 (rtn1) to PC 03    movfw 0x24              ;moving value in location 0x24 to location 0x23 through                                            ;w register 04    movwf 0x23 05    rtn1:   movlw 0x8f      ;moving value 0x8f to location 0x24 in data 06                 movwf 0x24             ; memory 07                 call rtn2  ;pushes 08 to stack and loads 11 to PC. 08                 movlw 0xa0 ;moving value 0xa0 to location 0x24 09                 addwf 0x24 ;adding the value 0xa0 to content of                                            ;location 0x24 10    return                  ;pops 03 from stack and loads to PC 11    rtn2:     movfw 0x24    ;moving content of location 0x24 to location 0x25                                            ;through w register 12                  movwf 0x25 13                  return    ; pops 08 from stack and loads it to to PC                                         In the above code snippet, the PC increments by one  until the execution unit receives the call instruction at 02. Once it receives the call, the immediate next location address (03 here) is ‘pushed’ (stored at the top most level) into the stack and the destination location (05) is decoded form the instruction and is loaded into the PC. Thus , at the next execution cycle, the instruction fetched is the movlw at 05. Then the regular operation take place with the PC increment from 05.   At 07, another call instruction is encountered. Again, the immediate next PC (08) is pushed into the stack. this makes the previously stored 03 to go to the second level of the stack and 08 recedes in the first level.   At 13, a return instruction is received  this causes the first level of stack (storing 08) to ‘pop’ the topmost level into the PC. So, now the PC points to 08. Normal execution continues till 10 where the return instruction pops 03 from the stack.   Here we utilized two levels of the stack. If there are more than 8 consicutive ‘call’s without return, the first pushed data will be over written. This will corrupt the firmware. In those cases, we have to develop routines for stacking.   PIC memory map   Operations resulting from the execution of instructions can all be considered as manipulation of data in different parts of the micro controller. It may be data in the RAM or in other special registers with designated purpose. These special registers (Special Function Registers (SFRs)) also reside within the RAM register block, but cannot all be manipulated like ordinary memory registers, without ‘side effects’.   The RAM block (512 bytes in size) is not one single continuous block of memory, but is rather divided into four banks of 128 bytes each. Of this, 368 bytes are General Purpose Registers (GPRs) and the remaining 56 are SFRs. The distribution of this GPRs and SFRs in the RAM register block can be seen in page 17 of the datasheet. The location of each of the SFRs and the range of the GPSs in each bank is clearly shown there.   Data cannot directly be written into or transferred from one RAM register to another using any of the 35 available instructions. rather, it has to be transferred via the scratchpad registers. Before the transfer the appropriate bank is also to be selected.   Bank selection is done by writing the appropriate values into the bank selection bits of the STATUS register. ‘STATUS’ is an 8bit SFR available in all the four banks. The 6th and the 5th bits of the STATUS register are named RP0 and RP1 and is responsible for selecting the current bank. The table of selection values is given below.                                         The working register, commonly referred to as the W reg, is the only scratchpad memory register accessible to the user in the PIC 16F877a. It resides outside the RAM register block. Every data transfer in or out of the RAM register has to go through the W reg.   For example, if we want to store the value 0xff (Hexadecimal of 127) into the GPR located at 0x20, we have to first transfer the value 0xff (literal) to the W reg, and then transfer the value to the register location (refered to as ‘file location’) 0x20 in another instruction. The code for the same will look like this.   bcf STATUS,RP0  ; Clear(set to 0) bit RP0 of the file register                                 ;STATUS for bank selection. bcf STATUS,RP1  ; Clear(set to 0) bit RP1 of the file register                                 ;STATUS for bank selection. movlw 0xff      ; move literal value 0xff to w movwf 0x20      ; move value in w to file location                 ;(register) 0x20   Once this is done, the value remains in the location until either it is changed by another instruction or the micro controller is reset or powered off.   Execution Cycle   A microcontroller is a synchronous digital device.i.e. it works based on the timing pulse recived from the systems clock circuit. The PIC 16f877a can generate its own clock from a piezo crystal connected to its specific pins.(This is the most popular method of clock generation for its accuracy. Other methods are also available, which will be discussed later).This crystal can be up to a maximum speed of 20Mhz. But, this doesn’t mean that the PIC can execute instruction at a maximum speed of 20,000,000 instructions per second (50ns per instruction).   The clocking signal derived from the crystal is internally divided by four. This is to provide synchronization timing and clock signals to all parts of the micro controller. However, the division of master clock is primarily to establish an instruction pipeline. Thus, if we generate a 20Mhz master clock, the execution speed will be a maximum of 5Mhz. The single cycle instructions execute at this speed.   We can get a better idea of pipelining by considering the famous laundry example.                                         Consider a laundry with one washing and one drying machine. If operations are carried out one after another, the entire task (to complete two sets of laundry) takes 2 hours. This is like fetching, decoding and executing instructions only once the previous instruction is completely finished.   But, while the first set is being dried, if the second set is put to wash, the operations are carried out parallel, thereby saving net time. This is the case of instruction execution with pipelining. When one instruction is being executed, the next instruction is fetched and decoded, making it ready for execution. This is illustrated below                                         Ports   A port is the microcontrllers’ interface into the real world. All the data manipulation and operations that are done within the microcontroller ultimately manifests as output signals through the ports.   To make the concept clear, let us consider an air conditioning system built around a microcontroller. The temperature sensors measure the room temperature and gives it as input to the microcontroller through the ports. The data coming in through the ports will be stored in some GPR by the microcontroler. The data in this GPR will be compared against a set temperature. If the external temperature reported by the sensor is higher that the threshold, the microcontroller switches on the air conditioning mechanism. This is done by switching on the corresponding port pin.   Physically, ports are some of the pins that are seen in the IC package. There are 6 ports for PIC 16f877a. They are named as PORTA, PORTB, PORTC, PORTD and PORTE. Ports B, C and D are 8 bit wide (8 pins each), while PORTA is 5bitand PORTE is 3 bit wide. The pin allocation of the ports are given in the IC pin diagram in page 3 of the data sheet and is reproduced below. The individual port pins are named 0 throug n. for eg 1st pin of PORTA will be RA0.                                         As it can be seen from the pin diagram, the port pins are bi-directional and and most of them are multiplexed in function. i.e the pins act as regular general purpose I/O as required for the air conditioning example, or as the I/O s of some of the internal modules of the microcontroller. For example, port pins RC7 and RC6 (pin number 25 and 26) are regular I/Os as well as the interface to the UART module that handles the RS-232 protocol, which is commonly used to interface the PIC to a regular computer.   The RS-232 based UART module requires only two data lines to effectively transmit and recieve data from a regular computer to the PIC or even a printer or PDA with a serial port. This module is integrated into the PIC package and can be configured using firmware instructions. Exact way of doing this will be discussed later.   Each port has a corresponding SFR in the RAM register block. Therefore , when we are referring to switching a port pin on as in tha air conditioner, it is actually writing data into the corresponding port register. Similarly, receiving data from the registers is actually, reading the data stored in the corresponding data register.   Along with the data holding port registers, there is a set of configuration registers associated with the ports. These are the TRIS registers that configure the ports to be in input or output mode. These also reside in the RAM register banks as SFRs. Writing a 1 into the corresponding TRIS bit configure the port pin as an input pin, and the data comming in throught the port pin will be latched into the corresponding PORT bit in the immediatly next execution cycle.   The code snippet below is to read a byte from PORTB and write it to file location 0x120. Note that the TRIS registers are in bank1 where as the PORT registers are in bank 0 and file register 0x120 is in bank 2. This bank selection concept is to be kept in mind whenever we are dealing with RAM registers of the PIC. The list bank location listing is in page 17 of the data sheet.   bsf STATUS,RP0    ;Selecting BANK 1 for TRISB register bcf STATUS,RP1 movlw 0xff        ;Moving the 1s (since PORTB is to be set                    ;as input port) to be stored at TRISB to w reg movwf TRISB       ;moving the 1s to TRISB bcf STATUS,RP0    ; Selecting Bank 0 for PORTB movfw PORTB       ;Moving Input at PORTB to w reg to later move to 0x120 bsf STATUS,RP1    ; selecting BANK 2 for 0x120 movwf 0x120       ; moving valiw in w reg from PORTB to 0x120   Note that the port pins can also be individually configured, i.e. any combination of input and output configuration is possible in any of the ports.   Instruction Set   The key architectural concepts of the PIC 16f877a microcontroller has been discussed. Now, we are going into the actual program writing process. Some of the instructions like MOVWF and MOVLW should already be familiar to you since it has been used over and over again in most of the examples already discussed. I hope you have also gone through the instruction set summary (Section 15 of datasheet) as instructed. If not , this is the point of time to do so. Go through the explanation of each and every instruction.   An instruction set is the entire set of commands that a microcontroller/processor can execute. Any task to be accomplished with the device is to be split up and written in terms of the defined instruction set. For example, if multiplication of two numbers is to be performed in PIC 16F877a , there is no direct instruction to do it. This is because the ALU of the microcontroler has no provision to do it. Therefore , if we have to do the multiplication, we have to write a routine out of the available looping, counting and addition instructions that are available.                                         Writing routines in the native instruction set is called assembly language programming. This method will give the most optimized firmware, but required a great deal of skill and is hard to debug, especially as the system size grows and advanced concepts like multi-tasking, resource sharing etc comes into picture. However, assembly language programming is the most effecient way of learning the architecture of the system and is also fun.   The programs written in the assembly language is further to be converted into binary encoded format so as to use it in the microcontroller. This is dome by the assembler software. Most of the device vendors provide a free asembler for their parts. So is the case with microchip. The MPASM software that converts assembly language programs into binary .hex files usable by the controller.   A simulator is a software that simulates the working of the device in detail. Using a simulator, we can simulate the loading of a program, execute it step by step, analyze the effect of each instructions in different registers of the controller and thereby debug and fine-tune the firmware. MPLAB IDE is the integrated development environment provided by microchip, which can assemble our firmware and also simulate it. It is available for download, free of cost from here.   detailed explanation of how the assembler works can be found here.   We will be using the MPLAB IDE to explore the instruction set, and observe the results.   Since a very detailed explanation about the instructions is available in the datasheet, we will be focusing more on how to get things done  using the 35 instructions. This will be mainly my taking  simple examples and running it in the simulator in the initial stage and later on using the hardware itself.   MPLAB IDE   Before beginning the actual coding and analysis, let us familiarize with the simulator – MPLAB IDE.   We will be dealing each example as a different ‘Project’. It is the IDE’s way of handling all the associated files like the assembly file, the hex file etc as a single virtual entity. It is also easier for us to keep track of our files this way.   Once the IDE is initialized (opened), to begin a project, go to project - ‘project wizard’ from the task bar.                                         Click Next…                                         Select the device (PIC16F877A) from the drop down list and click next.                                         The IDE is an environment which integrates different simulation tools and compilers to provide a single window solution to development and debugging.   Here, we have to select the toolsuite (Microchip MPASM Toolsuite) which include the assembler, the linker and the libraries. the tool suite components resides in the ‘MPASM Suite’ folder within the installation folder. In case any of the components are not correctly pointed correctly, a red cross mark appears near the component. browse to the location the remove the indication before continuing.                                         Now, browse to the location where the project files are to be stored and give your project a name. Note here that if the full project location name length is greater than 62 characters, then the assembler will show error during the linking process. Keep that in mind while choosing the project name and location.                                         If the assembly or some other relevant files like libraries that are to be added to the project already exist, then add it here. 1st timers can skip this step…                                         Now we have successfully created our project and a summary will be generated.   Now create a new editor file by going to file - new (ctrl+n). This is where we will be entering our code. To activate the colour coding that will highlight keywords, type in something and save the file with the .asm extension to the project folder.                                         Now we have successfully created our project and a summary will be generated.   Now create a new editor file by going to file - new (ctrl+n). This is where we will be entering our code. To activate the colour coding that will highlight keywords, type in something and save the file with the .asm extension to the project folder.   Once this is done, we have to add the file to the project. For this first activate the project view by clicking view - project. This will add the following screen to the workspace. Right click on the ‘Source files’ and click ‘Add Files…’   Browse to the assembly file we saved before and click add.   Next step is to define the simulator that we will be using. for this go to Debugger - Select tools&gt; and select MPLAB SIM   Now we are all set to go coding. We will start with some examples and later go into building intelligent machines :smile: .   Hello World   This is our first program that we will be simulating using MPLAB IDE. This will be to add two numbers and store the value in a file register. To keep things simple, there will be no user input, rather the literals to be added will be stored within the micro-controller and so will be the result.   Problem Statement   Two values stored in file register locations 0x20 and 0x21 are to be added and the result stored in register 0x23.   Code   list p=16f877a                        ; compiler directive to set the device used #include &lt;p16f877a.inc&gt; ; Compiler directive to include library  org 0x00      ; Initial location of program memory  ;Setting the initial condition bcf STATUS,RP0 ;setting bank 0 bcf STATUS,RP1 movlw 0xf0     ; literal 1 movwf 0x20    ; stores in 0x20 movlw 0x0f     ; literal 2 movwf 0x21    ; stores in 0x21  ;actual operation  movf 0x20        ; moves value in 0x20 to w reg addwf 0x21,0   ;adds the values and stores result in w reg movwf 0x23     ; stores result in 0x23 end   Now, we are ready to start the simulation and watch the internal operations of the microcontroller.   Note: The device library p16f877a.inc contains the SFR name to address mapping. It is included so that we can use the register names like STATUS and RP0 instead of having to give in their address each time. Writing code as bcf STATUS,RP0 is easier than writing  bcf 0x03,5 since we’ll have to remember the addresses of all the SFRs and debugging will be tough.   Compilation (make)   Once the code is entered, it needs to be compiled . Compilation invokes the assembler and generates the hex file. It also checks for syntax other possible errors during the process.   To compile the code click the ‘make’ button in the tool bar.                                               Workspace         Simulation   Before actually beginning the simulation, there are a couple of settings to be made.      Switch off the device watchdog timer by going to configure - configuration bits  (The reason will be explained later on)   Enable the Watch window and program memory window by going to view                                         To switch off the watchdog timer, uncheck the configuration bits set in code check box indicated in the figure and select the option from the drop down menu.   In the watch window, add the relevant register names by double clicking under the corresponding tab (Adddress/ Symbol name) and typing it in or in the case of SFRs, selecting it from the ‘AddSFR’ drop down.   The Program memory window gives us an idea as to where our program is residing in the program memory. It will be interesting to note this change in location by Changing the ‘org 0x00’ line to some other locations like say, ‘org 0x20’. But corresponding lines will have to be added at the reset vector (0x00) for successful execution. the next example will demonstrate that.                                         Now we are ready to run our simulation.  The required  buttons are shown in the workspace figure.   Click on the ‘step into’ button to execute each instruction sequentially. The green arrow shows the current execution that has been decoded and ready for execution. The corresponding trace of the program memory location can also be seen in the program memory window. The resulting change of values in the registers after each operation can be noticed in the watch window. Once the execution reaches the end statement, the program fetch and decode continues and can be noticed in the PM window, but the instructions are all NOPs. however, leaving the controller to blindly execute instructions in unhealthy practice, and the control should be rightly directed using CALL or GOTO operations.                                         The reset button can be used to regain the control to the reset vector .   Note: Each click to the ‘step into’ button can be considered to be generating a pulse of the execution clock.   Simulator Tweaks   After the first program, we are now all set to try out more interesting stuff. I’ve introduced the basic simulation technique in the previous ‘Hello World’ page. Let us explore some more cool features of the simulator, that will allow us to further explore the architecture and functionality of the device.   These sections introduce the simulator as well as the coding styles. So pay attention to the codes and comments…   Animation   In the previous example, we ran the simulation by manually clicking the ‘Step Into’ button to execute each instruction. This is like manually providing the execution cycle clock for each instruction. In realtime, the execution clock is automatically generated from the crystal and is continuous. This can be simulated using the animate button.   Before jumping into the implementation, a couple of settings are to be made so that we can observe the animation sequence.   Go to debugger&gt;settings .   In the Osc/Trace tab set processor frequency as 4Mhz. (We are using it so that it’ll comply with the hardware experiments later)   In the Animation/Realtime updates tab set, set animate step time as 500ms and check the ‘enable realtime watch updates’ check box and set the value to 5 (x100) ms.   The later step is to keep the animation steps visible. otherwise, the animation will be too fast for us to see. (The snapshot in the ‘Hello World’ page is running with these settings. (see it here)). The animate step time of 500ms is the execution frequency for the simulation (only!).   Simulator Logic Analyzer   The simulator logic analyzer tool is a very handy tool that gives the feel of the familiar oscilloscope interface. The following example of generating a square wave in RB0 shows a rookie level example of how cool this tool is.   The tool is available at View&gt;Simulator Logic Analyzer   Problem Statement   Generate a square wave of 50% duty cycle at RB0. and verify the output using the Simulator Logic Analyzer.   Code   list p = 16f 877a ; compiler directive to set the device used #include &lt;p16f 877a.inc&gt; ; Compiler directive to include library org 0x 00 ; Initial location of program memory goto start ; instruction at PM location 0x 00  org 0x20 start ; label marking PM location 0x 20   banksel TRISB;a compiler directive that eases                 ;the bank selection job   movlw 0x00   movwf TRISB ;setting PORTB as output port   banksel PORTB  loop ;infinite loop generating the SQW   bcfPORTB,0   call delay ;used to set the time period   bsf PORTB,0   call delay   goto loop  delay  ;This code segment counts down from 255 to 0        ;each count takes up some time thus ;generating a delay loop   banksel 0x20;bank selection   movlw 0xff ; initial value   movwf 0x20 ;count register  decloop ; The decrement loop   decfsz 0x20 ;decrement 0x20, skip next instruction is value in ;0x20 is 0   goto decloop ; executes if value in 0x20 is not 0   return   ; executes when value in 0x20 is 0. end   Notes:                                             Note here that at reset, the control jumps to 0x20 since it is the instruction at the reset vector   When Delay is called, the next PM value (0x0028) is pushed into the stack and the PC is loaded with 0x2D   When return is called, the Stack is popped. This can be observed in the hardware stack window.   The Delay: Delay routines are generated usually using timers inside the controller. here, we are using a decremental counter. The decrement operation takes one execution cycle per operation. i.e. once the control enters the routine , it takes 256 execution times to pop the stack. When converted to time, it will be a delay of (256x1us) assuming a oscillator frequency of 4Mhz.   Result                                         The animation result as seen in the logic analyzer is shown in fast forward here. Without the tool, we will have to visualize the square wave from the alternating ones and zeros in the watch window. This is also shown in the image.   The binary view can be availed by right-clicking within the watch window and selecting the corresponding option.   Breakpoints and the Run button…   When we where dealing with the delay routine, you would have noticed that the execution time of the instructions in the animation mode is equal to the animate step time we have set in the debugger options and not the actual time the microcontroller takes in the field. This is rather annoying when we have to run the entire sequence (code) for a while to see say, 10 square waves in the logic analyzer, since the operations are in “slow motion”. Also, the time period of the square wave is not what the original hardware would produce.                                         To solve this problem, we use the concept of breakpoints and the ‘Run’ button.   Problem statement   Generate a 10 second (real time) delay using delay loops.   Code   list p=16f877a ; compiler directive to set the device used #include &lt;p16f877a.inc&gt; ; Compiler directive to include library  org 0x 00 ; Initial location of program memory goto start ; instruction at PM location 0x 00 org 0x20 start   calldelay ; Calling the delay loop   nop ; Breakpoint is set here    delay ;This is a 3 level   cascadeof our previous delay loop   movlw 0x 35 ;value 1   movwf 0x 22 ;Total = 53 x 256 x 256 x 256   loop6   decfsz 0x22   gotoloop1   gotoend1 loop1   movlw 0xff ;Value 2   movwf 0x21   loop4   decfsz 0x21   gotoloop2   gotoloop6 loop2   movlw 0xff ;Value 3   movwf 0x20   loop3   decfsz 0x20 ;Innermost loop   gotoloop3   gotoloop4 end1 return end   The functionality is somewhat self explanatory 3 level cascade of the previous delay loop. It can also be observed using the simulator for clarification.   Our primary aim is to measure the amount of delay this loop can give us.   The Run button executes the command eliminating the visual effects we get from the animation. But we need to have a control as to in which step of the execution, we need to pause to read out the register values from the watch window. For this, we use the concept of breakpoints.                                         To set a breakpoint, double click in the grey area near the line we need to breakpoint. The breakpoint symbol will appear. The execution pauses when it reaches this step. We can use this pause to make required readings and continue with the simulation.   Now, to simulate real-time execution data, we have to use the ‘Stopwatch’ feature in the Debugger&gt; menu. The readings it shows is dependent on the processor frequency that has been set in the Debuggre&gt;settings&gt;Osc/Trace tab. This is the crystal frequency we anticipate to implement in hardware.   Now, let us find out how much delay our routine can give us…   Set the breakpoint near the nop in line 10, and hit the run button.   The entire routine will be executed and the execution halts at the breakpoint. the exact time taken to execute the routine viz our delay will be shown in the stopwatch…                                         To change the delay value, experiment changing the different seed values indicated in the comments.  ","categories": ["Articles","Tutorial"],
        "tags": ["PIC microcontrollers","MPLAB IDE"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/pic-microcontrollers/",
        "teaser": null
      },{
        "title": "USB-2.0",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   Universal Serial Bus is a host-centric, 4 wire bus protocol targeting a standard, low cost interface with self-identifying, dynamically attaching peripherals that automatically map functions to drivers and configurations and low protocol overhead and guaranteed bandwidth and low latencies for demanding applications like telephony, audio, video etc.   The USB specification was jointly developed by a conglomerate of which the major players included HP, Intel, LSI, Microsoft, Renesas and ST-Erricson. The specification and certifications are maintained by the USB implementer’s Forum (USBIF)          Table of contents     History of revisions   The Basics   Bus Topology   System Components   The USB interconnect and Signalling           Interconnect       Signalling           Protocol Basics           Basic Terminology           USB Basic Communication Flow   Enumeration   Transfer Modes           Control Transfer       Isochronous Transfer                   Bus Bandwidth                       Interrupt Transfer       Bulk Transfers           USB Reliability           Errors                   Error types                       Error handling       Data toggling for synchronization           USB Classes   Host Scheduling        History of revisions      Revision 1.0 (January 15, 1996)      Low-speed transfer rate of 1.5 Mbits/s   Full-speed transfer rate of 12 Mbits/s.   Revision 1.1 (September 23, 1998)      Improved specification and was the first widely used version of USB.   Revision 2.0 (April 27, 2000)      High-speed transfer rate of 480 Mbits/s.   Revision 3.0 (November 17, 2008)      Super Speed USB (Raw data throughput up to 5.0 Gbit/s)   Through this article we will be concentrating on the high speed USB protocol revision , popularly called USB 2.0.   The Basics   Bus Topology   Basic USB follows a tier Star Topology as shown in the below figure.      The Host is at the top of the tier and all “functions” connect to the host ultimately. A “hub” is required to expand the connectivity of the “root-hub” which is the ultimate terminating point in the host. A hub is the center of each tier and each tier has a star topology centered at the hub.      There are limitations set to the maximum delay that can be induced by each element of the USB protocol. As a result, the net “turnaround time” restricts the number of tiers to be 7 – including the top (root/host). A function which contains a hub and a function together is called a “composite device”. The 7th tier is not supposed to contain a hub since no further levels are allowed.   System Components   The USB system is defined in terms of 3 major areas. This includes the USB device, the USB host and the USB interconnect.   A USB device can either be a hub which provide additional attachment points to the USB or functions which provide additional capabilities to the system.   There is only one host in any USB system . The USB interface to the host computer system is called the host controller. The host controller may be implemented in hardware, firmware or software. A root hub is integrated within the host system to provide one or more attachment points.   The USB interconnect is the physical interface of the bus and is described by the electrical and mechanical specification of the bus.   The USB interconnect and Signalling   Interconnect   The USB achieves specified protocol speeds over copper using a 4 wire physical interface. The bus also supplies power to the devices .      Maximum length of the cable is defined in terms of the signal latency and signal integrity . A standardcable conforms to the color coding as shown in the figure      Signalling   USB protocol follows differential signalling in the D+ and D- lines of the cable. Data is encoded using NRZI protocol and is represented in terms of J (where current is driven into the D+ line) and K (where current is driven into the D- line)   In NRZI encoding, a stream of 0s is represented by J-K toggle every bit time whereas a stream of 1s maintains the previous state of the D+ and D- lines. A single ended zero (SE0) is signalled by driving both D+ and D- lines      Synchronization between the host and the device is maintained by a digital phase locked loop (DPLL) at both ends. However, a long stream on 1s will cause the DPLL to lose sync and hence, a zero is “stuffed” after every set of 6 consecutive 1s. This is called bit stuffing.   “Bit times” are determined by the speed mode in which the function is operating. For example, a device operating at low speed has a data rate of 1.5Mbps. Thus one bit time will be (1/1.5 us).   Speed modes are differentiated by the terminating resistors in the D+ and D- lines.   For a Low speed the D- line will terminate in 1.5K ohm pull-up resistor whereas a full/high speed device will have a similar pull-up in the D+ line. Thus, initially a high speed device acts as a full speed device . The device generates a “high speed chirp” which is a set of 15 J-K pairs to indicate to the host that it is a high speed device. (480Mb/s)   Once the speed mode is detected, the host begins the “enumeration” process, which is nothing but configuring the function for use. But, before going to the details of enumeration, we need to understand certain basic terminologies. This is explained in the next section.   Protocol Basics   Basic Terminology      Signalling mode     One of the speed  modes            Low Speed  : 1.5Mb/s       Full speed   :  12Mb/s       High speed  :  480Mb/s                Host      The computer system (which can also be an embedded system) to which the usb device is connected. The host initiates all communication and is the master of the bus. There will be only one USB host in a bus topology.            Device    The usb function which is connected to the host. A device may contain multiple functionalities which may or may not be available together. This is discussed in detail in later sections.            Device Address    A unique 7 bit address assigned to the device by the host when it is attached. This address is used by the host to direct all communication to the desired device. A device when connected initially to the host, will have address 0 (default address).            End Point     An endpoint is the most granular classification in a device to which the host can communicate. An endpoint is usually implemented as a buffer to which host sends data (protocol specific or not).       Every USB device should have the default Endpoint 0 (EP0) which is used by the host to identify the capabilities of the device and send configuration information to.       There can be a maximum of 31 endpoints including EP0. All endpoints except EP0 are unidirectional and each endpoint is identified with a unique 4 bit endpoint number and its direction.Endpoint direction is denoted in the lower nibble of the 8 bit endpoint address .Thus EP1 IN will be addressed as 0x10 and EP1 out will be addressed as 0x18.            Pipe     A logical connection between a point in the host and a point in the device. A device can have maximum of 31 pipes including the default pipe terminating at EP0.       USB pipes are classified in two as unidirectional stream pipes which can carry data in any format and stream pipes which can carry data with no specified formats.       The default pipe terminating at EP0 is always a message pipe and the pipes to other endpoints are all stream pipes.                  Interface and configuration     Interface and configuration are two levels of hierarchical arrangement of the capabilities within a function. A function (device) may have multiple configurations and multiple interfaces associated to each configuration. Each Interface may have multiple endpoints associated with it. At any given point of time, only one configuration can be loaded by the host. The interfaces group the endpoints logically and makes it easier to load the device drivers.       The host reads out capabilities of the device in terms of the configurations and interfaces with the device descriptor, interface descriptor and endpoint descriptor. these are pieces of information passed on to the host when it issues certain standard commands to the device .       A specific interface/configuration can be loaded or unloaded by the host during device operation.                  Device Class     The USB devices are classified into different classes so as to make driver development and classification easier. However, there is no directive that a device must fall into a class specification. A certified device can still be using completely proprietary drivers . however, the device must confirm to basic protocol specified behavior as specified in Chapter 9 of the SUB 2.0 specification along with electrical and mechanical compatibility.            Frame     The entire communication happening in the USB bus , irrespective of the target device, happens in segments divided into frames. For a low/full speed device, a frame is 1ms wide where as a high speed device has a frame width of 125 us. Each frame is distinguished by a 11 bit frame number which is issued as a special packet called Start of Frame (SOF). Within the frame there can be multiple packets of data indented to be delivered to different devices. The actual packet delivery is explained in following sections.                     Stall     When an endpoint is not ready to send or receive data, the endpoint sends a NAK. Where as if the endpoint receives a wrong command or is no longer able to respond because of some internal error, then the device sends a STALL handshake. Upon receiving a STALL handshake, the host has to take corrective measure to restore the functionality in the endpoint.            Timeout     A high speed device connected to the last tier (7th tier) of the topology can take up to a maximum of 736 bit times to respond to any of the host requests. If no signal is seen within 816 bit times, the host will timeout. Following a timeout, the host will issue a reset and try to re-enumerate the device.       USB Basic Communication Flow      All data communication in the USB bus happens as packets. A communication is initiated by the host by issuing a token packet addressed to the device and endpoint to which the following packets are indented . The token packet contains an 8bit packet identification (PID) that defies the type of transaction that is about to happen      The PID is actually of 4 bits width. The lower nibble is compliment of the higher nibble to provide a sanity check.   Following a token packet is a corresponding data packet in the direction mentioned in the token packet. This is an optional stage for different types of tokens and are described in the corresponding places in this document.   Once the transaction happens, it is given a handshake packet.   A start of frame (SoF) packet is broadcasted by the host every millisecond (in case od LS of FS devices) or every 125us (for HS devices).   Thus every  transfer contain multiple transactions. Each transaction has three stages      A token stage: where the host issues a token packet addressed to the device to which further communication is going to happen. It also indicates the direction of transfer.   A Data Stage: where the actual data transfer happens through a data packet   A status stage: where the data transfer is acknowledged with a handshake.         Enumeration   The process of identification of the capabilities of a USB device and mapping it top the corresponding drivers in the host system is the process of “Enumeration” . Once a device is enumerated the Host will be aware of the speed mode, available number of endpoints and their properties and classes etc. The steps are as described below.   Here we are assuming just one device plugged directly into the root hub of the host.      As soon as the device is plugged into the hub port, there will be a bus reset with a SE0 (D+ and D- low) followed by power up of the bus.   This is followed by speed mode detection of the device using the  data line pull-up   A low speed device will have a 1.5 K Ohm resistor connected to its D- line where as a full/high speed device will have it connected to the D+ line.   A high speed device starts its operation as a full speed device. Once the host resets the bus , the device sends a low frequency chirp (16 J-K pairs) to indicate to the host that the deice is capable of high speed operation. This is acknowledged by the host with a reverse chirp.   Once the speed mode is detected, the host issues a “standard request” termed ‘GetDescriptor (device)’ to EP0 to read out the capabilities of the connected device.         Reading the descriptor happens in multiple levels since descriptors are differentiated hierarchically depending on the type of information they convey as explained in the previous section.   After reading the device descriptor for the first time, if the device can be supported by the host (decided by various factors), the bus is reset again and the device is provided a unique 7 bit address.   After getting the device to the addressed state, further details of the device are read out via other descriptors as shown in the following figure         The set configuration comand is used once all available configurations of the device is read out. The comand uses an index to set which configuration of the device is to selected.   Once a configuration is set, the corresponding class driver (more on classes later) will be loaded and class spec   Once the device is enumerated and the endpoints are configured, the logical communication arrangement is as shown in the figure below.      The system software is primarily the root hub driver that keeps track of all the devices connected to it. Any configuration related communication happens via the default pipe to EP0.   Drivers specific to the endpoints can communicate via the corresponding pipes.               Transfer Modes   A usb device can contain multiple endpoints as discussed earlier. Each endpoint can be an IN or OUT endpoint depending on the direction of data transfer, Another parallel classification is based on the type of data transfer happening in the endpoint.   The four types of transactions that happens in the USB bus are      Control Transfer   Bulk Transfer   Isochronous transfer   Interrupt Transfer   During enumeration , each endpoint that will be active for teh selected configuration will be classified to have  one of the four transfer types.   Control Transfer   Control transfer is used for communication between the host and EP0 function (device) . Examples include the chapter 9 command transactions like SetAddress, Getdescriptor, SetDescriptor etc.   A control transfer happens in three stages namely      setup stage   data stage   handshake stage         The setup stage starts with a setup token issued by the host followed by a data packet which contains the actual command information which is in turn followed by the acknowledgement stage.   The data stage is optional This is followed by an optional data stage depending on the type of command issued. for example, the set configuration command has the index of the configuration to be set embedded into the command. thus no separate data stage is required in its case whereas the Getdescriptor command requires multiple data stages to transfer all required information.   Even when multiple devices are connected to the host, control transfer is always assured 10% of the bus bandwidth. i.e 10% of the time in a time in a 1ms (20% of the 125us micro frame in case of HS devices) will be always reserved for control transfers.This is because if there are some data errors in the otehr endpoints, control transfers are use to perform corrective action on those functions.   Isochronous Transfer   This data transfer mode where where there is guaranteed access to the USB bus bandwidth.   Bus Bandwidth      The USB communication happens in the form of frames. Each frame can contain transfers indented for multiple devices. Bandwidth specifies the amount of time in a 1ms / 125us frame that is dedicated to a particular device. This can translate to  the amount of data when we consider the speed mode (1.5/12/480 Mb/s)       Interrupt Transfer   Interrupt transfer provides a mechanism to have an ensured communication between the device and the host in periodic intervals of time.   When an interrupt endpoint is enumerated , it can specify a polling interval ranging between once every frame to once every 255th frame.   In case if the device do not have data to transferred when polled, the next retry will only be in the next service frame.   Maximum data packet size is 8 bytes (LS), 64 bytes (FS) or 1024 bytes (HS) per transaction.      The figure below shows the endpoint descriptor for the interrupt endpoint of an optical mouse and periodic polling of the endpoint.      Note on Keep Alive:      All hub ports to which low-speed devices are connected must generate a low-speed keep-alive strobe, generated at the beginning of the frame, which consists of a valid low-speed EOP. The strobe must be generated at least once in each frame in which an SOF is received. This strobe is used to prevent low-speed devices from suspending if there is no other low-speed traffic on the bus.    Bulk Transfers   Bulk transfers are used when large amounts of non-time critical data is to be transferred between the endpoint and the host. Examples of devices utilizing bulk data transfer mechanism include flash drives (mass storage class devices), printers , scanners etc. The reliability of data content is more important than data rate for these devices. Thus, bandwidth available for bulk transfers can vary from frame to frame depending on the configurations of other devices connected to the host.      USB Reliability   Errors   There can be errors occurring in the USB bus communication due to various reasons. This section gives a brief overview of the common errors and the corresponding recovery mechanism implemented by the USB framework.   Error types           PID Errors     The 8 bit PID is actually constructed out of a 4 bit PID and a 4 bit PID compliment as shown in the figure below. If there is a mismatch , the device should not respond and a timeout will happen at the host side (ref: section 8.3.1of USB 2.0 spec).       Full decoding of the PID is mandatory frame all the devices .                  CRC Error     Fields following the PID are used to compute a cyclic redundant code which is used to validate the sanity of the received data. Any data with corrupt CRC will be responded with a NAK.       CRC is 5 bits for a token packet and  SOF whereas it is 16 bit for a data packet. There is no CRC for handshake packet.             Invalid command     Whenever an endpoint receive a “intact but logically wrong” packet like a wrong command, or if something goes wrong within the device which makes the endpoint un-usable, the endpoint stops functioning and any requests addressed to the endpoint is gives a STALL handshake as response   Error handling           NAK     Whenever the host receives a NAK from the device retries are triggered at the hardware level for the first 3 times. Beyond this, there will be software intervention.            STALL      When an endpoint STALLs, the host has to send a ClearFeature command with the ‘feature selector value’ addressing the interface to be cleared . This is sent to the control endpoint (EP0) . Thus even if an endpoint stops functioning, a ClearFeature is applied to all endpoints under that interface.            Timeout     When no signals from the device reaches the host for a specified interval of time , the host resets the device by applying a reset signal (driving SE0 for more than 2.5us on the data lines)          Data toggling for synchronization   USB employs a simple mechanism known as data toggling to ensure synchronization of data transferred between the host and the devices.   While doing data transfers, the host uses two types of data tokens namely Data0 and Data1. Initial data transfer will be done using the Data0 token. Upon getting an ACK from the device, the next piece of data is sent using Data1 token. The device also maintains a sequence bit which is toggled upon successfully ACKing the transfer.      However, in case there is some problem and the data is not accepted (NAK sent), then the host and device maintains the sequence bit status and the immediate next transfer happens with the previous data token type      Suppose an ACK is missed in the bus and not received at the host, then the host re-sends the data with same token (DATA0). however, at the device side, the sequence bit is already toggled as a result of the previous transaction. So the device ignores the received data and sends an ACK.         USB Classes   After enumeration , the actual communication to the device is initiated by a driver software within the host which knows the behavior of the device. USB protocol is very versatile to use existing drivers along with the new drivers so that USB can act solely as an interface or communication medium. This is implemented using a filter type driver hierarchy by dividing the USB devices into classes.   We will try to understand the concept of USB Classes by considering the well known example of a Mass Storage Device aka USB Flash drive.   A USB flash drive primarily had a mass storage memory and memory controller attached to a USB controller.      A typical USB flash drive will have a memory controller interface that follows the SCSI protocol which is yet another protocol like USB which is used to have entire control of the memory at the logical block address level. Thus, after enumeration the device (flash drive) has the role of a slave to respond to the SCSI commands that are issued by the host.   SCSI has 3 stages for every transaction namely      Command stage   Data Stage   Status Stage   The file table and all logical memory handling is done by the SCSI driver that resides in the host.   Now, the SCSI driver is a generic driver that has no relation what so ever to USB protocol. USB being a bus protocol acts as a medium to transfer SCSI protocol data units between the host and device. This is enabled by identifying the device as a Mass Storage Class device (MSC device) at an interface level. Once a device is identified to have a MSC interface, a MSC driver stack is loaded by the host. Any communication happening to or from the  interface specified as MSC interface will be initiated by the MSC driver by placing a request to the root hub driver . The root hub driver (RHD) collates multiple requests from different drivers and forms the frames.   Major part of the MSC interface driver,  is a wrapper to the actual SCSI driver. The SCSI driver will place requests to the MSC driver which is in turn transmitted over USB by the root hub driver (RHD). However, the SCSI driver need not have any awareness of the USB protocol or structure. It can call the MSC driver APIs which will in turn place a request to the RHD. On the other hand, the RHD need not know any implementation details specific to SCSI or MSC.      The figures below illustrate clearly as to how the SCSI commands are embedded into the USB transfers        Thus the class based architecture enables a USB device to be controlled by an existing driver whose implementation is independent of USB protocol (reuse of existing drivers)   Host Scheduling   So far we have been concentrating on a single device being connected to the Root Hub . However, the protocol is designed to handle up to 126 devices connected in a tiered star topology. In this section , we will discuss how the host schedules transactions to multiple devices of varying speed modes connected to it at the same time      As seen from the topology, the devices are connected to a single point in the root hub via port expansion units called hubs. A hub has an up stream port connecting towards the host and a downstream port which actually provides the connection points to the devices.   Thus hubs play a major role in the topology of USB bus by taking up some of the ‘duties’ of the host like sending frequent ‘keep alive’ signals. Discussing the hub protocol is outside the scope of this tutorial.   Once multiple devices are connected to the topology, the host gives each of them a unique 7 bit address, for this the initial communication happens with device address 0 (the default address of a device before getting the actual device address.) preventing device addressing at address0 is collaborated between the host and the hubs.   Once addressed, the actual enumeration of the devices happens by which the host reads out the ‘capabilities’ and ‘requirements’ of the devices in terms of the descriptors as described in pervious sections.   If a device requests for more resources than that available with the host, the device will not be enumerated. To elaborate this use-case, consider the scenario where there are three isochronous endpoints (functions) that are already enumerated which consume all the available bandwidth. When a newly attached device requests for some bandwidth that is not available (since it is pre-occupied by devices that are already enumerated), then the device will not be enumerated by the host.   Once devices are enumerated, the host creates its internal data structures based on the type of endpoints and the requests received from the client software. This is explained in brief below.   Once a device is enumerated there will be a endpoint data descriptor generated for the device at the host. This could be considered as the head node of the linked list holding the data to be transferred to the specific device endpoint. Attached to it will be the transfer descriptors which hold data to be send to the endpoints. Data coming as part of the Client software (device drivers) requests will be attached as transfer descriptors.   Transfer scheduling to multiple endpoints is done in terms of frame number. For example, an interrupt endpoint can requests to be polled every 10th frame, then the host will schedule to send the corresponding transfer descriptor every 10th frame. As for the example shown in figure the transfer descriptor can contain an IN token.         Many of the images are taken from sample traces provided by Ellisys USB analyser tool and the USB 2.0 Specification itself.  ","categories": ["Articles","Tutorial"],
        "tags": ["USB 2.0","USB"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/usb-2-0/",
        "teaser": null
      },{
        "title": "USB-3.0",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   Note: The target audience for this a tutorial are people who are aware of the USB 2.0 Architecture. If you are new to this, please go through my tutorial on USB 2.0 (here) first.   There are numerous articles in the internet that gives a peripheral understanding of this technology. However, this article is all about the core concepts and building blocks of USB 3.0          Table of contents     1. Introduction   2. USB 3.0 Design goals   3. Architectural overview           3.1 Physical Layer       3.2 Link Layer       3.3 Protocol Layer           4. Physical Layer           4.1 8b/10 Encoding       4.2 Data Scrambling       4.3 link initialization and training                   4.3.1 Clock Locking           4.3.2 Data Locking                       4.4 Low Frequency Periodic Signaling (LFPS)       4.5 Spread Spectrum Clocking           5. Link Layer           5.1 Packets and packet framing                   5.1.1 Header packets and framing           5.1.2 Data Packets and payload           5.1.3 Link Commands                       5.2 Link packet transfer and flow control                   5.2.1 Link Initialization           5.2.2 Basic Link Operation           5.2.3 Logical Link Idle and Link power management                           6. Protocol Layer           6.1 Transaction Overview       6.2 Packet Types       6.3 Transactions                   6.3.1 Burst transactions           6.3.2 Bulk transaction           6.3.3 Bulk Stream Protocol                                1. Introduction   Universal Serial Bus is a part of our day to day life. The number of USB compliant devices is growing day by day. And now, we have a relatively new protocol that is meant to be backward compatible to USB 2.0 and at the same time be faster and more efficient. Let us take a look into what makes up this USB super speed (SS) technology. This article assumes that the reader is aware of the USB 2.0 Protocol.   Most of the diagrams are taken from USB 3.0 specification document. Screenshots from sample traces included in Ellisys SuperSpeed USB Explorer 280 software are also used.   2. USB 3.0 Design goals   USB 2.0 is a very popular and dominant technology. Even by sheer numbers, it would be unwise to think of a technology to replace it altogether. The key to change would be to come up with a technology that is backward compatible yet better that USB 2.0. This is the basic philosophy behind the USB 3.0 protocol. The design Goals can be summarized as below      Preserve the USB 2.0 model of a smart host and simple device   Improve power management   Preserve investment – backward compatibility   3. Architectural overview   This section discusses about the key architectural components of the USB 3.0 protocol. To start with, the model of a smart host and simple devices as in USB 2.0 is preserved. However, the enhancement is a dual bus with dedicated physical channels for Tx and Rx. There is an additional pair for the backward compatibility (USB 2.0 Traffic). This new USB cable with 3 twisted pairs of data lines and a power line pair is called the composite cable.     USB 3.0 interconnect     USB 3.0 Architecture overview  USB 3.0 inherits most of the core architectural elements from the previous version. It maintains the following with minor mortifications      Single host Tired Star Topology   Transaction types : Control, bulk, isochronous and interrupt   Concept of functions, endpoints, pipes etc.   However, there are some key differences. They are tabulated below      The USB 3.0 protocol is a layered protocol as shown in the figure below. The key features of each layer is described below     USB 3.0 Layered Architecture  3.1 Physical Layer   The Key functions of this layer include      Enables chip to chip communication of protocol data with minimal processing   Detection and presentation of speed modes   Ensure seamless high performance communication   The functional modules include      8b/10b data encoding   Data scrambling   Low Frequency periodic signaling   Spread Spectrum Clocking and elastic buffers   3.2 Link Layer   Physical connection between two USB 3.0 ports is called a link and the link layer maintains this physical connections. Functions include      State machines to maintain the physical connections   Port to port reliability management   Buffering of data and protocol layer information elements   Packet framing   Error checking of header packets   3.3 Protocol Layer   The protocol layer defines the end to end communication between the device and the host. Packet headers are the building blocks of the protocol layer. They are used to route data to relevant ports only.   Now, let us look into the detailed implementation of each layer   4. Physical Layer   The PHY layer maintains data integrity and enables reliable communication of high speed data through the link by using number of techniques. The key techniques used are described below.   4.1 8b/10 Encoding   USB 3.0 uses ANSI standard (X3.230-1994/INCITS 230-1994), commonly known as 8b/10b encoding for medium encoding. This technique is used to maintain a balance in the number of 0s and 1s being transmitted over the medium. The standard encoding ensures a difference of less than 2 in a set of 20 bits that are transmitted in the medium. The standard encoding is slightly altered to ensure that there are not more than 5 1s or 0s in a row so that the DPLL remains locked.   Basics of the 8b/10b encoding are given below.   An byte of data is split into two groups of 5 and 3 bits and are called the 5b/6b and 3b/4b group respectively. Additional bits i and j are added to the groups respectively thus making the encoded data *ABCDEiFGHj* **. The bits i and j are selected from a lookup table specified in Appendix A of the USB 3.0 specification      The 8b/10b encoding maps a set of 256 elements (28) into a set of 1024 elements (28+2). Some symbols that have continuous run of more than 5 1s or 0s are ignored. The remaining is divided into two sets namely      K codes : used as special symbols for data framing and link management   D codes : used for actual data encoding      The original data bits (un-encoded) are represents as D.x.y where x ranges from 0-31 (5b/6b group) and y ranges from 0-7 (3b/4b group). The lookup table has two sets of entries named the D.x.n and D.n.y. The correct encoding for the data is selected by the PHY layer from the lookup table.   In the case of a “running disparity” where there is a run of 5 consecutive 1s or 0s when two pieces of data are kept together after encoding, an alternate encoding is to be chosen from the table. (hence the RD+ and RD- entries)      Special symbols (reserved symbols) follow a different encoding represented as K.x.y. Any unknown symbol is replaced with K.28.4 and is passed on to the link layer for further processing.      4.2 Data Scrambling   Data scrambling is a technique used to reduce inter signal interference by spreading the power spectral density of the data being transmitted in the links. Data scrambling is implemented prior to 8b/10b encoding by serially XOR-ing the data with output of an LFSR. Training Sequence codes and K codes are never scrambled. The LFSR is synchronized by re initialization with K.28.5 (COM)      4.3 link initialization and training   One of the first operations that take place once a USB 3.0 compliant device is plugged into the host/hub is link initialization and training. These operations sync the link operators in terms of the operating parameters and make them ready for actual data transfer. The key player in Link initialization and training is an “ordered set” which is a predefined sequence of D and K values. The key functions of this operation include      Configuration and initialization of the link   Bit and Symbol locking   Rx Equalization   Lane polarity inversion   4.3.1 Clock Locking   The clock and data recovery circuit (CDR) extracts the phase and frequency information of the link attached to. For this, long sequence of D.10.2 symbols are used since this symbol have alternating 1s and 0s   4.3.2 Data Locking   The start and end of every 10 bit symbol is demarcated by a COM symbol (K.28.5, comma). The symbol is unique and will not be found in any other combination of data being transmitted in the link. This symbol is used by the clock and data recovery circuit to get a data lock on the link.   The figures below shows a training sequence and the COM symbol embedded on to it.     Training sequence from Ellysis traces    COM symbol separating data pieces  4.4 Low Frequency Periodic Signaling (LFPS)   LFPS is a sideband communication mechanism for low level interaction between the link partners in low power states or during a training sequence. The purpose of the low frequency signal is determined based on the burst timing          4.5 Spread Spectrum Clocking   The super speed USB architecture supports the use of separate clock reference drivers at the two link partners. However, the accuracy should be ±300 ppm. However, the clocks are not generated in the common square wave pattern but are rather in a spread spectrum pattern with ±5000 ppm accuracy. This is because a fixed frequency square wave has power dissipation in a narrow peak in the power spectra which will make the device non compliant to FCCI EMI regulations. However, when the clocks are generated in a spread spectrum, the power dissipation peaks also spread out in the spectra.   The variation in frequency of the Tx and Rx ends are compensated by injecting extra data at the transmitter. An average of 1 symbol per 354 symbols is injected in this manner. This extra data insertion/absorption is facilitated by “elastic buffers”. The transmitter allows to buffer up to four SKP ordered sets in the elastic buffers to facilitate frequency delta compensation.      5. Link Layer   The USB 3.0 link layer is responsible for maintaining the link level interaction and state machines. It also handles link power management, packet framing and link errors. A link can have the following states.      U0 : The normal functional states   U1 : Low power state with no packet transmission   U2 : More power efficient state with increased exit latency   U3 : Suspended state   Recovery : State waiting for host reset. Needs retraining   5.1 Packets and packet framing   Packet headers constitute the basic building units of the link management protocol. They are used to determine what to do with the data received and also to maintain link states. Each header packet is 20 symbols long (20 bytes decoded). The 4 data packet headers are      Data Packet Header   Link Management Packets   Transaction Packets   Isochronous time Stamp Packets   The figure shows a setup transaction with a header followed by data          5.1.1 Header packets and framing   All header packets have the following common fields      Header packet start ordered set (HPSTART) made up of 3 start header packet symbols (K.27.7 – SHP) and 1 end packet framing symbol (K.23.7 – EPF)   2 byte CRC16 computed on the 12 byte header information   2 byte link control word   The 12 bytes of information determine the purpose if the header packet.          The link control word (2 bytes MSB) facilitates link level and end to end flow control. In includes the information required to route the packet to only the desires link in the tired star topology. This prevents broadcast of data and enhances power utilization. Only the ITP will be broadcast in USB 3.0     Link control word structure  5.1.2 Data Packets and payload   Any data packet starts with a DPPSTART ordered set and is followed by 0-1024 bytes of data. A 4 byte CRC will be computed on the data and appended after the data. This si followed by a DPEND or a DPPABORT (End Data Bad (EDB) symbol) ordered set to denote the end of data. The data payload and framing are tightly packed with no spacing in between.     DPP and DPH  The figure below shows transfer of 8 bytes of data      5.1.3 Link Commands   LPM commands are used to ensure link level data integrity, flow control and link power management. Link commands are 4 symbols long with the following constituents      4 symbol packet framing ordered sets.   2 symbol link command word   Replica of the link command word for error tolerance     Link Command Word     Link credit handling  The 16 byte long link command word is formed of 11 bit link command information and 5 bit CRC (CRC5). Link commands are divided into 4 classes based on use case. They are commands for      Ensuring successful packet transfer   Link flow control   Link power management   Declaring presence in active power state      5.2 Link packet transfer and flow control   The link layer has a Link buffer that can hold up to 4 unacknowledged (unprocessed) headers. They are circular and are named A through D. The receiver has to inform the transmitter about the availability of a link buffer. This is done in terms of link credit and the process is called “Rx header buffer credit exchange” and uses LCRD_x (x= A through D) link commands. The link commands should be transmitted sequentially. In case of a missing order, retransmission is triggered. This command is embedded in bits [10:9] of the link command word.   Link level acknowledgements are established using LGOOD_n, LBAD and LRTY commands. The three bit header sequence number in the header packet link control word is used to determine the LGOOD_n sequence number. i.e n can have a value ranging from 0 to 7 depending on which header sequence number is being acknowledged. If an LBAD is received, an LRTY is used to signal a retry.   5.2.1 Link Initialization   Link layer initialization of the link includes header sequence number and header buffer credit advertisement. This is done by sending LCRD_x for all available buffers and an LGOOD_n with n=(Rx header sequence number -1)&lt;sub&gt;modulo 8&lt;/sub&gt;. More on this will be discussed in the protocol layer      5.2.2 Basic Link Operation   Initially, the header packet sequence counter will be zero at the transmitter and receiver. The LCRD_x will be initialized to LCRD_A at the receiver and the transmitter will also keep track of this. A framed header packet with this sequence packet number of 0 will be sent by the transmitter to the receiver. Upon receiving this with a clean CRC, the receiver will send back a LGOOD_0 back to the transmitter. However, the link credit buffer is not freed since the date in the header packet is not yet processed by the receiver. When the receiver processes the data and the link credit is available, receiver sends a LCRD_A to the transmitter. This will update the next available link credit buffer as LCRD_B. both the receiver and transmitter keeps track of this information.      In case of an error, say a bad CRC on the header packet, then the receiver responds with a LBAD instead of an LGOOD_n. In this case, the transmitter sends an LRTY followed by a retransmission with the same header sequence number. Once this is received correctly, the transmitter sends a LGOOD_0 and a LCRD_A when the link credit buffer is free.   The transmitter can keep transmitting 4 pieces of data without any acknowledgement from the receiver since there are 4 link credit buffers available.   5.2.3 Logical Link Idle and Link power management   In USB 3.0 a logical idle condition of a link is one or more symbol period with no information transfer in the link. The link layer uses LGO_s (s ranging from U1 to U3) to enter the corresponding power state. This is acknowledged by the receiving end with a LAU and in case of rejection, an LXU is sent. LPMA is used in conjunction with LGO_x and LAU to ensure that both ports are in the same state. When a port is in U0 state, and no other data is to be transmitted, it keeps sending the LUP packet every 10us to declare its presence in U0 state   Polling is a part of link training which is achieved with a polling LFPS.      6. Protocol Layer   The protocol layer facilitates the actual data transfer as requested by the host application (driver). The key components of the protocol layer are the different packet and transaction types. USB 3.0 allows a new type of transfer called a stream bulk transfer.   6.1 Transaction Overview   The key difference in USB 3.0 transactions with respect to the USB 2.0 transactions is the fact that Host can schedule one or more OUT transactions while waiting for the completion of current bus transaction. This is partially facilitated by the link credit mechanism discussed in the link layer.   IN tokens in USB 2.0 is replaced by integrating the functionality into the Data Packet Headers (DPH). This gives rise to a burst model data transfer. More details regarding this is discussed later on.   Another key difference from USB 2.0 is that a device can asynchronously send ERDY (without the host explicitly scheduling it) to the host when it is ready for a data transfer.   When the host request for a data transfer, the device can respond with one of the following three packets.      Data packet : in case if the requested data is available.   NRDY : in case if the requested data is not available. In this case, the   Device has to send ERDY at a later stage asynchronously to the host when data is available      STALL : In case of error   The transactions have a direct path between the host and the device and are not broadcast.   6.2 Packet Types   There are 4 main packet types defined in USB 3.0 protocol. They are      Link Management Packets that travels between a pair of links only   Transaction Packets  between device an host direct path   Data Packets  between device an host direct path   Isochronous Timestamp Packets   that are Multicast on all links   All packets will have a common type field, a Link control word and a CRC 16. Other fields vary with the packet type      The link management packets carry no addressing information and are hence not routable. This is because they traverse between direct partners of the link pair.   A transaction packet can have any of the following sub types           ACK     This packet will contain a SeqNum field to specify the next expected packet sequence number. It will also denote the expected number of packets with the NumP field. Thus the ACK doubles as a data synchronization mechanism along with the intended handshaking       NRDY     The packet indicates temporary inability of an endpoint to receive or transmit data. This is used only by a non-isochronous device.   ERDY     Send asynchronously by an endpoint to denote availability of data .   STATUS      Sent by control endpoint to denote start of STATUS stage   STALL     Denotes a halt in deice endpoint   PING and PING_RESPONSE     Used to initialize all links in the path to U0 prior to initializing an isochronous transfer.   DEV_NOTIFICATION     This packet is used by a device in general rather than from a specific endpoint. The device notification can have one of the following sub-types.   FUNCTION_WAKE    Used to identify that caused a remote wakeup in a device   LATENCY_TOLERANCE_MESSAGE     This is an optional feature enabling more power efficient operations. It is used to intimate the Best Effort Latency Tolerance (BELT) value of the device. BELT is the time in ns that a device can wait for a service before experiencing unintentional side effects. They are represented as multiples of 1024, 32768 or 1048576. The BELT value is common for all the configured endpoints. The device has to ensure frequent estimation of its BELT value. The minimum BELT value is 123us and system shall default to a BELT of 1ms for all devices.   BUS_INTERVAL_ADJUSTMENT_MESSAGE     A device might face issues while trying to sync hosts bus interval clock with its internal clock. We need a combination of hardware and software techniques to resolve this. Elastic buffers discussed in the PHY layer is the hardware solution. As for the software part, USB 3.0 specifies a mechanism called bus interval adjustment message that allows a device to increase or decrease the bus interval. The protocol allows the device to request incremental shots no greater than ±4096 at a time to achieve a maximum adjustment of -32768 to +32767. Only one device can control the bus interval at a time. The protocol uses a first come first served mechanism to facilitate this and the address of the controlling device will be indicated in the next isochronous timestamp (ITP) packet.   A data packet header + data packet results as a response to an ACK TP. This eliminates the use of the USB 2.0 IN tokens in USB 3.0. Data payload with a 32 bit CRC immediately follows a data packet header. The data packet will have the sequence number of the first data packet, and data length (excluding CRC32) along with other relevant information.   Isochronous timestamp packets are multicast to all ports in U0. Timing values are accurate to 125us and contain a delta field specifying a time delta from the previous bus interval boundary.      6.3 Transactions   USB 3.0 transactions are mostly point to point and not be broadcasted. This is facilitated by a 20 bit routing string in every downstream port (DSP) directed packets. This is generated by concatenating the DSP numbers (4 bit per hub) for each hub traversed to reach a hub. (This limits the number of ports to 15 in a hub). This is added by the hub and the device sees it as reserved field.      USB 3.0 transactions are very much like the USB 2.0 transactions. However, there are few enhancements incorporated. To understand those, we will look into the details of a bulk transfer.   6.3.1 Burst transactions   USB 3.0 allows a device to transmit and receive packets without waiting for an acknowledgement. This is the burst mode of transfer. The number of packets per burst inintimated in the endpoint companion descriptor at enumeration.   During a transaction, the remaining number of packets that can be received in a burst can be calculated from the NumP field of the latest acknowledgment TP. However, all packets need explicit acknowledgement and the NumP field should not be decremented by more than 1 at once. Also, in a burst, all packets except the last will have maximum size of data.      6.3.2 Bulk transaction   An ACk transaction packet implicitly acknowledges the last received Data Packet (DP) with the previous SeqNum as being successfully received by the host. It also serve similar to the USB 2.0 IN tole and asks for the next NumP DPs with starting at SeqNum. In case of an error, an ACK TP with the first erroneous DP SeqNum and RTY bit set will be sent. This means that even though subsequent DPs in the burst where received correctly, they need to be re transmitted.   The first transmitted packet will have the SeqNum set to 0. This value rolls over to 0 after 31. If device donot have sufficient number of data packets as requested the last packet of the burst will have the EndOfBurst bit set to one.   In case of a bulk OUT transaction, host sends a DPH followed by a DPP to the device. If the device is not ready to receive data, it responds with a NRDY . Later, when the device is ready to accept data, it asynchronously sends the ERDY packet.      6.3.3 Bulk Stream Protocol      USB 3.0 defines a new protocol built over the USB 3.0 bulk transfer protocol that expands the usability of a bulk endpoint. The gist of the bulk stream protocol is to extend the number of host buffers accessible by an endpoint from 1 up to 6553. i.e, large amounts of data can be transferred in parallel without wasting time form the data to be processed from the EP buffers. The protocol follows the semantics of the bulk pipe and thus packets in a stream pipe cannot be distinguished from a normal bulk packet except from the “Current Stream ID”.   Stream pipes are maintained by a stream protocol state machine   ","categories": ["Articles","Tutorial"],
        "tags": ["USB 3.0","USB"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/usb-3-0/",
        "teaser": null
      },{
        "title": "UPnP Device Architecture",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   1. Motivation For UPnP          Table of contents     1. Motivation For UPnP   2. Introduction   3. Key Features   4. Building Blocks   5.Groundwork           5.1 Uniform Resource Identifier       5.2 IP Multicast       5.3 HTTP                   5.3.1 HTTP 1.0                           Request structure:                                   5.3.2 HTTP 1.1           5.3.3 HTTP Extension framework                       5.4 HTTP over UDP       5.5 Extensible Markup Language (XML)           6. UPnP Protocol Stack           7.1 Addressing       7.2 Discovery       7.3 Description       7.4 Control       7.5 Eventing       7.6 Presentation           8. Addressing           8.1. Dynamic Host Control Protocol (DHCP)       8.2 AutoIP       8.3 UPnP device Addressing           9. Discovery           9.1. Simple service discovery protocol (SSDP)                   9.1.1. SSDP Discovery request           9.1.2. SSDP Discovery response           9.1.3. Presence Announcement (Advertisement)           9.1.3.2. ssdp:alive advertisement           9.1.3.2. ssdp:byebye advertisement           9.1.3.3.  ssdp:update                           10. Description           10.1 Device Descriptor Document       10.2. Service Descriptor Document (SCPD)       10.3. Retrieving Descriptors           11. Control           11.1. Simple Object Access Protocol (SOAP)                   11.1.1. SOAP over HTTP                       11.2. UPnP Action Request and Response over SOAP           12. Eventing           12.1 Event Subscription       12.2 Event Message           13. Presentation   14. Summing up        USB provides a universal solution to the PC plug and play problem. It has minimal configurations to be done by the user, user (generally) doesn’t have to reboot or make configuration changes are to add a new device. At the same time, the USB architecture is is highly hierarchical .   Currently, numerous pervasive and cross platform technologies like (TCP/IP, HTTP, XML, UDP etc) are available. So , the question is,  why not integrate them all to get a “Foundation of the Connected Home”   2. Introduction   UPnP targets at universal digital, open home networking platform that requires      Zero configuration   Reuse existing technologies so that we don’t re-invent the wheel and  support from standard systems that already has most of these technology stacks in place.   UPnP was introduced by Microsoft Corporation at the Consumer Electronics Show in January of 1999. It is currently maintained by the UPnP forum.   3. Key Features   Major features provided by UPnP are      Device connectivity     Devices can join and leave the UPnP network transparently, advertise their services, discover other devices and services, send events, and control other devices.   Ad-Hoc networking     No dedicated network infrastructure have to be out in place for enabeling UPnP home network. Networks are configured on the fly without manual intervention. thus, it is a “Zero configuration network”   Standard based Architecture     All foundation technologies used in UPnP are  existing and proven. Thus, realizing UPnP in any platform is extremely simple. This also reduces the time-to-market.   Platform and medium independence     UPnP can be implemented over any medium for which an IP stack is available. Thus, UPnP can be used to aggregate devices connected through phone lines, power lines, RF , Ethernet, WiFi (ya, it is RF again:) )  etc   Programmatic and manual device control     Devices in the UPnP network can be controlled programatically as well as manually   4. Building Blocks   Basic abstractions of the UPnP device architecture includes 3 major components . They are      Devices   Services   Control points   Service is a unit of functionality implemented by a device. there can be  0 or more services present in a device.   Specifics about service implementations are defined by a separate UPnP forum working committee and are not part of the device architecture.   A service description will define “actions” ,input and output parameters , return value etc and will include mandatory device services. For example, an audio rendering device, such as a CD player, might have a service that provides the ability to play, stop, and pause audio content.   A “Control point” invokes the services provided by a device. They can control the device and request updates on state change of a service.   We will visit each of  these topics in depth in later sections. However, first we need to get an understanding of the underlying technologies used in UPnP.   5.Groundwork   In this section, we will try and get a quick glance of the underlying technologies used in UPnP. This include   This section introduces      URI   IP Multicast   HTTP   HTTP over UDP   XML   5.1 Uniform Resource Identifier   The world wide web stores information (web pages, images, data, video, songs etc) in the form of resources. A URI is a compact string used to identify a resource. URI folloes syntax specified in rfc2396. URI is further classified into URL and URN   A Uniform Resource Locator (URL)  identifies a resource by location rather than by name or some other identifier. Thus the content in this location can keep changeing. A good illustration of this is the arbitrary URL – http://example.com/quote-for-the-day. Each day, the quote in this page changes but the URL remains the same and points to the same internet resource that generates the quote for the day.   A Uniform resource names (URN) is a unique and persistent identifier of a resource. It cannot be reused even after the resource cease to exist   Note: http://example.com domain is established by IANA to be used for illustrative examples in documents.   5.2 IP Multicast   A sender sends a multicast message to a multicst address (:) and all the members of this  “host group” receives data from the sender. Any device interested in listening to multicasts join the group with an ICMP message and the router ensures that the device receives the message. Sender need not be a member of this group.   Multicast addresses are class D internet addresses. i.e range of addresses from 224.0.0.0 to 239.255.255.255 (with reservations)   UPnP control points and devices use the multicast address 239.255.255.250, and port 1900.Any source can send data to all UPnP devices and control points on a local network with this address. Thus UPnP network requires support of multicast capable IP routers.   Reach of multicast is controlled by using the Time To Live (TTL) field of IPv4 headers.TTL is decremented by 1 for each hop (router). A packet is transmitted only if TTL &gt; 0.   Reach of multicast can also be restricted with administrative scope of IP multicast. 239.0.0.0 to 239.255.255.255 is administratively scoped IPv4 multicasts address space. (Termed IPv4 Local Scope). Network administrators partition this space and limit multicast range. Generally 239.255.255.250 falls within the IPv4 Local Scope and is the smallest of the administrative scopes.   5.3 HTTP   5.3.1 HTTP 1.0   Hyper Test Transfer Protocol 1.0 is a simple request-response protocol. The current HTTP 1.1 is built on top of this protocol. The major features of this protocol are discussed below.   In HTTP 1.0, connection is closed by server after each request. This makes it a stateless protocol. States are stored in “cookies” and are presented to the server with each request.   Request structure:   The initial line of the request specifies the  METHOD (GET,PUT,POST etc). This is followed by more header lines specifying variables and values and an optional message body (file, query data etc). Below are some of the components of an HTTP request-response.           Initial request line:       Format: &lt;method name&gt; &lt;request path&gt; &lt;http version&gt;       Eg: GET /path/to/file/index.html HTTP/1.0            Header lines:       Provides information about request, response or object sent in message body.Format is       header-name: Value            Response code:       Response code will indicate outcome of action        HTTP/1.0 200 OK  or   HTTP/1.0 404 Not Found                Message body:       In response, this is used to return requested resource to client or explanation text of an error       An HTTP 1.0 transaction to  retrieve file at URL http://www.example.com/path1/file1.html has the following steps:       First open a socket at http://www.example.com at port 80 (default http port)  Send the following string through the socket           GET /path1/file1.html HTTP/1.0        From : someuser@example.com        User-Agent : HTTPTool/1.0        (blank line)      Server responds with something like       HTTP/1.0 200 OK     Date: Fri, 20 Nov 2002 23:59:59 GMT     Content-Type: text/html     Content-Length: 1354     &lt;html&gt;     &lt;body&gt;     &lt;h1&gt;Sample Header Text&lt;/h1&gt;     (more file contents)     .     .     .     &lt;/body&gt;     &lt;/html&gt;   5.3.2 HTTP 1.1   The major improvements brought in by HTTP 1.1 are discussed here.   HTTP 1.1 supports multiple transactions over persistent connection by default. Requests are sent in a pipelined queue.This is applicable to the responses also. Once the transactions are done, the client uses “connection: close” header to denote end of connection. It can also be used by server to denote that connection is being terminated.   HTTP/1.1 100 Continue is used in slow connections to indicate data is on the way :smile:   HTTP 1.1 has support for data caching. All responses contain a GMT time stamp with the Date:  header. If-Modified-Since or If-Unmodified-Since headers can be used to request newer version of the resource.   HTTP 1.1 Chunked encoding allows response to be sent even before total length is known. This mechanism uses uses the Transfer-Encoding header set to chunked   Sample of chunked encoding transfer   HTTP/1.1 200 OK Date: Fri, Dec 2002 23:59:59 GMT Content-Type: text/plain Transfer-Encoding: chunked 1a: ignore-stuff-here                             |size of chunk in hex abcdefghijklmnopqrstuvwxyz                        |chunk data 10                                                |size of chunk in hex 1234567890abcdef                                  |chunk data 0                                                 |0 to denote end of chunks some-footer: some value another-footer: another value (blank line)   The footers should be considered as if they were sent before the data   HTTP 1.1 allows multiple domains to be served from same ip address. This is called multi-homing and is facilitated by the HOST header. eg   GET /path/file.html HTTP/1.1 Host: http://www.host1.com (blank line)   HTTP 1.1 also accepts absolute URL requests like GET http://www.intoast.com/path/file.html HTTP/1.2   Summary of HTTP 1.1 server requirements are:      Host header with each request   Accept response with chunked data   Support persistent connections or use Connection: close with each request   Handle 10 continue response (by ignoring it)   Summary of HTTP 1.1 client requirements are:      Accept absolute URL   Accept request with chunked data   Include Date: header in each response   5.3.3 HTTP Extension framework   HTTP provides a mechanism to address private headers and requests by extending a single HTTP message. This feature allows to defines few HTTP header fields to denote extensions.   5.4 HTTP over UDP   UPnP Exploits the connection less model of UDP and uses it primarily for sending HTTP multicast messages.      HTTPMU for multicast   HTTPU for UDP unicast   UPnP introduces 3 new headers to resolve issues arising from HTTP over UDP.           MX request header       To avoid chances of large number of responses coming back to the sender (Host) at the same time, HTTP over UDP uses a new header which denotes the number of seconds the multicast UDP HTTP resource can wait before it can send a response initiated by a multicast request.       The resource generates a random number between 0 and MX and waits for that much time to send response. Thus, if multiple responses are there, they will be spread out between 0 and MX.            Sequence (S) general header       The Sequence header is used to associate a unique URI with a request-response pair. It is Generated at request time and uses the same URI used in response            Alternate (AL) general header       This header is used for multiple “Location:”  header transmission       Request-URI in case of MulticastHTTP means the recipient and not the requested resource.  Thus the “*” request URI means, send to all.   Eg: M-SEARCH “*” HTTP/1.1   5.5 Extensible Markup Language (XML)   XML is a documentation language used to specify structure of data and how various elements relate. An XML document contains markup and character data. Markup gives the document its structure while character data is the actual content of the document. An XML document type definition (DTD) or XML schema (template) taht specifies teh constraints of data in teh document.   A typical XML document will have a prologue, root and misc part.   The Prologue contains XML declaration, processing instruction, comments, whitespace and a document type declaration. A sample declatation will look like:  &lt;?xml version=”1.0″ encoding=”UTF-8″ standalone=”no”?&gt;   Here standalone means no external dependencies   An XML DOM or document object model can be considered as a A programmatic view of the xml document as trees and nodes. DOM APIs are used to access and manipulate XML docs.      6. UPnP Protocol Stack     UPnP protocol stack  The figure shows an IP based protocol stack . (even though UPnP can be implemented over any medium/base protocol supporting socket based communication). Further discussion assumes an IP based protocol stack.   Basic abstractions of the UPnP device architecture includes three major components.      Devices,   Services   Control points   A physical device may have more than one root device. A root device can have multiple embedded devices. Thus UPnP provides a very flexible logical arrangement.   A device will have a state table that maintains state variables. All operations on devices are based on the states of these state variables.   There are 6 phases of operation for any UPnP device. they are      Addressing   Discovery   Description   Control   Eventing   Presentation   first we will have a breif description each phase. We will visit each in detail later   7.1 Addressing   A UPnP device aquires an address using either DHCP or AutoIP as soon as it joins the network. This enables Adhoc , Zero Configuration networking.   7.2 Discovery   As soon as a UPnP device gets an address, it searches for presence of other UPnP services in the network. It also announces presence to other devices present in the network. This is done using Simple Service Discovery Protocol (SSDP) over HTTPMU   7.3 Description   Once a device identifies other devices available in the network, it obtains service details of required devices in the network. The device should also provide its capability details to other devices requesting it.   7.4 Control   A Control point can  invoke the functionality provided by a service via SOAP messages. Simple Object Access Protocol (SOAP) is a Microsoft technology used for cross platform RPCs. Once a device service receives a message, it has to act upon it   7.5 Eventing   UPnP follows a publisher-subscriber model to notify monitoring/interested control points about change in the state variable. General Event Notification Architecture (GENA) is used for event notifications.   7.6 Presentation   UPnP devices also provide a browser based, manual interface. It is HTML based and the presentation URL is part of device descriptor document (DDD) that is provided as part of the description process.   Now, let us visit each of the stages in detail.   8. Addressing   DHCP allows two modes of Addressing      DHCP – which requires dedicated infrastructure (a DHCP server)   Auto IP  – a true AdHoc mechanism with lots of limitations on network reach   8.1. Dynamic Host Control Protocol (DHCP)   For DHCP, a server sits and allocates IP addresses to all newly arriving devices. It relies on UDP based transport and the server receive requests on port 67. The response is posted to the hosts on port 68.   DHCP alows three allocation modes      Automatic – where the IP address is allocated from a pool of addresses upon request by a client   Manual – where the admin configures the IP of each host and the DHCP server conveys it to hosts on request   Dynamic – where the DHCP server “Leases” an IP to the host for a limited period of time after which the device has to renew the IP.     Basic DHCP flow  8.2 AutoIP   First and foremost, AutoIP is Not a DHCP replacement. Rather, it is a temporary solution when the DHCP server is down. AutoIP address assignment happens in two major steps.           IP Selection       The device selects a candidate IP address within the non-routable address range (169.254/16) with default class B subnet mask 255.255.0.0. The first and last 256 addresses in this range are reserved and MUST NOT be used.              The Non-routable address range (169.254/16) do not cross network gateways and are known as LINKLOCAL net range of IPs            The selection algorithm is implementation dependent and is not defined by AutoIP. However, it is recommended to use device’s or control point’s Ethernet hardware MAC address as seed for randomization.            ARP       Send out an Address Resolution Protocol (ARP) probe for the chosen address. If no one responds, then it is yours :smile:       ARP is generally used to determine the MAC address of an IP address holder. Here it is used in an unusual way. Recommended probing pattern is four times at two-second intervals.         AutoIP sequence  AutoIP addressed devices will not be visible beyond the local network segment. IP packets whose source or destination addresses are in the 169.254/16 range will not be sent to any router for forwarding. In case this has to be by-passed, senders must ARP for the destination address and then send the packets directly to the destination on the same link. Devices and control points MAY assume that all 169.254/16 destination addresses are on-link and directly reachable.   8.3 UPnP device Addressing   Steps in UPnP Device Addressing are as follows:      If DHCP is available, get it from there.   Else, use Auto-IP to get the IP   If DHCP becomes available afterwards, get IP from there   Suggested recheck period for DHCP server availability is 5 minutes. However, the current IP address should be relinquished only after the current connections are over.   Once addressed, do a DNS registration if required. DNS addresses can be preconfigured or obtained from DHCP server.   9. Discovery   Once addressed, a device is ready to provide services to other devices in the network and control point, can start utilizing the available services. Discovery allows control points to search for devices and services matching a search criteria.   Devices advertise and inform. Control points discover and select. All this is done using simple service discovery protocol (ssdp)   9.1. Simple service discovery protocol (SSDP)   SSDP is a protocol for small networks that runs over HTTPMU. It facilitates a mesh model where there is no central store of information . However, it gets ugly as number increase.   UPnP adds two concepts to generic SSDP. They are      Service type   Unique Service Name (USN)   USN is a URI to identify functionality of a resource. It is a 128 bit Universally Unique Identifier (UUID). Its generated based on the host network address, a timestamp and a randomly generated component. (In linux based systems the uuidgen utility can be used to generate a uuid.)   service type: “printer” USN: “uuid: 2fac1234-31f8-11b4-a222-08002b34c003”   There are two types of SSDP requests.           Discovery request:     Sent by a device to know the devices that have come in before itself on a per service basis.            Presence announcement:     Sent only at the time of joining the network and leaving the network       As a result of this model, neither server nor client will have to send out steady stream of messages   9.1.1. SSDP Discovery request     SSDP Device Discovery header template  The headers in ssdp are      M-SEARCH: Search request broadcast method   ST      : Header to specify search target   MX      : Response delay limit to avoid cluttering as described in HTTP over UDP   MAN     : Always set to ssdp:discover   SSDP supports very simple searches without logical expressions, name-value pairs etc. Legal search type (ST) values are:      ssdp:all         : to search all UPnP devices   upnp:rootdevice  : only root devices . Embedded devices will not respond   uuid:device-uuid : search a device by vendor supplied unique id   urn:schemas-upnp-org:device:deviceType- version : locates all devices of a given type (as defined by working committee)   urn:schemas-upnp-org:service:serviceType- version : locate service of a given type   All SSDP requests are sent to 239.255.255.250 on the default port 1900. However this can be overridden with SEARCHPORT.UPNP.ORG header.   SSDP start line will always be one of the following      NOTIFY * HTTP/1.1: In case of advertisement   M-SEARCH * HTTP/1.1: In case of search   HTTP/1.1 200 OK: In case of response   SSDP also supports additional proprietary headers. Some of the defined headers include:   BOOTID.UPNP.ORG CONFIGID.UPNP.ORG NEXTBOOTID.UPNP.ORG SEARCHPORT.UPNP.ORG   Domain (vendor) specific header format will be of the form   myheader.example.com: “some value”   9.1.2. SSDP Discovery response   SSDP response is a unicast reply to the sender of search broadcast. It starts with HTTP success response followed by few headers. The response is sent to the same port that made the request.     SSDP response  SSDP response includes:                  Cache-control       This header is part of HTTP cache control settings. UPnP uses max-age for this field.                 Date (recommended)       Response generation timestamp                 Ext       Confirms that man header in request (ssdp:discover) was understood                 Location       URL to the device description document of the root device                 Server       concatenation of OS name, OS version, Product name and product version                 ST       Same as that specified in the discovery request                 USN       Unique service name. It takes different value formats depending on ST.           Some sample USN values are:       uuid:device-UUID:upnp-rootdevice     uuid: device-UUID     uuid:device-UUID:urn:schemas-upnp-org: device:deviceType:ver     uuid:device-UUID :urn:schemas-upnp-org:device:serviceType:ver     urn:domain-name:device:deviceType:ver     urn:domain-name:service:serviceType:ver   USN+ST provide unique identification to the service.   9.1.3. Presence Announcement (Advertisement)   When a UPNP device is joining, leaving, having location change or expiration info change, it has to send an advertisement to let other devices be aware of the changes that are happening.   The presence announcement is sent To standard multicast address (239.255.255.250:1900) where all control points listen.   Presence announcement uses (General Event Notification Architecture)GENA which relies on the NOTIFY method over HTTPU/HTTPMU.   There are three kinds of advertisement.      Device available : ssdp:alive   Device unavailable : ssdp:byebye   Update : ssdp:update   Advertisement sends info about root, embedded devices, and how to find them. So, if a root device has d embedded devices, s embedded services and k distinct service types, There will be 3+2d+k advertisement messages.   UPnP 1.1 adds 4 new header fields to SSDP           BOOTID.UPNP.ORG     A non-negative 31-bit integer, ASCII encoded, decimal, without leading 0s which is increased each time a device (re)joins the network and sends an initial announce. It can also be the same as the field value of the NEXTBOOTID.UPNP.ORG header field in the last sent SSDP update message.Otherwise same value is used in all repeat announcements, search responses, update messages and eventually bye-bye messages. A convenient mechanism is to set this field value to the epoch time of initial announcement            NEXTBOOTID.UPNP.ORG     Indicates the field value of the BOOTID.UPNP.ORG header field that a multi-homed device intends to use in future announcements after adding a new UPnPenabled interface       CONFIGID.UPNP.ORG     A non-negative 31-bit integer, ASCII encoded, decimal, without leading 0s that represents the configuration number of a root device. The configuration of a root device consists of the following information:            The Device Descriptor Document of the root device and all its embedded devices,SCPDs of all the contained services       Control points can parse this header field to detect whether they need to send new description query messages.           the values ranges from 0 to 16777215 (224-1) and higher numbers are reserved for future use       SEARCHPORT.UPNP.ORG (optional)   9.1.3.2. ssdp:alive advertisement     SSDP Alive Advertisement  This notification is sent when joining a network with one of the following notification type :      upnp:rootdevice   uuid:device-UUID   urn:schemas-upnp-org:device: deviceType:ver   urn:schemas-upnp-org:service: serviceType:ver   urn:domain-name:device:deviceType:ver   urn:domain-name:service:serviceType:ver   USN can be one of      uuid:device-UUID::upnp:Rootdevice   uuid:device-UUID   uuid:device-UUID::urn:schemas-upnp-org:device: deviceType:ver   uuid:device-UUID::urn:schemas-upnp-org:service: serviceType:ver   uuid:device-UUID::urn:domain-name:device:deviceType:ver   uuid:device-UUID::urn:domain-name:service:serviceType:ver   9.1.3.2. ssdp:byebye advertisement   This notification when the device is leaving the network     ssdp byebye  9.1.3.3.  ssdp:update   This notification is sent when a UPnP enabled interface is added or removed from the multi-homed device. This is applicable to all root and embedded devices. The BOOTID.UPNP.ORG header is increased when ssdp:update is sent.     ssdp update  10. Description   Control points gets the device and service descriptor URIs from discovery phase. Now, the  next step is to read and understand the required descriptor documents.   Descriptor documents are written in XML following the Schema provided by UPnP forum.   Descriptors come in two logical parts      Device Descriptor   Service Descriptor   These XMLs are retrieved using HTTP GET method.   10.1 Device Descriptor Document   Devicve descriptor document describes physical and logical containers. It contains the device identifiers and list of services provided by the device.     Device Description Document  The DDD spsecfies a UDN which is the unique device name and a UPC (universal product code). UPC is a 12 digit all-numeric code ID for consumer package.   The DDD also contains list of services and the service description document(SCPD) URL. Specific requests to the service can be sent to the “control URL” and event subscriptions are sent to the “EventSub URL”.   10.2. Service Descriptor Document (SCPD)     SCPD document  The SCPD primarily contains the list of all actions the service provides. It also contains an argument list.   One or more arguments may be marked as return value and each argument corresponds to a state variable.   The service state table describes the state variables. The sendEvents attribute denotes eventing status of the state variable. If it is set to  “yes”, an event will be sent when the state variable changes. The allowedValueList and allowedValueRange determine the legal values of the state variables.   10.3. Retrieving Descriptors     Retrieving descriptors  First get device descriptor, parse it to get service descriptor URL, then get requires service descriptors.   11. Control   When services are distributed over a system, one component cannot directly access data in another component or invoke services directly. So, we have to use a client-server message passing like mechanism where each component is a mini-server. Thus devices in the network can make simple calls to a network based remote entity as if it is a local component.However, this requires components to meet the following minimum requirements      A message based protocol with request, response, error invoking etc.   A Platform independent data representation to package argument parameters            Either predefined types or mechanism to pass type info           An Optional security feature   In case of UPnP, service requests are made to the controlURL sub element of the service element in the device descriptor. As long as the discovery advertisement has not expired, service can be assumed to be available.   UPnP uses Simple Object Access Protocol (SOAP) for controlling other device services.   11.1. Simple Object Access Protocol (SOAP)   SOAP is an XML based mechanism for web based (over HTTP) messaging and remote procedure call mechanism. It consists of 4 main parts.           SOAP envelope     An XML schema based infrastructure defining processing rules of the message. It is the outermost container of SOAP messages            SOAP encoding rule     An XML schema for defining user defined data types            SOAP bindings     Conventions for using different transport protocols that Generally uses HTTP            SOAP RPC representation     For representing RPC calls and responses       Any system capable of packing and unpacking XML can use SOAP since tt is platform independence and it can pass complex messages and data.   SOAP message envelope &lt;envelope&gt; has two child elements –           &lt;Header&gt; [optional]     It contains auxiliary information for authentication, transactions and payments. It can contain unlimited number of child elements.            &lt;Body&gt; [required]     Contains core of messages       11.1.1. SOAP over HTTP   A UPnP SOAP message is sent as an HTTP POST to the control URI of the service obtained from descriptor document. The Content type will be set to “text/xml”.   SOAP introduces a a new header “SOAPAction” into HTTP. It contains a URI that indicates intention of the soap request. This URI need not point an actual web resource since it is used by the recieving web server to route the request. An empty string (“”) implies, the intent can be known from the HTTP request URI. No value means no indication about the intent.     Sample of a SOAP Message over HTTP  SOAP Uses HTTP Extension framework and introduces the MAN header and modifies POST to M-POST. M-POST requires HTTP server to read and understand the URI in MAN. First request is sent without MAN. If it fails with 405(method not allowed), then retries with M-POST. If the retry fails, then the whole request fails.     Soap Response  SOAP HTTP response is also a XML document where the &lt;Body&gt; contains the encoded result instead of the method call. Void methods omit &lt;return&gt; in the body   11.2. UPnP Action Request and Response over SOAP   Action request is sent to the service control URI with the action name, argument name and argument values as described in the service descriptor document.     UPnP action request over SOAP     SCPD XML showing action and argument list  Upon receiving the request, the service has to respond within 30 seconds.     SOAP Response with HTTP Header  If large amounts of data has to be transferred, it is recommended to do an out of band transfer, i.e. publish URL as part of SOAP and later get it using HTTP GET. Error response follows a similar schema with error values.   12. Eventing   UPnP eventing uses General Event Notification Architecture (GENA) over HTTP. It follows a subscriber-publisher model where the services are publishers and control points are subscribers.   Eventing begins with a subscription request from the control point to the event subscription URL of the service.  The publisher (service) responds with a subscription ID and duration of subscription.   GENA brings in 3 new HTTP methods:      SUBSCRIBE   UNSUBSCRIBE   NOTIFY   It also introduces the following HTTP headers:      CALLBACK : Used by subscriber to register notification URL   NT : Notification type   NTS : Notification sub type   SID : Subscription ID generated by publisher, used by both parties   If a service state variable is marked as evented in the SCPD , control point can subscribe to notifications in case there is a change int eh state variable value.     Evented state variable  GENA uses XML based event messages whic includes state variable name and UPnP template language for eventing. An initial event message is sent upon subscription which includes all subscribed state variable names and states. The publisher maintains an event notification counter known as “Event key” that is incremented and embedded into each event notification.     UPnP eventing Schema  12.1 Event Subscription   A common event subscription is made to all evented state variables in the service. The event subscription request will also contain the CALLBACK header which specifies the URL to which event notifications are to be sent.     Event Subscription request  the event subscription response will include the HTTP response code and the subscription UUID (SID) (if success).     Subscription Response  The subscriber can send a renewal or cancellation request that wil be identified by the published by the SID.     subscription renewal request    Subscription Cancellation  12.2 Event Message     Unicast event message  Generally event notifications are sent to the registered callback URL as a unicast. The receiver acknowledges the notification with the appropriate HTTP response. However, there is also an alternate mechanism to publish events as a multicast to 239.255.255.246:7900.     Multicast event message  13. Presentation   UPnP also has provision for a browser based control of the device . The Presentation URL is part of Device Descriptor Document.   14. Summing up     Intel SDK – libupnp  Every UPnP device/control point have a specialized HTTP server Assisted by an XML Parser. They process and route the messages. Messages are routed into the corresponding Protocol stack for processing   Broadcasts are done over UDP at standard ports  ","categories": ["Articles","Tutorial"],
        "tags": ["UPNP","Networking"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/upnp-device-architecture/",
        "teaser": null
      },{
        "title": "Home made single sided PCBs",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   There are numerous tutorials available in the internet about home made PCBs and the toner transfer method. Here is my addition to that :smile: )   Step 1: Design your PCB in your favorite tool. I personally prefer CadSoft EagleCAD.   This is true only at the time of writing the article :smile: . My current preferred tools is KiCAD.   Step 2: Print the mirrored PCB artwork in a high GSM high gloss paper with a laser printer . I use the card like ones available to print posters.   Step 3: Cut out your copper-clad PCB to the required size. Clean the copper side and remove the natural oxide coating. I use mild steel wool used to clean utensils. But, be extra cautious not to make the copper surface uneven or scratched. While cleaning, it is better to move the steel wool in the same direction always.                                         Step 4: Keep the printed artwork face down into the copper side of the copper-clad. Ensure that the edges of the paper and copper-clad are aligned.   Now place a clothes iron box at maximum temperature over the paper and apply pressure for around 20 seconds. Now move around the iron box applying pressure. Pa special attention to the edges.   At the end of this process you will be able to see a shadow of the traces over the paper as shown in the figure below.                                         Step 5: Keep the board aside to cool down. Once the board is cooled down, immerse the board into water  (at room temperature) for around 20 minutes. This will make the paper soft.                                         Step 6: Now use a soft bristle toothbrush to gently remove the white part of the paper. Once you start seeing the copper side, be extra careful so that you don’t accidentally remove the traces (seen in black). Do not try to peel off the paper.                                                                                    Step 7: If in case you damage any of the traces, use a fine tip marker (the ones used with OHP sheets or to label CDs) to make up for the damage.                                           Step 8: Prepare a concentrated solution of ferric chloride in water. It is better to use a plastic or glass container for this since the solution is highly reactive. The reaction is also exothermic . So, take necessary precautions.   Make sure that all particles are dissolved and the solution is clean since floating particles can damage your board.                                           Step 9: Place the board gently into the solution. Stir the solution once every 30 seconds. Depending on the quality of your solution, the base material of the board will be visible in around 15 minutes. The solution will also turn green.                                                                                    Step 10: Once you ensure that all visible copper is removed, remove the board from the solution and clean it under running water and dry the board with a tissue paper.                                                                                                                             Step 11: Remove the toner with turpentine or nail polish remover and drill holes or appropriate size.  ","categories": ["Articles","Tutorial"],
        "tags": ["PCB","Electronics"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/home-made-sigle-sided-pcbs/",
        "teaser": null
      },{
        "title": "Test coverage analysis with GCOV",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   Any large code base needs to be incrementally tested for each release for regressions and bugs. When the code base grows, manual testing fails to meet the requirements and we have to move into automated systems for testing. Once a test is written, we have excellent continuous integration systems like Jenkins or Cruise Control to validate the tests against any changes made on the code. However, the CI system is only as effective as the test. If the test does not cover all use cases, then there is no point in running the tests in CI.   This article intends to illustrate the use of ‘gcov’ to estimate the dynamic coverage of a test. The article speaks solely from the viewpoint of a C programmer working on a Linux PC (or similar systems)   What is coverage?   All large scale, usable C code is written in the form of blocks of code enclosed in functions. A block can be a set of simple execution statements or a logical branch.   The ideal test code should logically call all the functions and execute all statements in the blocks. The percentage of lines of actual code that gets executed when a test code runs is called the coverage of the test code. More the number of lines of code that is tested, less is the probability to get a last minute surprise bug.   There are two types of coverage analysis that is possible.   Static code coverage analysis is done by analyzing the test code and the actual code to primarily estimate the function call coverage. Static code coverage analysis is much faster and simpler since the generated object file need not be executed. This is particularly handy in the case of small scale embedded systems.   Dynamic code coverage analysis is much more elaborate and requires the test code to be executed on the target. It also requires the object file to be generated with special compilation options. However, it gives much more detailed analysis of how effective the test is.   For obvious reasons, it is not practical to manually compute the coverage of a test code. Thus we have some tools that can compute the coverage of our test code for us. We will now look into the details of how ‘gcov’ can be used for dynamic code coverage analysis.   GCOV   As per Wikipedia, Gcov is a source code coverage analysis and statement-by-statement profiling tool. Gcov generates exact counts of the number of times each statement in a program is executed and annotates source code to add instrumentation. Gcov comes as a standard utility with GNU CC Suite (GCC)   Gcov provides the following details:      How often each line of code executes   What lines of code are actually executed   How much computing time each section of code uses   Getting Started With GCOV   When using gcov, you must first compile your program with –coverage GCC option.   This tells the compiler to generate additional information needed by gcov (basically a flow graph of the program) and also includes additional code in the object files for generating the extra profiling information needed by gcov. These additional files (.gcno) are placed in the directory where the object file is located.   Once the object file is generated, execute it once to get the profile file (.gcda)   Once we have the gcna and gcda files , we can now run gcov.   To illustrate the usage of gcov, we will consider a very minimal library (lib.c) and it test suite (test.c) .   lib.c   int libfn1() {     int a =5;     a++;     return (a); }  int libfn2( int b) {     if (b&gt;10)     {         libfn1();         return(b);     }     else     return(0); }   test.c   #include &lt;stdio.h&gt; extern int libfn1();  int main () {      libfn1();      libfn2(5); }   Compilation command for the test code :   gcc --coverage lib.c test.c –o test   This will generate the following files:      lib.gcno  –  library flow graph   test.gcno – test code flow graph   test      – test code executable   Now, execute the test code object file. This will generate the following files      lib.gcda – library profile output   test.gcda – test code profile output   Now we have all the inputs required for gcov to generate the coverage report. To generate the coverage report, run the following command   gcov -abcfu lib.c   Coverage summary will be displayed as below when gcov finishes execution:   Function 'libfn1' Lines executed:100.00% of 4 No branches No calls Function 'libfn2' Lines executed:60.00% of 5 No branches No calls File 'lib.c' Lines executed:77.78% of 9 Branches executed:100.00% of 2 Taken at least once:50.00% of 2 No calls lib.c:creating 'lib.c.gcov'   Detailed coverage report will be available in the lib.c.gcov file generated by gcov                                         Each block is marked by a line with the same line number as the last line of the block and the number of branch and calls in the block.   Each function is preceded with a line showing the number of times the function is called, number of times it returns and the percentage of function blocks that were executed.   Each line of executable code contains the number of times the line was executed and the actual source code line number. Any line that was not executed will have ##### in place of the execution count. Blocks that are not executed will have $$$$$ in place of the execution count.   The command line option summary for gcov is given below:                  -h, –help       Print this help, then exit                 -v, –version       Print version number, then exit                 -a, –all-blocks       Show information for every basic block                 -b, –branch-probabilities       Include branch probabilities in output                 -c, –branch-counts       Given counts of branches taken rather than percentages                 -n, –no-output       Do not create an output file                 -l, –long-file-names       Use long output file names for included source files                 -f, –function-summaries       Output summaries for each function                 -o, –object-directory       DIR\\FILE Search for object files in DIR or called FILE                 -p, –preserve-paths       Preserve all pathname components                 -u, –unconditional-branches       Show unconditional branch counts           Visualizing results with LCOV   The Linux testing project (LTP) has come up with a tool called lcov that provides a more user-friendly graphical visualization of the gcov output. It generates html files and integrates well with web based CI systems.   To make lcov generate html reports for you, give the following commands once the gcno and gcda files are generated.   lcov --directory . –zerocounters   lcov --directory . --capture --output-file app.info   genhtml app.info                                               LCOV report         The lcov reports seen from a browser is shown in the screenshots below.   Cross Profiling   This section is adapted from the GCOV manual :   As we have seen so far, running the program to be profiled will cause profile output to be generated. For each source file compiled with the –coverage option, a .gcda file will be generated in the object file directory. This places a restriction that the target system should have the same directory structure. (The program will try to create the needed directory structure, if it is not already present).   As per the gnu GCOV documentation, redirection can be done with the help of two execution environment variables.      GCOV_PREFIX : Contains the prefix to add to the absolute paths in the object file. Prefix can be absolute, or relative. The default is no prefix.   GCOV_PREFIX_STRIP : Indicates the how many initial directory names to strip off the hardwired absolute paths. Default value is 0.   Note: If GCOV_PREFIX_STRIP is set without GCOV_PREFIX is undefined, then a relative path is made out of the hardwired absolute paths.   For example, if the object file /user/build/foo.o was built with -fprofile-arcs, the final executable will try to create the data file /user/build/foo.gcda when running on the target system. This will fail  if the corresponding directory does not exist and it is unable to create it. This can be overcome by, for example, setting the environment as GCOV_PREFIX=/target/run and GCOV_PREFIX_STRIP=1.  Such a setting will name the data file /target/run/build/foo.gcda.   You must move the data files to the expected directory tree in order to use them for profile directed optimizations (–use-profile), or to use the gcov tool.   ","categories": ["Articles","Tutorial"],
        "tags": ["GCOV"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/test-coverage-analysis-with-gcov/",
        "teaser": null
      },{
        "title": "Advanced Shell Scripting",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   Shell scripting refers to an automated way of interacting with a computer system over command line interface (CLI) . This article deals specifically with scripting with the bash shell   Shebang   Shebang is the method used in bash scripting to declare the interpreter to be used for the rest of the script. It is represented as #!. eg:   #!/bin/sh   Execution   When interacting with external programs or scripts (referred to as, file in general) shell scripts uses three major execution modes   source   In this mode, the external file is executed in the current shell environment. Exit status of the last executed command is passed on as the return value. eg:   source ./child.sh   or simply   . ./child.sh   eval   All arguments to eval are concatenated into one string and executed as a new command. eg:   eval \"hello $USER\"   exec   The command specified as the argument to exec will replace the shell . The arguments becomes arguments to the command.   Parallel Processing   Spawning parallel processes   A task can be run in the background by appending an &amp; to the end of the command. eg:   ./child1 &amp; ./child2 &amp;   A waiting parent   Once child a process is spawned, the parent has to wait until the child process returns. To wait for all children, put a wait at the end of the script. eg:   ./child1 &amp; ./child2 &amp; wait echo \" exiting since all children are done\"   To wait for a specific child, the child PID has to be passed to the wait command. The child PID can be fetched from the $! variable as soon as the child is spawned. eg:   ./child1 &amp; PID1=$! ./child2 &amp; PID2=$! wait PID1 PID2   To fetch the return values of the children, use the $? variable immediately after the wait for the corresponding child. eg:   ./child1 &amp; PID1=$! ./child2 &amp; PID2=$! wait PID1  RET1=$? wait PID2 RET2=$?   Irrespective of when the child finished processing, the return value will be collected when wait is called on its PID   Pitfalls   If the parent is killed while waiting for the children to exit (say, by a SIGINT (ctrl+c)), the children becomes Zombies and will start to eat-up system resources. To avoid this, install a trap to catch the probable kill signals.eg:   trap “kill -9 $child; echo child killed;”  SIGHUP SIGINT SIGTERM   Colorizing   To get color prints in the shell, use the following syntax   echo -e \"33[COLORm Hello World\"   or   printf \"\\e[COLORm Hello World\"   List of colors available are                  color       foreground       Background                       Black       30       40                 Red       31       41                 Green       32       42                 Yellow       33       43                 Blue       34       44                 Magentha       35       45                 Cyan       36       46                 White       37       47           For example, to print a text in red with yellow background, and then reset the colors back to white text in black background , one fo the following can be done:   echo -e \"33[31;43m Deleting33[37;40m\"   printf \"\\e[31;43m Deleting\\e[37;40m\"   Further possible effects are                  ANSI Code       Meaning                       0       Normal                 1       Bold                 4       Underline                 5       Blink                 7       Reverse Video           For example, to print bold red text in yellow background and then reset the colors back to white text in black background , one of the following can be done.   echo -e \"33[1;31;43m Deleting33[0;37;40m\"   printf \"\\e[1;31;43m Deleting\\e[0;37;40m\"   Outliving the sessions   When a session terminates, all processes that started from the session will receive a SIGHUP and will thus be terminated . However, the session can be outlived in one of the two ways.           Install a SIGHUP trap at the beginning of the script     trap \"echo 'I wont go down'\"  SIGHUP            Run your process under nohup         nohup is a POSIX command that will do the SIGHUP trapping for you.          #nohup &lt;scriptName.sh&gt; &amp;            disown the job (background process)     If the process to outlive the session is already in the background, it can be “disowned” so that the session termination will not terminate the process. one fo the following can be done for this.       #my_script.sh &amp;   disown &lt;PID of my_script.sh&gt;              #my_script.sh &amp; disown             I will keep adding more or these interesting  shell scripting stuff here. So, keep tuned . . .  ","categories": ["Articles","Tutorial"],
        "tags": ["shell scripting","linux"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/advanced-shell-scripting/",
        "teaser": null
      },{
        "title": "Booting your first cross-compiled Linux based embedded system",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   This tutorial introduces you to the world of Linux based embedded systems. Here we will learn to cross-compile your own Linux kernel and bring up a complete system with that kernel. And the best part is, you do not have to own a development board or brick your PC while learning this.   The main components involved in our tutorial today are      Buildroot   Qemu   Qemu is the emulator that will help up work around the requirement of a physical development machine by providing a virtualized environment to bring up our kernel without putting your actual system in “harms way” :smile:   Buildroot is basically a set of scripts the that will automate the process of cross-compiling the kernel and tool-chain for you.   First, to get the basics right, To bring up any software in any new platform, you first need to have a tool chain that includes libraries, compiler, assembler, linker etc that supports your platform. In our case, the platform is a virtualized environment. So, we first need to cross-compile the tool chain itself. so that it can generate binaries sporting the platform.   Following this, we need to compile the Linux kernel code using the cross-tool chain that we generated to produce a “Linux kernel boot executable” for the platform.   Now, in a general sense, the kernel needs a root file system (RFS) to boot up and be usable. (We will discuss the details of why this is required in a later session)   Buildroot does all the above for you automatically. We just have to tell the details of the platform to buildroot and it will generate the tool-chain, kernel image and minimal RFS for you.   For this, first download the latest buildroot from here. At the time of writing buildroot-2012.11.1 is the latest and greatest. Once you download the archive, un-compress it and go inside the folder.   The list of platforms with ready made support is in the configs directory.   In this tutorial , we are going to build a system around qemu_x86_defconfig. So, give the following set of commands.   make qemu_x86_defconfig    make   The first command will generate the .config file used by buildroot and the second command will download all necessary source packages and compile the actual images for you. (This will take some time:). I got the following time stat while compiling in 16x build server with 12 parallel jobs )   real 14m57.942s user 39m29.166s sys 5m57.716s   Once the build is done, you can go to the output/images folder and find the two output files:      bzImage: Linux kernel x86 boot executable RO-rootFS, swap_dev 0x1, Normal VGA   rootfs.ext2: Linux rev 0.0 ext2 filesystem data   The toolchain will be at output/host/usr/bin   Buildroot generates a toolchain based on uclibc which is a fully functional, yet size optimized avatar of glibc.   Now, we need to fire-up qemu. Here Iam going to show a GUI method to bring qemu up on Windows. However, the steps are similar in Linux based systems also.   Download Qemu Manager here and install it. Open the qemu manager UI and do the following configuration.   Create a new Virtual machine named QemuLinuxTest with “Standard x86/x64 PC as the platform and “Linux Distribution” as the operating system.                                         For now, choose “Do not use a Virtual disk image”                                                                                 Once this is done, Select the virtual machine from the panel in the left and do the following configuration.                                         Point “HardDisk 0” to the generated RFS image                                         Enter the kernel path and the additional command line append root=/dev/sda in the advanced tab.                                         Now, click the start icon and see your own cross-compiled kernel coming to life :smile:                                         Tweaks           Even without qemu manager, you can manage to run the virtual machine by issuing the following command       qemu-system-i386 -M pc -kernel output/images/bzImage -drive file=output/images/rootfs.ext2,if=ide -append root=/dev/sda                In case you are working in a non-graphic environment, you can use -curses option. In this case, use the poweroff command to return to your system.            To make changes to the default configuration (qemu_x86_defconfig) issue the following command after [make qemu_x86_defconfig]       make menuconfig            To make changes to the linux kernel configuration , issue       make linux-menuconfig            To make changes to the uclibc configuraition, issue       make uclibc-menuconfig      ","categories": ["Articles","Tutorial"],
        "tags": ["embedded linux","linux","kernel"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/booting-your-first-cross-compiled-linux-based-embedded-system/",
        "teaser": null
      },{
        "title": "Adding new menu item to PuTTY",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   I came across a requirement to add a new easy access menu item to the putty terminal window for Windows. This article explains how I did it.   Requirement   I collect a lot of debug logs for my Linux development boards while working with some new problem. While I have some environment constraints that force me to work with my Windows Box, I find it difficult to split the log files just before a particular scenario occurs (so that it will be easier to identify what went wrong)   The general approach I take is to go change the logging in putty re-configuration to none, apply it and then change it again back to “All Session Output”. But doing this every time was not fun.                                         So, I decided to include a  new menu item that automatically does this.   The problem was, there are no architecture documentation available for PuTTY (at least none that I came across). So, I had to dig through the code and here is how I achieved my objective   Code   I used the latest release source code for Windows available here   I used the express version of MS Visual Studio 2012 available for download here   The code changes I made are      Define the  SECURITY_WIN32 macro in putty.h to avoid compilation errors   #ifndef SECURITY_WIN32 #define SECURITY_WIN32 #endif   2) define the new SYSCOMMAND message in windows.c   #define IDM_RELOG    0x0080   3) Add the menu item in WinMain() of windows.c   AppendMenu(m, MF_ENABLED, IDM_RELOG, “RESTART LOG FILE”);   4) Add the callback handle code to WndProc() of windows.c   case IDM_RELOG: { \tcfg.logtype=LGTYP_NONE; \tlog_reconfig(logctx, &amp;cfg); \tcfg.logtype=LGTYP_DEBUG; \tlog_reconfig(logctx, &amp;cfg); } break;   Compile the code, and you now have a new menu item to restart the log file                                         ","categories": ["Articles","Tutorial"],
        "tags": ["putty","putty menu","putty customisation"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/adding-new-menu-item-to-putty/",
        "teaser": null
      },{
        "title": "XML parser Pitfalls",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   In the wake of technology revolutions like Internet of Things (IoT), XML is gaining more importance than ever as the language of inter-platform communication. Most embedded system developers who use XML in their system write their own parser to limit the footprint. However, XML can very easily become the point of vulnerability in your system if you are not careful to include some sanity checks.   In this article, I will introduce some of the well-known XML based attacks and will discuss some of the tips to avoid the pitfalls   Billion Laughs   Consider the following xml document injected into your system by a malicious user   &lt;?xml version=\"1.0\"?&gt; &lt;!DOCTYPE lolz [ \t&lt;!ENTITY lol \"lol\"&gt; \t&lt;!ENTITY lol1 \"&amp;lol;&amp;lol;&amp;lol;&amp;lol;&amp;lol;&amp;lol;&amp;lol;&amp;lol;&amp;lol;&amp;lol;\"&gt; \t&lt;!ENTITY lol2 \"&amp;lol1;&amp;lol1;&amp;lol1;&amp;lol1;&amp;lol1;&amp;lol1;&amp;lol1;&amp;lol1;&amp;lol1;&amp;lol1;\"&gt; \t&lt;!ENTITY lol3 \"&amp;lol2;&amp;lol2;&amp;lol2;&amp;lol2;&amp;lol2;&amp;lol2;&amp;lol2;&amp;lol2;&amp;lol2;&amp;lol2;\"&gt; \t&lt;!ENTITY lol4 \"&amp;lol3;&amp;lol3;&amp;lol3;&amp;lol3;&amp;lol3;&amp;lol3;&amp;lol3;&amp;lol3;&amp;lol3;&amp;lol3;\"&gt; \t&lt;!ENTITY lol5 \"&amp;lol4;&amp;lol4;&amp;lol4;&amp;lol4;&amp;lol4;&amp;lol4;&amp;lol4;&amp;lol4;&amp;lol4;&amp;lol4;\"&gt; \t&lt;!ENTITY lol6 \"&amp;lol5;&amp;lol5;&amp;lol5;&amp;lol5;&amp;lol5;&amp;lol5;&amp;lol5;&amp;lol5;&amp;lol5;&amp;lol5;\"&gt; \t&lt;!ENTITY lol7 \"&amp;lol6;&amp;lol6;&amp;lol6;&amp;lol6;&amp;lol6;&amp;lol6;&amp;lol6;&amp;lol6;&amp;lol6;&amp;lol6;\"&gt; \t&lt;!ENTITY lol8 \"&amp;lol7;&amp;lol7;&amp;lol7;&amp;lol7;&amp;lol7;&amp;lol7;&amp;lol7;&amp;lol7;&amp;lol7;&amp;lol7;\"&gt; \t&lt;!ENTITY lol9 \"&amp;lol8;&amp;lol8;&amp;lol8;&amp;lol8;&amp;lol8;&amp;lol8;&amp;lol8;&amp;lol8;&amp;lol8;&amp;lol8;\"&gt; ]&gt; &lt;lolz&gt;&amp;lol9;&lt;/lolz&gt;   When the parser expands this, lol9 will be expanded to ten lol8. Now, lol8 expands to ten lol7. This goes on to expand into a huge XML containing 109 lols and will eat up all your systems memory.   This attack is also known as XML bomb or exponential entity expansion and many standard XML parsers consider entities expanding to entities as an entity loop and terminates parsing.   Quadratic blowup entity expansion   A parser is hardened against exponential entity expansion, allowing only one level of entity expansion, will not be able to catch an extremely long entity being referred multiple times. Though not as bad as an XML bomb, this type of attack still pose a serious threat if entity size limit restrictions are not imposed. External entity expansion   Consider the following xml entity.   &lt;!ENTITY myVal SYSTEM    \"http://www.example.com/myVal\"&gt;   This seemingly harmless entity requests the parser to pull the value of myVal from an external resource. However, at the point of parsing, we do not know what the resource is going to return (or whether it will ever return at all. A user with malicious intent can point this external resource to something that never returns. In that case, the parser will be stuck until the resource is available (or a timeout if your code is smart enough). Even worse is when the resource returns a huge amount of data. The resource need not even be engineered to generate infinite data but can rather point to some huge file – say, a movie download. If the attacker has knowledge of your system internals, he can even point to some resource in localhost.   Solutions to this problem are:      Enable entity expansion in your parser only if it is necessary   Engineer your download time-outs wisely   Restrict to downloads from known hosts if possible   Disable entity resolution to localhost if they are not necessary   DTD retrieval   This is a specific case of external entity retrieval where the entity is the seemingly harmless document type definition. DTD basically defines the data that you can expect in an XML document and is often used as a resource linked to the original XML.   Along with other protection mechanisms, it is preferable to download a DTD only if it is absolutely necessary.   Decompression bomb   This is also known as zip bomb or zip of death and is applicable to XML parsers that are capable of handling compressed stream of data like gzipped HTTP streams. A malicious attacker can engineer a compressed file to be such that it takes considerable amount of resources to un-compress it.   One of the well-known zip bombs is 45.1.zip, a 45.1 KB file that expands to around 1.3 exa Bytes of uncompressed data. It is created by compressing a 1.3GB file full of zeros into a zip file and compressing ten copies of it into another zip file and repeating the process for 9 times.   If it is necessary for your system to deal with compressed data, ensure that proper buffer size limits are imposed so that your system will not hangup trying to uncompress exabytes of data.  ","categories": ["Articles","Tutorial"],
        "tags": ["XML","IoT"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/xml-parser-pitfalls/",
        "teaser": null
      },{
        "title": "cscope + vim for code navigation",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   When you are working with a large code base, you need a powerful tool to navigate through the code seamlessly . There are many paid tools put there that does this efficiently and for the open source enthusiast in you, you have OpenGrok that gives and web interface to your code and Cscope that provides a command line interface to code navigation.   In my project where I had to make changes to parts of code that includes thousands of file that pours in from teams across geographies, I found it difficult to open up Cscope each time I had to find the reference to a variable or search for a definition. Then I came across this tutorial in the cscope website that illustrates how we can integrate cscope with my favorite editor – vim .   With cscope integrated vim (and some getting used to), code navigation is easier, faster and all the more interesting. In this tutorial, I will describe how to integrate vim and cscope in quick steps and provide some shell functions that will make your life easier.   NOTE: It goes without saying that this is all applicable for non-GUI code editors like me who prefer to use FOS tools for your day to day activities   STEP 1:   Install vim and cscope . – – – you figure out how to do it :stuck_out_tongue:   STEP 2:   Download the cscope vim map from here and copy it to your ~/.vim/plugin folder   I had to move the set nocscopeverbose line from the end of CSCOPE_DB addition branch to the beginning of the branch to avoid some prints at vim start-up . (from line 49 to above line 33 in teh 2002/3/7 version of the file)   STEP 3:   Add the following functions to your ~/.profile (or ~/.bashrc) file   function build_cscope_db_func() { \tfind $PWD -name *.c \\ \t\t-o -name *.h \\ \t\t-o -name *.mk \\ \t\t-o -name *.xml\\ \t\t-o -name *.cfg\\ \t\t-o -name *.ini\\ \t\t-o -name *.dat\\ \t\t-o -name *.cpp &gt; $PWD/cscope.files \t\tcscope -RCbk \t\texport CSCOPE_DB=$PWD/cscope.out } alias csbuild=build_cscope_db_func  function cscope_export_db_func() { \texport CSCOPE_DB=$PWD/cscope.out } alias csexport=cscope_export_db_func   Once this is in place , save the file and import it into your environment by executing . ~/.profile .   NOTE: you can alter the type of files to be included in cscope parsing in the above function to suite your needs   STEP 4:   Now go to the root folder of your code and execute csbuild. This operation might take some time depending on your code size and type of files included in the csbuild function. This operation is valid until there is code change to be parsed.   STEP 5:   Now open vim and you are ready to navigate your mighty code base. To start with , suppose you want to fins the file main.c, issue the vim command ` :cs f f main.c ` [i.e. cscope find files of name main.c]   Another commands that can be issued ` :cs f s main  ` to find all references to the symbol main. List of all commands will be listed on issuing ` :cs h ` to vim.   Once a file is open, to perform the operations (like “s” to find all references to a symbol under the cursor), while in vim command mode, do a ` ctrl + \\    s`. Same goes for other commands (like f to open a file with name under the cursor)   STEP 6:   Once a cscope database is built (using csbuild), its link to vim is valid only until the session is active (since it is controlled using an environment variable). Once you open a new session, you have to go to the location of the cscope.out file (root folder of your code) and do a csexport.   This is also valid when you want to switch between projects (code bases). csbuild can be executed on all project roots and csexport can be used to switch between projects  ","categories": ["Articles","Tutorial"],
        "tags": ["CSCOPE","vim"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/cscope-vim-for-code-navigation/",
        "teaser": null
      },{
        "title": "Writing Linux Device Drivers - Part 1",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   Step 1:- Setup   This is the most important component that you require to start writing Linux device drivers. I use an Oracle VirtualBox with a 32 bit Ubuntu desktop image . This is essential when you start experimenting with kernel code and drivers since mistakes like a simple pointer corruption can even wipe your disk.   On this VM, install GCC (if not already present) and the linux kernel headers. I am using an older version of the kernel (found out by the command ” uname -a “). So I used the following command to get the headers.   sudo apt-get install linux-headers-3.5.0-17-generic   Step 2 :- Compilation environment   To begin with, we will create a blank kernel module and get it compiled. This will prove the compilation environment and we will also try inserting it into the kernel.   You will need the following files and commands to do this.      blank.c   #include &lt;linux/module.h&gt; MODULE_LICENSE(\"Dual BSD/GPL\");      Makefile   obj-m := blank.o   Use the following command to compile the module:   make -C /usr/src/linux-headers-3.5.0-17-generic M=$PWD modules   The -C option is to point to the root folder of compilation and the M= option is to point to the location at which the compilation has to happen.   This will give a blank.ko which can be inserted into the kernel using the command ` insmod blank.ko . Once this is done, insertion can be verified by issuing the command  lsmod . To remove the kernel object, use the command  rmmod blank.ko . The inserted module and its major number can also be seen from /proc/devices` file.   Step 3 :- your first kernel print   Now we will make a kernel object that just prints some debugs when inserted and ejected. This illustrates how the kernel loads the drivers into itself.   You will need the following files:      simple.c   #include&lt;linux/module.h&gt; #include&lt;linux/init.h&gt; #include&lt;linux/kernel.h&gt; MODULE_LICENSE(\"GPL\");    /*else kernel complaints about kernel taint*/ int simple_init(void) { \t printk(KERN_ALERT\"Hello kernel space world\\n\");   /*Kernel mode printf - messages appear in /var/log/messages . or use command dmesg | tail*/ \t  return 0; } int simple_exit(void) { \t printk(KERN_ALERT\"Bye bye kernel world\\n\"); \t  return 0; } module_init(simple_init);    /* register the module init function to be executed on insmod*/ module_exit(simple_exit); /* register the module cleanup/exit function to be executed on rmmod*/      Makefile   obj-m := simple.o   Use the same make command as above to compile the kernel module   On inserting the module, simple_init will be executed by the kernel and it will print the welcome message into kernel logs. On removing the module, simple_exit will be called. When the module is inserted, its presence can also be verified using /proc/modules (other than lsmod)   Step 4 :- associating modules to devices   The beauty of Linux character devices is  that you can access a device and its buffers as if you are accessing a regular file (with some restrictions). What happens internally is that Linux maps generic system calls like open, close, read, write etc that targets a file into functions provided by the corresponding device driver. In the following example, this is exactly what we are doing. Since the actual device driver file is quite big, I have placed it here. I will highlight the major APIs and the flow here. [Makefile is available here. Issue the same command as above to build the ko]   Ultimately, the device.ko that is inserted will be used to perform operations in the kernel space when certain system calls are made. Now, we need two items to achieve this – a way to link a device file to the ko and actual functions to perform the system calls   The mapping of system calls is done using a structure (device_fops) of type file_operations. This is further used to register the device into the kernel. So, for example, when an open systemcall is made the kernel will call device_open.   Now, there should be a unique identifier that can be used to identify the ko for it to be linked to a device. Since a single ko can handle multiple devices, module name is not appropriate. So, we use the concept of major number and minor number. A major number identifies a ko while a minor number identifies the devices it controls. We will be dynamically allocating major and minor numbers.   Once a module is inserted, we need to create a device node/file and associate it with the ko. For this, first read the allocated major and minor numbers from /proc/devices and use the mknod command to create the device file.   device.c flow – device_init()           allocate device number with alloc_chrdev_region            initialize and add the cdev structure using cdev_add            while exiting, do unregister_chrdev_region and cdev_del       You can void manually creating the device node by using the class_create and device_create APIs from the ko itself. Make sure you do a class_destroy and device_destroy on exit. Sample code can be found here. You still need to provide permissions to the device file using chmod  ","categories": ["Articles","Tutorial","Device drivers"],
        "tags": ["Linux","Kernel","Drivers"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/device%20drivers/writing-linux-device-drivers-part-1/",
        "teaser": null
      },{
        "title": "Writing Linux Device Drivers - Part 2",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   The first part of this article is available here.   In this second part we will discuss some of the advanced topics related to writing Linux device drivers   Associating multiple devices to same module – method 1   The same kernel module can be used to associate functionality to different devices. This is implemented using the concept of major and minor numbers. One kernel module will be identified by the major number and the different devices that can be controlled controlled by the module will have different minor numbers.   When different devices to be controlled by the same module are created using mknod, different minor numbers are provided. When an operation is performed on the file, the minor number of the device can be identified from the inode structure that is passed into the module. (if inode is not directly passed in, it can be read from the file structure pointer as filep-&gt;f_dentry-&gt;d_inode). The minor number is obtained using the macro iminor().   Once you compile the module here and insert it, you have to create two nodes using the following comand: sudo mknod filep1 c 250 1 and sudo mknod filep1 c 250 0. The module implements a forked read that returns a message according to the device minor number of the node in use.   Associating multiple devices to same module – method 2   If the functionality to be implemented on the fops for different minor numbers are very much different, we can have a different fops set for different minor numbers. This is done by tweaking the f_op stored in the file structure pointer upon open based on the minor number used in open. This is illustrated in the module code here   Procfs   Creating entries in proc file system is a common way to provide quick information about the modules that are inserted in the kernel. Some of the popular procfs entries created be the kernel are self, cpuinfo, modules etc ehich are used by many user space applications to report what is happening within the kernel. Though it is not a good practice to clutter the procfs with custom entries, it is a good starting point to understand how creating an entry there works.   A simple read entry in procfs is very  much similar to the read implementation for character devices. proc_create can be used with a file_operations pointer to create the entry as illustrated here. The easiest (not the best) way to put this entry inside a directory is to first create the directory before inserting the module and include the full path in the create call.   However, more complex mechanisms are required for larger procfs entries.  ","categories": ["Articles","Tutorial","Device drivers"],
        "tags": ["Linux","Kernel","Drivers"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/device%20drivers/writing-linux-device-drivers-part-2/",
        "teaser": null
      },{
        "title": "MQTT for IoT - a quick hands-on trial",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   MQTT is a light weight, opensource, publish-subscribe protocol designed with small devices and IoT in mind. MQTT stands for Message Queue Telemetry Transport and aims at devices with a small code footprint requirement. The smallest valid packed is 2 bytes in length with the fixed header.   MQTT is primarily designed to work over TCP/IP with optional TLS security. However, there also exist a variant called MQTT-SN for sensor networks over non-TCP transports like Zigbee.   Facebook mobile messenger is one of the big players who has adopted MQTT. It is used for functionality like presence notification to limit bandwidth usage and enhance battery life.   Hands-on   There are numerous MQTT broker and client libraries of varying complexities available out there. I used the Mosquitto MQTT broker and client to get a feel of this protocol.   For ubuntu, Mozquitto broker and client are available from a  standard repository. To add the repo, and install, use:   sudo apt-add-repository ppa:mosquitto-dev/mosquitto-ppa   sudo apt-get install mosquitto mosquitto-clients   To start the server, use   mosquitto -d   This will start the server in a daemon mode at TCP port 1883   Now, some basics of MQTT pub/sub model           publish and subscribe are done to topics arranged in a tree. The node separator is /\t. So, topics can be like books/science, books/art, books/science/physics, books/science/chemistry       Subscription can be to a specific topic of a wildcard topic tree            + represents a single node. So, +/science can refer to “books/science” or “papers/science”       # represents a node set (like * in regex). So, books/# refers to books/science as well as books/art           There is no need to create a topic. As soon as a pub/sub is done to a topic, it is created   Time to get your hands dirty. In a terminal session, run the following command to subscribe to books/science/+   mosquitto_sub -h localhost -p 1883 -t “books/science/+”   Now, to publish to the topic, open another terminal and run:   mosquitto_pub -p 1883 -h sid -m “physics” -t “books/science/physics”   mosquitto_pub -p 1883 -h sid -m “physics” -t “books/science/chemistry”   Both messages will be received at the subscription terminal.   If the messages are published with a -r flag, the last published message will be retained and any subscriber to the topic will receive it as soon as they subscribe to the topic.   A publisher can set a will and will-topic when it first connects to the server. This will will be published in the will topic if the publisher disconnects unexpectedly   In the example here, we did not enforce security. However, SSL and user based authentication is provided for by the protocol and supported by Mosquitto implementation  ","categories": ["Articles","Tutorial"],
        "tags": ["MQTT","IOT"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/mqtt-for-iot-a-quick-hands-on-trial/",
        "teaser": null
      },{
        "title": "Building a RaspberryPi daughter card - warnings and pitfalls",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   Raspberry Pi is a $35, credit card sized, single board computer that can run Linux and decode HD videos . There are many articles that describe how to bring up the board and play around with interfaces and GPIO. This article is for the DIY enthusiast who wants to extend the functionality of the “Pi” by building your own hardware around it.   I made a lot of silly mistakes wile I fabricated my first board at home. So, here I am sharing the experience I gained so that you wont repeat them in your trials.   Spec   The first problem I faced was my over-ambitious spec I had. Some of my key requirements where      Single layer routing for toner transfer fabrication at home (being the first version).   Small form-factor : even after attaching the card, I should be able to enclose the unit in its case.   28 pin pic micro controller running on a 20Mhz crystal oscillator with ICSP . [DIP package since that is all I had :smile: ]   2 user input switches, 3 LEDs, 1 potentiometer (A/D), SPI and UART connectivity for the µC   After spending 2 sleepless nights with eagle CAD free version, trying at least a hundred different placement combinations, manual and automated routing I realized the hard way that I have to admit defeat, or at least a partial one. All my specs can’t be met :weary: .   The constraints where:      The final board size was predefined by requirement 2.   Routing grid and clearance where defined by requirement 1. (especially by the toner transfer method that I chose )   These two made it impossible to accommodate all my peripherals. The saddest part is, I had enough experience and expertise in this kind of task and still I got excited and jumped ahead .   Finally I revised my spec to eliminate the pot and user switches and decided to provide the functionality from the  μC.   The lesson:  Do some analysis on paper to save time and hair :smile: . Patience is key to success   Placement   This daughter card had few specialties brought in by the fact that it is to be attached on top of an existing board. an initial mistake I made is not accounting for the irregular z-axis dimensions of components placed on the Pi. In the picture below, the yellow on black thatched area shows the clearance that is actually available on my board.                                               Diagram created in a combination of FreeCad and MS Word         While designing your boards, keeping track of the clearance after placement is critical for the final board to sit snugly on the  Pi. I had to move the PIC a bit to the right to accommodate the power trans that stands proud on the component line. Using a vero-board to simulate the placement  of components will be a fruitful exercise.   The lesson: A vero-board is your friend to simulate component clearence   Orientation   My board had some of the “user-passive” components like teh PIC placed under the board [i.e the component side], while the “user-active” components like LEDs and reset switch placed on trace side of my single layer board. [The LEDs and switches where SMD packages]  This presents a challenge in the placement orientation of the components for a single layered toner transfer design.   In case where there is a mixture of I generally do all my rouging in the top layer (red). Once this is decided, I place all my SMD components “as-is” [No flipping, no turning]. Then I placed the 2×13 row header [i used a con-harting 13] that connects to the Pi. This had to be flipped as seen in the image below so that the connector aligns properly when inserted.                                         The PIC part could be placed as such since the flip orientation is automatically taken care of.   Using the “set pad_names on” command in eagle is very useful in these scenarios.   The lesson: Visualization is key to orientation success.   Pads   As you can see in the header picture above, the pads are very thin. Working with this type of pads in toner transfer method and hand soldering is a nightmare . Having to hand drill the pad holes makes things even more challenging. The easiest way to overcome this is to edit the parts to suite our requirement. This is much more easier than designing parts from scratch.   To edit a part, right click on the aprt and select open package. Then make the required changes, save and replace the existing part with the new part. The diagram below shows a partially edited part                                         The lesson: understand your design constraints and re-use existing parts to suite your needs.   My Board   This is a pic of my final board                                         ","categories": ["Articles","Tutorial"],
        "tags": ["Raspberry Pi","PCB"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/building-a-raspberrypi-daughter-card-warnings-and-pitfalls/",
        "teaser": null
      },{
        "title": "Buildroot and Raspberry Pi",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   I reticently got a RaspberryPi on loan and started exploring the options. Being an embedded guy, I did not want to go with the miniature computer concept where you write and compile your programs in the target (RPi) itself. Rather, I wanted to cross compile and boot my own kernel and bring up a minimalist system up on the RPi .   Compiling and booting the image   To start with, I looked up buildroot support for the Pi and was happy to find a default configuration on the buildroot snapshot  The last one I checked is here. Following are the steps to get started:      Download Buildroot and uncompress it to a work folder   Build the configuration with the command make raspberrypi_defconfig   Build the image with the command make. This will take long time to download the sources and compile the toolchain and images . (took around 1.5 hours in my old laptop)   Once the build is completed, you can find all the required files in output/images folder   Now, partition your SD card with two partitions (I use gparted to avoid nuking my hard disks by mistake)            The primary fat32 partition marked as bootable – to hold the primary boot data       The second ext4 partition to hold by root file system           Copy zImage and all files in rpi-firmware folder to the primary partition            By default , serial console is not supported in the generated image. So, open your commandline.txt and append the following to the existing line:    console=ttyAMA0,115200 kgdboc=ttyAMA0,115200           login as root and untar the contents of rootfs.tar to the second partition            sudo tar xf rootfs.tar -C /media/partition2       To enable a login console via serial port, add the following line in etc/inittab after copying the files:   ttyAMA0::respawn:/sbin/getty -L ttyAMA0 115200 vt100           sync your write buffers with “sudo sync” and unplug your SD card when done   Boot your RPi with your own kernel and enjoy.   Using the serial interface   I used the minicom interface with a USB to serial converted in my Laptop that has ubuntu installed. To configure minicom, start minicom with sudo minicom -s. Configure the port to use /dev/ttyUSB0 with 115200 baud, 8N1 (8 bits, no parity and one stop bit) and no flow control (hardware or software). For further use, just issue sudo minicom   Device driver support   The minimalist kernel we compiled before has the bare minimum configuration to bootup Linux in the RPi. To support additional hardware, we need to enable additional drivers. We will look into the steps to enable some of the drivers and using them.   1) USB Mass storage driver   Open the Linux configuration from buildroot work directory using make linux-menuconfig.   In the device drivers section, enable SCSI device support and SCSI disk support as builtin (not “M”odule)   In the USB Support section, enable USB Mass storage support as builtin. Save the configuration and exit   Recompile the required parts (kernel alone) by issuing make. Since we have enabled the modules to be supported as builtin, only the zImage needs to be copied into the SD Card.   To verify that everything works fine, boot up your Pi with the new kernel and plug in a USB Flash drive. On giving the dmesg command, you should be able to find the device not name (SDA, SDB etc) from the logs. You can mount the device to a folder and start accessing the flash drive content.   2) Onboard LEDs   The green on-board LED can be controlled via a sysfs entry if the appropriate kernel module is available.   To enable driver support for the LED, compile the kernel with GPIO LED and LED trigger modules enabled in the kernel configuration   Once the kernel is compiled and “installed” with these drivers, the led trigger can be controlled by writing to the sysfs entry:   cd /sys/devices/platform/leds-gpio/leds/led0 cat trigger   This will list all the triggers available for the led device. To switch it on do:   echo default-on &gt; /sys/devices/platform/leds-gpio/leds/led0/trigger   and to switch off,   echo none &gt; /sys/devices/platform/leds-gpio/leds/led0/trigger   After each operation, you can cat the trigger file and see the change reflecting.   My next post will illustrate how an end to end system was built using this kernel and rootfs.   Cheers  ","categories": ["Articles","Tutorial"],
        "tags": ["Buildroot","linux","Raspberry Pi"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/buildroot-and-raspberry-pi/",
        "teaser": null
      },{
        "title": "Building an E2E IoT system with Raspberry Pi",
        "excerpt":" This article has been migrated from my original post  at embeddedinn.wordpress.com.   Internet of things (IoT) is the future of embedded systems . Though it is nothing new (technically), it brings together advances in hardware engineering, software architecture, networking etc into a small package that makes our life easier and exciting. There are huge umber of articles throughout the internet that explains various aspects of IoT .  In this article/tutorial I will jot down one of my experiences with building an end to end IoT node.           Table of contents     The system   WebServer on Rpi           1) DHCP IP address       2) Static IP address           Bringing up the PIs’ WiFi interface           make linux-menuconfig       make menuconfig       wpa supplicant and iw           ad-hoc connection with iw   setting up the DHCP server   Induino and MPU6050   ThreeJs   Serial Port   WebSockets   Bringing the pieces together        The system   DISCLAIMER:   This system is not at all security focused. In fact, security has been disabled as much as possible.   I will not be going into the motivation I had to build this system . Through this article I am just sharing the experience I gained.   The end system has an embedded web-server hosted over an ad-hoc wi-fi mesh network that serves a 3D rendering of the orientation of the systems MU6050 6-access motion processing unit.   Though it is not a very utilitarian project, it presents huge learning opportunity.   Since there where too many unknowns for me to start with, I wanted to follow an agile development methodology for the system that will allow me to see a small working portion of the system at each point in time. Also, I wanted to use my brand new “Induino” to see how it flares.   Ultimately I ended up building a system which does this:     Now, on to how the components where build.   WebServer on Rpi  To get started, I wanted to build something that I had successfully build recently. So, as the first step, I brought up the Raspberry Pi with a minimalist kernel with lighttpd baked into the rootfs. If you are totally new to this, you can follow my tutorial here to get it done.   To include lighttpd in the final image, enable the following configuration in menuconfig:   Target packages -&gt; Networking applications -&gt; lighttpd [BR2_PACKAGE_LIGHTTPD =y]  There are two options to setup an IP address for the interface:   1) DHCP IP address  Run the following command to get an IP assigned to your interface from the router’s DHCP server.  udhcpc -i eth0   2) Static IP address   To setup a static IP address for your interface, add the following lines to /etc/network/interfaces. [Assuming that your router assigns IP in the 192.168.1.x range]   iface eth0 inet static address 192.168.1.30 netmask 255.255.255.0 gateway 192.168.1.1 network 192.168.1.0 broadcast 192.168.1.255   I chose to go with a static IP since I will have to modify and use it to further setup my adhoc network.  To bring up the network with this IP, just issue the command ifup eth0and verify the IP with ifconfig eth0   Once my network is up, I was able to serve simple static webpages over Ethernet from the Pi from /var/www. I did the following changes to the lighttpd default config generated by buildroot to enable cgi modules for my second step – dynamic pages.  in /etc/lighttpd/modules.conf,     add mod_cgi under server_modules     uncomment include \"conf.d/cgi.conf\" in modules.conf  in /etc/lighttpd/conf.d/cgi.conf     cgi.assign = ( \".sh\" =&gt; \"/bin/sh\")  in /etc/lighttpd/lighttpd.conf     add  \".sh\" under static-file.exclude-extensions  NOTE:   Being a non-production, minimalist prototype, I decided to serve my dynamic pages and web-services from shell scripts as much as possible. Note that this is never advised for a production system or even a system that you are planning to do something beyond a temporary prototype.   Bringing up the PIs’ WiFi interface   Raspberry Pi doesn’t have an on-board WiFi interface . The easiest way to add WiFi capability to the Pi is to use one of those cheap USB WiFi adapters available in the market. I user a EDUP mini module which has a Ralink chip in it.   Official Raspbian image supports this module as plug and play. But with buildroot, we have to enable the required modules (only :smile: )   To enable kernel support for wireless networking and the Ralink chip, the following configuration has to be done. I enabled all these modules to be built into the kernel (as opposed to Modules) .   make linux-menuconfig      Netwoking support -&gt; Wireless -&gt; cfg80211 – wireless configuration API [CFG80211 =y]   Netwoking support -&gt; Wireless -&gt; cfg80211 – wireless configuration API -&gt; enable powersave by default [CFG80211_DEFAULT_PS =y]   Netwoking support -&gt; Wireless -&gt; Generic IEEE 802.11 Networking Stack (mac80211) [MAC80211 =y]   Netwoking support -&gt; Wireless -&gt;Minstrel[MAC80211_RC_MINSTREL =y]   Netwoking support -&gt; Wireless -&gt;Minstrel-&gt;Minstrel 802.11n support [MAC80211_RC_MINSTREL_HT]   Netwoking support -&gt; Wireless -&gt;Enable mac80211 mesh networking (pre-802.11s) support [MAC80211_MESH =y]   Netwoking support -&gt; Wireless -&gt;Enable LED triggers [MAC80211_LEDS =y]   Device Drivers -&gt; Network device support  -&gt; Wireless LAN -&gt;Ralink driver support [RT2X00 =y]   And all devices under it [RT2800USB_RT33XX =y , RT2800USB_RT35XX =y, RT2800USB_RT3573 =y, RT2800USB_RT53XX =y, RT2800USB_RT55XX =y, RT2800USB_UNKNOWN =y]   make menuconfig      Target packages  -&gt; Hardware handling -&gt; Firmware -&gt; linux-firmware  -&gt; WiFi firmware -&gt; Ralink rt27xx/rt28xx/rt30xx [BR2_PACKAGE_LINUX_FIRMWARE_RALINK_RT2XX =y]   This step is required since the Ralink module relies on the firmware to be loaded into the chip each time it is powered on. This step will download the .bin and copy it into  /lib/firmware/ folder of the rootfs   Now we need the user space tools to connect to WiFi networks and check if our kernel support for the USB module is fine.   wpa supplicant and iw  The following configurations have to be enabled in the buildroot menuconfig   Target packages -&gt; Networking applications -&gt;iw [BR2_PACKAGE_IW =y]    Target packages -&gt; Networking applications -&gt;wpa supplicant [  BR2_PACKAGE_WPA_SUPPLICANT =y]   First off, let us try to connect to an existing WiFi network through wlan0 created by our USB “dongle”   To do this, first enter the network details in /etc/wpa_supplicant/wpa_supplicant.conf . A sample is given below:   update_config=1  network={     ssid=\"my SSID\"     scan_ssid=1     psk=\"Pass phrase\" }   scan_ssidis set to 1 to connect to hidden networks.   Now, run the wpa_supplicant tool to connect to the network (preferably run this in the background if you are not just testing if your configuration is fine)   wpa_supplicant -i wlan0 -dd -c /etc/wpa_supplicant/wpa_supplicant.conf   When you run this command, you should see the state change from GROUP_HANDSHAKE to COMPLETED in the prints. This means, a WiFi channel is established. Now you can run udhcpc comand (while wpa_supplicant is running in background) to get an IP (if dynamic IP is configured)   ad-hoc connection with iw  Once the WiFi interface is up, we can now setup the Ad-Hoc network.   The intention of setting up an AdHoc network is that when the final prototype is ready, the AdHoc network hosted by the Pi can be used as the initial configuration interface which allows us to setup the system parameters as required.   We use the user space tool iw to setup the interface mode to IBSS and then start a network . To do this automatically on each boot, we setup an init.d script at /etc/init.d - S90Adhoc as below.  The S90 prefix is to wait for other init scripts that bring up other services (DHCP, dropbear etc) before the IBSS is setup (I know, not the best order :smile: ).   /etc/init.d - S90Adhoc    #!/bin/sh   echo \"hello world - vysakhp\"   ifdown wlan0   iw wlan0 set type ibss   ifup wlan0 up   iw wlan0 ibss join rpinet 2422   This will create an Ad-hoc network named rpinet in the 2422 channel that other devices like a laptop or ipad can join to.   However, at this stage, the devices joining the network wil have to setup a static IP address. This can be avoided by hosting a dhcp server in the Pi. Note that the Pi itself will need (?) to have a static IP   setting up the DHCP server  To enable the DHCP server in the Buildroot image for Raspberry PI, it has to be compiled with the following options enabled (in menuconfig)      Target packages -&gt; BusyBox -&gt;  Show packages that are also provided by busybox [ BR2_PACKAGE_BUSYBOX_SHOW_OTHERS =y]   Target packages -&gt; Networking applications -&gt; dhcp [BR2_PACKAGE_DHCP =y]            Target packages -&gt; Networking applications -&gt; dhcp -&gt; dhcp server [BR2_PACKAGE_DHCP_SERVER =y]       Target packages -&gt; Networking applications -&gt; dhcp -&gt; Enable delayed ACK feature [BR2_PACKAGE_DHCP_SERVER_DELAYED_ACK =y]           S50–dhcp script under /etc/init.d will be as below. This will automatically start the dhcp server once the RPi is up.     Also, /etc/dhcp/dhcp.conf will look like:   ddns-update-style none; option domain-name \"example.org\"; option domain-name-servers ns1.example.org, ns2.example.org; default-lease-time 600; max-lease-time 7200; log-facility local7; subnet 192.168.10.0 netmask 255.255.255.0 { range 192.168.10.5 192.168.10.150; }   Once these configurations are done and the Pi rebooted, it will expose the rpinet AdHoc network to which any device can join.  If you need to do an SSH login to the Pi over this network, dropbear can be used. To enable:      Target packages -&gt; Networking applications -&gt; dropbear [BR2_PACKAGE_DROPBEAR =y]   Induino and MPU6050   I purchased an Induino R3 instead of a an Arduino UNO  since it has a lot of inbuilt peripherals that will help me with rapid prototyping. I bought the GY-521 MPU6050 module that comes with on-board voltage regulator from ebay. Jeff Rowberg’s  i2cdevlib arduino library for MPU6050 comes with an example code on how to use the MPU and an excellent processing “teapot demo” similar to the one supplied by Invensense.   To get started, I soldered the berg strips to the GY-521 module, hooked it up with the Induino and loaded the sample sketch that came with Jeff’s library with OUTPUT_TEAPOT enabled.          Then I ran the teapot demo in processing (after installing toxic libs for processing and tweaking the code to use my serial port)  . The result was inspiring :smile:   ThreeJs   Being totally new to threejs, I wanted to first see how the whole thing works. To begin with, I followed the tutorial here and ironed out the setup issues . Since I did not want to compromise the integrity of my setup, I hosted the sample page in my local Apache2 setup. Firefox Firebug came in extremely handy to sort out the initial issues.   I had just a conceptual understanding of websockets at this point. So, I wanted to use it get the values from Induino  into the webpage in real time so that I will get a practical understanding of what websockets are.   However, as I mentioned in the beginning I wanted to follow an Agile methodology that allows me to see a working piece of the system at each milestone. So I decided to first get the threejs part of the simulation working. For this, I captured the quaternion values that the Induino spits out and stored it as a Javascript . These where then used to rotate a plane box that was created using threeJs methods.   The sample with a small sample set is given below:     Serial Port   I enabled support for the FTDI232 chip in my Induino R3 in the kernel configuration at :   Device Drivers -&gt; USB support -&gt; USB Serial Converter support -&gt; USB FTDI Single Port Serial Driver  [USB_SERIAL_FTDI_SIO = y]      Once the Induino is connected, the serial port device [/dev/ttyUSB0 in my case] needs to be configured to start reading from it. I used the following stty command.   stty -F /dev/ttyS0 cs7 cstopb -ixon raw speed 115200      Once configured, quaternion values coming form the MPU can be read from /dev/ttyUSB0 as if being read from a regular file. The use of this can be seen in the next section where I am discussing my websocket server.   WebSockets   A websocket is a beautiful mechanism that allows you to have a near-real time, duplex communication between a server and webpage . All modern browsers support it and the data can be readily used in javascript.   Since my target system is a minimalist RPi based embedded system, I did not want to go with full fledged socket server implementations. Moreover, I wanted to learn how these cuties are build.   At first, I glaced through the documentation available on socketservers and found this MDN article to be very straight forward and useful.  I also looked into this beautifully written shell script websocket server to get an understanding of how to setup my own server. The coproc redirect was particularly interesting in his script.   After getting a hang of how websockets work, I went ahead and wrote my own websocket server (adapting portions from Krey’s script). Since my data is always smaller than 127 bits (7 bits limit used to denote length in RFC6455 base framing protocol ), my header is more or less hard-coded. Initial glitches where ironed out using wireshark to trace the local loop-back traffic using its support to parse websocket traffic.   Note: I marked my packets as “text frame” in the header. This needs the data to be terminating with \\r\\n .  [increment length field by 2]   Finaly, my Websocket Server code looks like this:     Bringing the pieces together   I modified my sample code to move a threeJs plane with the quaternion data dump as below and copied it to its final resting place /var/www of RPi root and started my websocket server .   Final webSocket code:     Final HTML:     the end result — geekNirvana  ","categories": ["Articles","Tutorial"],
        "tags": ["IoT","Raspberry Pi","Linux","buildroot","ThreeJs","websocketServer"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/building-an-e2e-iot-system-with-raspberry-pi/",
        "teaser": null
      },{
        "title": "Getting started with ESP8266 WiFi Module",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   I recently purchased an ESP8266-03 module from hacktronics for ₹549/- and was pumped up by the fact that I can WiFi enable virtually any of my microcontroller projects with a spare serial interface, at a very small cost. But, Little did I know that it was just the tip of the iceberg.   However, First things first – How to get started with the module?   The main problem that I faced initially was the non-breadboard friendly package. I deliberately went for the V03 since it has more GPIOs exposed, and it has a chip antenna. But, the pitch was 2mm as opposed to the 2.54mm  DIP package pitch. So, I designed a breakout board to make it usable.   Once soldered, it looks like this:                                         If you are interested, you can download the toner transfer template here.   If you are interested in making your own board at home, follow my tutorial here.   Here is a collage of the steps I followed for this board (out of vanity :smile: )                                         The board needs some minimal connections to get started. I followed the schematic from Scargill’s post here.                                         After getting used to the comforts of Arduino and Raspberry Pi, I found it irritating to work with wires dangling around from the breadboard. So, I made a home for the spider in a vero board.                                         This board has the power control circuitry to convert 5V supply form the USB plug into the 3v3 supply required by the module. It also has some jumpers and switches to enable power saving modes and programming modes. It is all as per the circuit provided in the above diagram.   Next step is to check if the module’s original functionality of “serial to WiFi” works fine. I connected the module to the serial port (over USB) of my laptop and issued some AT commands as given in the official documentation.                                         The module responds, I can connect to my home WiFi router, fetch HTML pages and I am all set .   Once this is done, I read through a lot of documentation and tutorials to setup an SDK and compilation environment to start writing standalone programs for the module . After a lot of trouble, I was able to setup a stable environment . I have put up the instructions in Github so that you can start coding in 5 minutes .   I used this environment to compile the official IoT_Demo from espressif and was able to control a LED over WiFi with curl Commands.     I will be doing more experiments after the holiday season on the standalone functionality of the device and try to setup a more transparent http server. Once I am done, I will publish a detailed programming tutorial for the module.   Happy holidays! Cheers!   ","categories": ["Articles","Tutorial"],
        "tags": ["ESP8266","IoT"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/getting-started-with-esp8266-wifi-module/",
        "teaser": null
      },{
        "title": "IoT: Networked embedded system caveats",
        "excerpt":"  This article has been migrated from my original post  at embeddedinn.wordpress.com.   If you have experience developing software over the linux TCP/IP stack and start to develop something similar for an embedded system, you will soon realize that there are some things that cannot be granted while you work a level closer to silicon.   I recently got cocky with my experience and commuted to deliver a simple IoT device that uses a brand new platform and a well known service (foolishly) thinking that it can be done in a fairly small amount of time. Afterall, at the heart of it was a HTTP client and a couple of sockets . Boy was I wrong!!! Anyhow, finally after many sleepless nights and a ruined week end, I was able to deliver it a day before the deadline.   I am jotting down some pointers from this experience that would help anyone that is getting started with developing a networked embedded system.   System definition:   My system has a TCP/IP stack that runs on bare metal and uses a loop to cooperate among other “tasks”. Luckily , I also had access to all the stack and MAC code.   Issues, solutions and other pointers:   First and foremost, get comfortable with your networking tools. If you have a background working with networking, you would be already knowing that Wireshark is your friend. What complements it when you are developing stuff for embedded systems, is netcat. Especially if you are communicating to an external server with minimal to no documentation made for small systems.   Netcat is a powerful yet simple tool that allows you to open a connection to a socket (IP+Port combination) and send ASCII to it as a client. It can also act as a server by listening on any available port in the system. Say you are trying to connect to a service over a HTTP GET request, you can frame request in your text editor, open a connection in nc and pump in the request. The response will be available in the same session and now you know what your service accepts, what it rejects and how it behaves to different commands. It can also act as a local server instance which can receive and display the requests being sent out from the device.   This become in case of servers that are intolerant to malformed requests. If you directly start pumping in the requests without first checking,  it might result in the server temporarily blocking your IP.   Another villain from the same clan is keep-alive timeout. From a firewall point of view, a TCP  socket opened can stay alive for a configured period of time or until either of the parties send a FIN,ACK . Typically this time is 30 minutes. However, since the server has to keep listening to this socket for the entire duration, most servers (HTTP in my case) will have their own time out configuration. Hence, if you open a socket and send a HTTP GET with a “connection: keep-alive” header, the server will initiate a FIN,ACK after a predefined period of time. In my case , it was 60s. To avoid this, you need to sent a HTTP request via the socket atleast once in 60s. One thing to be careful is, a bad request will disconnect the session immediately even though the request is for a keep-alive connection.   Now, say you have opened a socket to be kept alive, and you are sending the mandated pings, you are also expected to acknowledge the response to the requests that you are sending. For instance, it the server responds with  a 200 OK, you need to read (and discard in my case) the response so that the TCP stack can ACK the packet.  Generally , server stacks will have a threshold for the number of such unacknowledged packets after which the socket will be terminated (FIN,ACK) . In my case, it was 45 messages.   In my case with HTTP requests, there is one peculiar behavior that was particularly tricky. The socket was able to handle only messages of the same type. So, when I use a GET request for a particular resource with a specific set of headers to start a keep-alive connection, future requests on the connection needs to have the same set of headers. I was opening my HTTP session with an additional ‘Accept’ type which the further keep-alive requests did not have. This was causing the server to drop the session after 45 unACKed transactions.   Many a times , IoT servers like those of messaging services will have a rate-limit associated with it. That means, you cannot send more than n messages in x seconds. In my case, it was a message every second. Though you can handle this using flood gates and timed mutexes, it is always better to check the server response for the corresponding header. General, trends are to send X-RateLimit-Limit and X-RateLimit-Remaining. There is also a 429 Too Many Requests response code. Exact response will depend on your server implementation. Again, nc and wireshark are your friends who will help you find this out. Trigger a flood of requests via nc and observe the response. Capture wireshark traces for extra information.   In my system, the stack was designed to be close to real time and so, there where no provisions for blocking calls. This is particularly important to notice when you are working with an embedded system. Many of the calls would return immediately, but you have to wait for flags and callbacks to make sure the operation completed successfully. We would be inherently cautious when working with peripherals like SPI or I2C . But when it comes to networking, my experience of working with larger systems that take care of most of these things tend to kick me in the nuts more often than I would like.   Embedded TCP/IP stacks , as rare as they are, might have some interesting configurations that are user controllable. The bugger of a configuration taught me about SYN flood attack trigger configurations in firewalls the hard way. What I essentially ended up doing was sending SYN requests to the server port at 60MHz . That is a lot of SYNs for a second , and the server was blocking my IP . Thanks to a friend with a handy firewall who almost smacked my head seeing the firewall logs, I learned a valuable lesson in embedded networking.   The last thing that got me stuck when I about to ship my code was a connection issue. The device that was working well for the last 6 hours continuously, stopped connecting to the server all of a sudden when I just reset the device. From Wireshark logs, I could see that I was not getting a SYN,ACK. Back to the firewall guy and this time, he almost strangled me. I was using the same source port for every connection request. Remember the TCP socket timeout configuration of 30 min that I mentioned earlier in the article? Turns out, the firewall implements this by having a session entry that is tagged using the socket info (IP+Port). When I reset my device, since I did not send a FIN , the firewall session log thinks that the session is still alive , and does not allow another session with the same tag to be created.   The interesting part of this problem is the root cause. Since this timing info is mentioned in the TCP RFC, the stack that I as using was already taking care of assigning a random ephemeral port. However, my hardware does not have a true random number generator (TRNG), and is very much deterministic by design. So, every time the rand() function is called to generate the port number, the LFSR generates the same port number. This was causing my local firewall to block the connection request.   The solution I found for this problem was to rely on the small variation in time taken to get the DHCP IP assigned (uses a UDP port) every time. I have a timer being used by the TCP stack that is initialized at init(). I added the value generated by a floating ADC channel and temperature value, and used it to seed the PRNG. Though this is not 100% reliable, it is practical and worked for 100 trials. Any way, I have requested to add a TRNG add-on to the board. Fingers crossed.  ","categories": ["Articles","Tutorial"],
        "tags": ["IoT","embedded networking"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/iot-networked-embedded-system-caveats/",
        "teaser": null
      },{
        "title": "Docker : Scratching an itch to build from ground up",
        "excerpt":"  This article has been migrated from my original post at embeddedinn.wordpress.com.   If you have never heard of docker the server container holder ever before, then this post is not for you. For a tech enthusiast, Docker is an interesting and current piece of tech that you should at least be aware of. But there are hundreds of well written and well produced tutorials out there to help you with that.   By nature, I need to understand how things work. And, I prefer hands on trial and error as the way to gain this understanding. So, when the tutorials started off by telling me how to run existing Docker containers and how to build on top of existing ones using a Dockerfile, that was not enough for me. Useless as it is, I wanted to build a lean , bare minimal , functional Linux container using my experience with bringing up Linux on new platforms.Be aware that Docker is just a container that uses the hostOS kernel and the docker image contents as a file system. So , even though you might see that the kernel is being compiled in the below steps, it is simply because that is the simplest approach to the given problem. The compiled kernel image will not be used as such. Le Platform   From a tutorial per-say, it is always better to prep the development environment from scratch and list all steps and dependencies as it is done. So for this tutorial, I spun up a Digitalocean instance with Ubuntu 14.04 as the base OS. Following are the packages  that I installed after an apt-get update.   apt-get install vim git unzip libncurses5-dev docker.io build-essential   In case you are not familiar with Digitalocean, it is a simple cloud computing solution built for developers (just as their tagline says). If you are new to it, you can get a $10 start up credit if you register with this link   I will be using buildroot to generate the minimal linux image that will form our base image. I took the latest buildroot image (2016.02) from here and got it into the instance and unpacked it using the following commands. If you are interested to know more about using buildroot to build systems from scratch, follow my tutorial here.   wget https://buildroot.org/downloads/buildroot-2016.02.tar.gz tar xvf buildroot-2016.02.tar.gz   I decided to go with an x86_64 image and configured it as the target, selected apache under Target packages &gt; networking applications and fired off a build with 16 parallel jobs.   After a lot of number crunching, the self contained, minimal rootfs.tar archive will be generated at output/images. This will be our base image for running docker. The interesting part is that , including an apache installation, the image is just 5.4MB   Stage 1 Testing   Now that we have our minimal rootfs built, it is time to test the system.   To import the tar as a docker image and see it in docker image listing using the following commands.   docker import - testrfs &lt; rootfs.tar docker images   Now, you can run this new image using the following command:   docker run -it testrfs /bin/sh   This step will execute the busybox shell on the host ubuntu kernel . You can verify this using the following two commands.   docker run -it testrfs /bin/sh uname -a   With a docker diff, you can see that the root of the image has changed. This change (docker adding a couple of files) is how docker binds itself to the rootfs hat we created. (I assume)   Output of the uname command will show that we are still running over the host (ubuntu) kernel.   Configuring Docker Apache   Unfortunately,  due to a bug (which I am yet to figure our a solution for), we need to manually enable a module in the httpd config file for this to work. Remove the commenting # from the below line of /etc/apache2/httpd.conf   vi /etc/apache2/httpd.conf LoadModule slotmem_shm_module; modules/mod_slotmem_shm.so   With this httpd can be started successfully. (issue httpd -k start command and observe the ps -ef)   Docker PID1   As we have discussed in some of our Linux tutorials, as soon as the kernel comes up, the link to user land is setup using the init process. However, since docker is running over the host kernel, we end up with some issues here.   We need apache to run automatically when docker runs our image. As we discussed in the Linux cross compilation tutorial, we could have used the busybox init and inittab mechanism to achieve this. However, Docker presence a challenge here.   On a normal system built with an inittab based init (as in the case of most embedded systems), you just have to register a startup script at the appropriate run level in the init.d folder. The rcS and rcK scripts triggered from init will take care of executing the start and stop commands. Buildroot has already taken care of creating a startup script for apache. We just have to link it to the init.d folder using the following command   cd /etc/init.d ln -s /usr/bin/apachectl ./S50apachectrl   However, this will work only if init is executed. As you can see with a simple ps -ef command while you are in the docker bash, within the container, the default PID=1 is for either a /bin/sh (in case you pass the -t flag to run) command or any command that we explicitly pass to the docker run command.   If we pass init as the command to be executed by docker run, then a shell will not be spawned , and we will not be able to attach to the  docker image and send signals to it or perform operations from the shell.   To overcome this problem, we will write a small script that will launch apache demon as well as a shell, and create a Dockerfile to call that script as soon as the image is run.   Create a file /bin/start.sh with the following contents and provide execute permissions to that file with chmod a+x /bin/start.sh   #!/bin/sh apachectl start&amp;amp;amp; /bin/sh   Creating the final image   Now that we have made all required changes and configurations to our rootfs, we need to commit the snapshot and create a new base image with these changes . Use the following commands to achieve this.   docker commit 3a4c6d3e80e2 apachemin:v01 #hash obtained from docker ps -a   Now create a Dockerfile with the following lines:   FROM apachemin:v01 CMD /bin/start.sh   Now issue the build command to get the new docker image that will spawn the processes automatically:   docker build -t=&amp;quot;apacheinit:v01&amp;quot; .   Now, for the final testing, run the following command and try loading the default httpd index page from a browser ar port 8080 after issuing the following command:   docker run -p 8080:80 -it apacheinit:v01   For changes, the default DocumentRoot points to /usr/htdocs   Happy hacking. . . . happy learning. . .   ","categories": ["Articles","Tutorial"],
        "tags": ["Docker"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/docker-scratching-an-itch-to-build-from-ground-up/",
        "teaser": null
      },{
        "title": "Building a custom IoT dashboard",
        "excerpt":"  I recently implemented an end to end IoT dashboard to demonstrate the capabilities of a new embedded connectivity product that I was working on. I could have gone with SAAS and ready made solutions, but where is the fun in that??   I have tried to capture all the implementation details in the writeup below.   Objective   The objective here is quiet simple - collect data from the device peripherals and display it in a glossy web frontend dashboard in near real-time. The system is to be developed as a proof of concept and “capability demo” only and need not be a  production ready rugged system.   The primary components of the system are      messaging server   dashboard   Application firmware   Each device will be configured with a thingID that will be used to pair the dashboard and device and the complete server side installation will be rolled into a Docker that would enable anyone to spwan their own dashboard by just running a script.   The final dashboard looks like this:                                         Messaging Server   In the traditional embedded systems development framework that I was operating from, the tribes advice was to go for a HTTP server with “API” to poll for peripheral data from a web page. The very idea made me sick and I decided to go rouge with the “unconventional” path.   For a “scalable” system that meets my objectives and perform reliably at the same time, I decided to experiment with the much hyped “MQTT” protocol. At the same time , I did not want to implement the full MQTT client stack on my embedded device firmware stack since a dedicated team was working on it in parallel. So, I derived inspiration from dweet messaging SAAS and decided to roll my custom service with a similar simple API fingerprint.   Since I was not planning to build a production grade system, I decided to build my APIs on shell scripts and a simple C program.   messaging core   For the messaging backend, I used “mosquitto” and slightly customized it. It comes prepackaged for Ubuntu and has a simple commandline based client implementation (mosquitto-clients) to send MQTT messages.   Mosquitto server comes with native support for Websockets. It can be enabled by adding the following lines to the configuration file at /etc/mosquitto/mosquitto.conf   listener 443 protocol websockets   I used port 443 so that it can pass through most firewall settings. I also enabled some logging and default settings using the following configurations in the same file.   allow_anonymous true  autosave_interval 1800 persistence_file m2.db connection_messages true log_timestamp true   listener 1883   These settings enables the core backend of my messaging server. Now, I need to build an HTTP API around it.   building messaging APIs   All messages passed along the MQTT path for communicating with the the Dashboard will be in JSON. This design decision was made to accommodate :      Mobile application communication   Simple Javascript based parsing at the Dashboard side.   There will be two APIs built as CGI scripts on an Apache server running on the messaging server.   The listen API is a simple shell script that uses the mosquitto_sub command line utility that comes with mosquitto.   Once the server is configured properly, with the following API, a “long living” http connection can be opened and it will receive JSON messages sent to a thingID:   http://&lt;serverIP&gt;/api/listen/to/&lt;thingID&gt;   The CGI script copied to &lt;cgi path&gt;/api/listen/to ( script named to ) is:   #!/bin/bash  echo \"Content-Type: application/json\" echo -e \"Connection: keep-alive\\n\\n\"  path=($(echo $REQUEST_URI | tr \"/\" \"\\n\")) thingID=${path[-1]} mosquitto_sub -t $thingID -q 1   When this script is called from the CGI framework, it parses the thingID out of the URL query string and starts a connection on Port 80 to send all messages published to the thingID topic.   The following curl command can be used to test this implementation:   curl -i http://&lt;server ip&gt;/api/listen/to/&lt;thingID&gt;   The server sneds back an acknowledgement in the form below and closes the connection.   { \t\"with\": { \t\t\"topic\": \"thingID\", \t\t\"created\": \"Sat Oct  8 07:27:34 2016\", \t\t\"content\": { \t\t\t\"key1\": \"value1\", \t\t\t\"key2\": \"value2\" \t\t}, \t\t\"session\": \"127.0.0.1:59406-157a3309ae0\" \t} }   Here the session ID is the remote IP + remote port + epoch time down to the millisecond.      It is not a good practice to rely on a long living HTTP connection for reliable message transfer. There are numerous reliability and load balancing issues associated with this. However , this method was used here since it is not a production grade system design.    The API to send messages is a bit more complicated. Though at the heart it uses mosquitto_pub CLI utility, there is a lot of preprocessing that has to be done to extract the information out of the query in the URI   Once setup properly, the following HTTP API (GET request) can be used to send a MQTT message using the server:   http://&lt;server ip&gt;/api/mdtweet/for/&lt;thingID&gt;?key1=value1&amp;key2=value2   This will trigger a MQTT message on the thingID topic with a json payload which has the key and values that are in the URI query string. This will also return a json with the values sent in the message along with some timestamp and serializing information for tracking and debugging.   The “send” API was implemented in C code with no dependency on external libraries (link to source) . So, to compile and install it, just issue the following command:   gcc sendMessage.c -o &lt;cgi api path&gt;/for  This will compile and “install” the send API with the name for in the CGI API directory.   CGI configurations   For the APIs to function as intended, the CGI framework of Apache had to be configured   Following configuration had to be done in /etc/conf-available/serve-cgi-bin.conf and ./conf-enabled/serve-cgi-bin.conf to enable CGI scripts to execute from /var/www/api folder.    &lt;IfDefine ENABLE_USR_LIB_CGI_BIN&gt;  ScriptAlias /api/ /var/www/api/  &lt;Directory \"/var/www/api\"&gt;  AllowOverride None  Options +ExecCGI -MultiViews +SymLinksIfOwnerMatch  Require all granted  &lt;/Directory&gt;  &lt;/IfDefine&gt;  Finally, to enable CGI module in Apache, do: sudo a2enmod cgi and restart the Apache service.   Since we are going with the suboptimal approach of using a long living HTTP connection, the server will be configured to accept infinitely long living HTTP keep-alive tunnels. to achieve this, keepalive timeout and max request in /etc/apache2/apache2.conf was increased. The number of ‘keep-alive’ requests that the server can accept was also increased to a large number (1000). However, this is not mandatory and this will in fact cause huge resource utilisation issues in a production environment.    MaxKeepAliveRequests 1000  MaxKeepAliveTimeout 60   Dashboard   A large portion of the IoT ecosystem revolves around glossy web based dashboards. I decided to implement mine using freeboard. However, the hosted version of freeboard was lacking many of the features that I wanted in my implementation. Support for MQTT based messages was the biggest loophole taht made me decide that I want to host my own version of the dashboard.   The major features that I ended up implementing in my custom roll of the dashboard are      MQTT based messaging   Automatic setup of server IP on page load   Auto re-configuration of MQTT client on thingID change   dynamic QR code generation   URI query based automatic configuration of thingID   Custom light indicators   MQTT on freeboard   Though MQTT is not in the list of messaging protocols supported by freeboard off the shelf, the cool guys who developed it has put in well documented hooks to add new messaging plugins. AL S-M has developed a wrapper around Eclipse Paho JavaScript Client that plugins into freeboard.   I decided to place my paho JS files on my server itself and placed them at plugins/mqtt/mqttws31.min.js. This information has to be updated in the plugin file placed at plugins\\mqtt\\paho.mqtt.plugin.js.   \"external_scripts\" : [         \"plugins/mqtt/mqttws31.min.js\", \t\t]   This file in turn needs to be referred from the index.html file to be loaded upon init. Look for the comment instructing to place plugins and add the plugin path:   // *** Load more plugins here *** \"plugins/mqtt/paho.mqtt.plugin.min.js\"   Once this is done, freeboard comes up with a dropdown option to select MQTT as one of the data sources in the design view. In my case, design/edit view would be disabled by default since the only parameter that needs run time configuration is the thingID and I will have a text box to enter that. Since my messaging server and dashboard HTTP server will be rolled into a single server instance (again, a suboptimal approach for a production system), I wanted to fetch the server IP name/IP from the launch URL and automatically configure the MQTT client.   Automatic setup of server IP on page load   As described in the MQTT server configurations, my MQTT websocket runs on port 443 since I wanted to comply with firewall installations. ( This is the worst way to achieve this + I am not running https :frowning: )   In paho.mqtt.plugin.js settings object, change the following to achieve this:      default_value of server to window.location.hostname   default_value of port to 443   default_value of use_ssl to false   default_value of json_data to true   With these configurations, the MQTT server and settings configured for the client will be automatically loaded upon page load. However, the topic to listen to has to correspond to the thingID configured in the dashboard.   Dynamic re-configuration of MQTT client   The thingID is entered into the dashboard using an html textbox with an onClickhooked to update a global variable by name “thingID”. This variable can be used to reconfigure the MQTT client and this is also plugged into the same update onClick()   dsSettings = freeboard.getDatasourceSettings(\"devStat\"); dsSettings.topic = thingID; freeboard.setDatasourceSettings(\"devStat\", dsSettings);   refreshPanels() { /*will be called bu onConnect of paho backend*/ if (true == updateCalled) {     thingID = document.getElementById(\"thingID\").value;     enableLedButtons();  /*DOM changes*/      message = new Paho.MQTT.Message(JSON.stringify({StatREQ:\"True\"}));     message.destinationName = thingID; pahoClient.send(message); }   paho.mqtt.plugin.js also needs to be edited to read the server IP from the URL as well as to call onConnect() on reconnect. To do this, onDispose function has to be edited by adding the following code just before the call to Paho.MQTT.Client.   currentSettings.server=window.location.hostname;   function onConnect() {     console.log(\"Connected to server\");     pahoClient.subscribe(currentSettings.topic);     if (typeof refreshPanels === \"function\") {       refreshPanels();     }   pahoClient is a global variable that I defined in paho.mqtt.plugin.js to hold the client object and all calls to client has been replaced by pahoClient   dynamic QR code generation   Freeboard has a good responsive design that makes it an excellent candidate to develop a dashboard that can be used to monitor and control my IoT devices from a Mobile phone as well. Though there was a companion Mobile App, I did not want everyone to have to install an App. Cross platform experience as well can be maintained uniformly using a web based solution.   However, having to enter the server IP and then having to configure the thingID every time seemed a bit unattractive. So, I decided to go with the generation of a QR code that can be scanned using the smart phone to instantly launch a “pre-provisioned” dashboard. As of today, Android and Windows phones can do this directly from the default camera app whereas iPhone requires a third party app.   To implement this, I had to make two modifications     hack index.html to fetch and configure the thingID from the URL query string   include a mechanism to dynamically generate QR code when thingID changes.   The later will be discussed here and former will be discussed in the next section.   To generate a QR code using javascript , I used jquery-qrcode from  jeromeetienne. I created a pane for the QR code in my dashboard and made the updateThingID() function call updateQrCode() with the new thingID as the parameter. This will in turn call jQuery('#qrcodeTable').qrcode() that would redraw the QR code .   However the QR code drawn by this library doesnot have a margin and when the dark background of the default freeboard theme would “corrupt” the outer boundaries of the QR code. To avoid this, I had to modify jquery-qrcode.js to achieve this.   qrcode.js has two mechanisms to generate a QR “image”. We can choose either a table based approach where the QR code is generated as a table with the square cells filled according to the QR code , or a canvas based drawing. Since I am using many features throughout my dashboard that would restrict it to be used only with a modern browser, I decided to go with the with the canvas approach.   To achieve a margin around the drawing generated within the canvas, I modified the createCanvas function element by adding the following code after the default code that sets the width and height.   canvas.width\t= options.width; canvas.height\t= options.height; canvas.setAttribute('style', \"left: 50%;margin-left:\"+options.marginLeft+\"px; top: 50%;margin-top:\"+options.marginTop+\"px;\");   This code would let me pass the margin width as parameters from the code that I call to create the QR code.   Query based thingID configuration   To make it possible to elegantly utilize the QR code as well as to have an appeal during demos, I wanted to implement a mechanism to provision the dashboard based on a thingID passed on as a query string. This would let me configure my demo , set it up and ask someone to scan the code , and a copy of the dashboard would be instantly available in their mobile phone. Asking them to type a URI and then configure the thingID would fade the charm of this glossy dashboard.   Freeboard’s default index.html has a mechanism to pass on the dashboard jsons name as a query string parameter. I hacked this portion to fetch the thingID call the updateThingID() function to update the MQTT client, and the QR code in the dashboard.   I added an else condition to the hashpattern checking code in the index file as below.   var hashpattern = window.location.hash.match(/(&amp;|#)source=([^&amp;]+)/); \t\t\t\tif (hashpattern !== null) { \t\t\t\t$.getJSON(hashpattern[2], function(data) { \t\t\t\t\t\tfreeboard.loadDashboard(data, function() { \t\t\t\t\t\t\t\tfreeboard.setEditing(false); \t\t\t\t\t\t\t\t}); \t\t\t\t\t\t}); \t\t\t\t} \t\t\t\telse{ \t\t\t\t$.getJSON(\"dashboard.json\", function(data) { \t\t\t\t\t\tfreeboard.loadDashboard(data, function() { \t\t\t\t\t\t\t\tfreeboard.setEditing(false); \t\t\t\t\t\t\t\tvar thing=getParameterByName(\"thingID\"); \t\t\t\t\t\t\t\tif(thing){ \t\t\t\t\t\t\t\tdocument.getElementById(\"thingID\").value = thing; \t\t\t\t\t\t\t\tupdateThingID(); \t\t\t\t\t\t\t\t} \t\t\t\t\t\t\t\t}); \t\t\t\t\t\t});  \t\t\t\t}   this would load my dashboard code (json) named dashboard.json and check for a query string named thing that would contain the thingID that needs to be configured in the dashboard.   Custom light indicators   Freeboard has a light indicator widget that was not working on passing the configured ON and OFF messages in the version that I pulled. To fix this, I added the following code in the indicatorWidget function object in freeboard.widget.js and freeboard_plugins.js . (to be frank, I was not sure which would work at that time and I did not bother figuring out)   this.onCalculatedValueChanged = function (settingName, newValue) { \tif (settingName == \"value\") { \t\tif (newValue == onText) { \t\t\tisOn = true;  \t\t} \t\telse { \t\t\tisOn = false; \t\t} \t} }   Now, the widget was supporting only one color but I wanted to match the colors of the LEDs I had in my demo board. So, I decided to modify the widget to have configurable LED color options.   Just above the indicatorWidget function element in freeboard_plugins.js, I added the following styles along with the existing .indicator-light styles.   freeboard.addStyle('.indicator-light.default', \"background-color:#FFC773;box-shadow: 0px 0px 15px #FF9900;border-color:#FDF1DF;\"); freeboard.addStyle('.indicator-light.orange', \"background-color:#FFB340;box-shadow: 0px 0px 15px #FFC600;border-color:#C6AB0F;\"); freeboard.addStyle('.indicator-light.green', \"background-color:#31F015;box-shadow: 0px 0px 15px #89F944;border-color:#47C620;\"); freeboard.addStyle('.indicator-light.red', \"background-color:#FF1818;box-shadow: 0px 0px 15px #FF4040;border-color:#F00;\"); freeboard.addStyle('.indicator-light.lgreen', \"background-color:#92B95B;box-shadow: 0px 0px 15px #869F62;border-color:#85A454;\");   to make these styles take effect, I modified the updateState() function as:     indicatorElement.toggleClass(currentSettings.color, isOn);   Now, to get these color options when I create a new widget instance, I added the following to the indicator-light settings json   {   name: \"color\",   display_name: \"Color\",   type: \"option\",   options: [     {       name: \"Default\",       value: \"default\"     },     {       name: \"Green\",       value: \"green\"     },     {       name: \"Red\",       value: \"red\"     },     {       name: \"Green(lite)\",       value: \"lgreen\"     },     {       name: \"Orange\",       value: \"orange\"     }   ] }   The new settings window for the widget looks like this:                                         Dockerizing the server   In the initial release of the dashboard that I was testing using dweet free tire messages, I hosted my modified web page code as a Github page. However, once I implemented the MQTT backend , this was no longer possible. The main reason for this is that all github pages gets loaded over https and the webSocket in the messaging server that I configured uses ws:// instead of wss:// . Since my objective was to roll out the server code and let people use it as they deem fit, getting wss:// configured with valid certificates would have been prohibitive.      an alternate approach is to lower the security of the browser and allow it to use ws:// on a page loaded over https.    My preferred development infrastructure is Digitalocean since it is a simple and flexible hosting service. So, for people to easily setup a “demo server” , I rolled all the development I did, into a Dockerfile that can be piulled from Github. So, it is just a matter of running few lines of commands and within minutes (if we are using the high speed network backed of a hosting service), the whole server will be up and running.   here is the Dockerfile that I wrote. It is pretty self explanatory   FROM ubuntu:16.04 MAINTAINER vysakhpillai  RUN apt-get update &amp;&amp; apt-get install -y apache2 supervisor mosquitto-clients mosquitto build-essential   RUN mkdir -p /var/lock/apache2 /var/run/apache2 /var/run/sshd /var/log/supervisor /var/lock/mosquitto /var/run/mosquitto  #apis RUN mkdir -p /var/www/api/listen COPY configs/listen.sh /var/www/api/listen/to RUN chmod +x /var/www/api/listen/to  RUN mkdir -p /var/www/api/mdtweet COPY  configs/sendMessage.c /var/www/api/mdtweet RUN gcc /var/www/api/mdtweet/sendMessage.c -o /var/www/api/mdtweet/for run rm /var/www/api/mdtweet/sendMessage.c  #apache COPY configs/serve-cgi-bin.conf /etc/apache2/conf-available/serve-cgi-bin.conf COPY configs/serve-cgi-bin.conf /etc/apache2/conf-enabled/serve-cgi-bin.conf COPY configs/apache2.conf /etc/apache2/apache2.conf RUN mkdir -p /var/www/html RUN rm -rf /var/www/html/* ADD html /var/www/html RUN a2enmod cgi  COPY configs/mosquitto.conf /etc/mosquitto/mosquitto.conf COPY configs/supervisord.conf /etc/supervisor/conf.d/supervisord.conf  EXPOSE 80 443 CMD [\"/usr/bin/supervisord\"]   ","categories": ["Articles","Tutorial"],
        "tags": ["Docker","IoT","freeboard","MQTT"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/Building-a-custom-IoT-dashboard/",
        "teaser": null
      },{
        "title": "HDLs and basics of hardware design",
        "excerpt":"  Introduction   I am not covering the entire process of silcon design in this tutorial. After a quick intro, we will jump into setting up a HDL simulator and get our hands dirty.   For an embedded engineer, hardware design generally referrs to physical components and printed circuit boards (and breadboards at times). While they form the basics of hardware design, when we move towards designing tiny circuits that pack millions, maybe billions of transistors into a tiny silicon die the size of a pencil head, it becomes obvious that we cannot design them in the conventional way. Here we are talking about the design and development of the components we mount on the PCBs themselves.   The design cycle of an application specific integrated circuit (ASIC) starts off with the description of the circuit itself. While schematic based design entry is a way out, over the years we have perfected advanced tools that can convert the logic description of a circuit into hardware schematics. Since traditional programming languages rely on sequential execution (among many other reasons) we have created a class of languages tailor-made to describe asynchronous and combinatorial logic, and we call them hardware description languages (HDLs).   Verilog and VHDL are the two most commonly used HDLs, and we will be using Verilog here.      Note: A caveat to using a high level language is the tendency for a noob to describe all the hardware logic as “behavioral logic” and leaving it for the tool to synthesize the hardware for it. Take extra care to avoid this mistake.    There are numerous in-depth articles available to wrap your head around the nitty gritties of Verilog and I will not be covering it here. I will however guide you through setting up a free and open-source Verilog simulator to experiment with the concepts. I will also run through the basic concepts that will help us go ahead with HDL based design in the following parts of Fun With FPGAs.   Icarus Verilog   Icarus Verilog (or iverilog) is a free and open sourced Verilog synthesizer and simulator. It runs on Linux as well as windows and is powerful enough to work with complex designs.   Installation of iverilog is quiet straight forward .   In Ubuntu, install it from the official repositories using the following command or compile it from source.   sudo apt-get install iverilog   The windows port for iverilog is available here   iverilog comes with two main tools.      iverilog   : the verilog compiler   vpp        : the simulator   To view simulation output waveform, we will use GTKWave.   Verilog design file   Each design file typically contains a Verilog module. A module is nothing but a basic building block of a design with a set of inputs and outputs. Later on in the design flow, these files will be synthesized into actual hardware blocks.   A typical block for an inverter will look like:   module myInv(A,B); input wire A; output wire B; assign B = !A; endmodule   This code goes into myInv.v in this sample design.   Breaking it down:   myModule is the name of the module and A and B are the associated signals. Further , we declare that A is the input wire and B is the output wire. Wire means that the signal needs to be driven always and it cannot hold a value on its own . Signals that can hold values are called reg. A signal can be inout as well.   assign keyword is used here to invert the input and assign it to the output.   = is a blocking assignment and all statements assigning using a = will happen sequentially. Non-blocking assignments are denoted using &lt;=. All non-blocking assignments in a block happens in parallel, at the same time.   Testbench   A testbench is written as a seperate file, primarily since it is the “non-synthesizable” portion of the design, and is used to just validate the design. Consequently, it can have implement some “exotic” constraints and gimmicks.   Essentially, a testbench is like a top level module that  encapsulates the design, feeds the design with input, and monitor the output. Generally, a test bench will not have any I/O signals.   A sample testbench file (myInv_tb.v) for our ivnerter will look like:   module myModule_tb(); wire out; reg clock;  initial begin $dumpfile(\"inv.vcd\"); $dumpvars(0,notGate);  clock=0; #10 $finish; end  always begin #1 clock = !clock; end  myInv notGate(clock,out); endmodule    Breaking it down:   As described above, there are no I/O lines for a test bench. However, there is an internal register named clock that will act as an input to myInv and a wire out that will capture the output.   This file illustrates one of the main concepts of HDL design. Unlike sequential programming languages, each block (initial and the “always”) executes in parallel. The designer has to keep this in mind always.   The initial block will execute only once at t=0, whereas the always block is like an infinite loop.   Statements that starts with a $ sign are called tasks and are used by the simulator.   Statements starting with a # are delay statements. Execution will halt in that block for number of time units mentioned after the # sign.   Thus, as per the initial clock will be initialised to 0 at t=0 , and execution will end ($finish) after 10 time units.   As per the always block, clock will be inverted every one time unit.   An important point to note about the always block is that it can drive only reg type data and not wire type data.   Line 19 creates an instance of myInv by the name notGate and assigns clock and out to A and B.   $dumpfile is the task used to open a waveform file named inv to capture the behavior of clock and out during simulation.   The 0 passed as first argument to $dumpvars instructs the simulator to capture all signals of module instance notGate. If a 1 is passed, only the signals named in the argument list will be captured in the output file.   “Synthesis” and Simulation   Execute the below command to synthesize the module and test bench.   iverilog -o myInv.vvp myInv.v myInv_tb.v   This is not similar to hardware synthesis where we end up with a hardware netlist. Rather, here we synthesize a netlist named myInv.vvp that the simulator can use.   To simulate this model, issue the following command   vvp myInv.vvp   At the end of simulation, a file named inv.vcd will be created. This is the waveform file that can be opened in GTKWave.   Issue the following command to view the waveform. Expand the signal tree and drag &amp; drop the signals the view to the Signals list   gtkwave inv.vcd                                               myInv simulation results         More Verilog concepts   To introduce more verilog concepts, let us design a simple arbitrator. An arbitrator is a mechanism to avoid a collision (much like a mutex in software) . For example, when a device wants to access a bus along with many other devicec connected to the bus, instead of directly accessing it, the device will place a request the arbitrator for bus access and wait for the “grant” signal. The arbitrator liaisons all such requests.   Internally, an arbitrator is a simple statemachine with the below state diagram                                               arbitrator state diagram         At a block level, an arbitrator looks like the below figure:                                               arbitrator block diagram         This article is a work in progress. I will be updating it soon with more infomration as soon as I get some time off my dayjob. :smile:  ","categories": ["Articles","Tutorial"],
        "tags": ["FPGA","HDL","Hardware"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/01-HDLs-and-basics-of-hardware-design/",
        "teaser": null
      },{
        "title": "Introduction to digital certificates",
        "excerpt":"      Introduction   Digital certificates are one of the most widely used yet largely misunderstood pieces of technologies in today’s world.They are are largely taken for granted without appreciating the underlying technologies.          Table of contents     Introduction   Foundation           Hashes / Digest       Encryption and Decryption       key exchange       “Trapdoors” and one way functions       Digital Signature       keys! keys!! keys!!!           Digital certificates   Digital certificates and networking        This blogpost explains some of the core concepts involved in the usage and implementation of digital certificates and secure systems that rely on them.   Foundation   To begin with, let us look at some of the core concepts involved in building certificates . At first, these might look disconnected. Consider each sub topic here as independent pieces of information and these will converge as we get going.   Hashes / Digest   Here we are talking about cryptographic hash functions and these are not to be confused with hash tables in data structures.   A cryptographic hash function is one which takes an input of arbitrary length and produce a unique digest of fixed length out of it. For example, see the operation of sha256sum hashing algorithm below:   echo \"hello world\" | sha256sum  a948904f2f0f479b8f8197694b30184b0d2ed1c1cd2a1ec0fb85d299a192a447  -  echo \"helloworld\" | sha256sum  8cd07f3a5ff98f2a78cfc366c13fb123eb8d29c1ca37c79df190425d5b9e424d  -   As you can see, the sha256sum hashing function takes an input string of varying length , and produces a hash/digest of fixed length (256 bytes). Any change in the input string results in a different, unique digest of the same length.   This property of the hashing function can be used to validate the integrity of any arbitrary piece of data. For example, many download sites will display the hash of the file being downloaded along with the hashing algorithm. The hash of the downloaded file can be computed independently and compared to the published hash to ensure that the download was intact.   Another popular use case of this property is to generate unique commit IDs in version controls like Git. The Diff of the changes being committed are passed through the hashing function to generate a unique commit ID of uniform length that represents the changes being committed.   Another important property of hashing function is that it is practically a one way function. Meaning, given a digest, it is impossible to generate the contents that resulted in the digest. The digest in itself does not indicate the length or any other property of the input. Therefore, the same input is needed to regenerate the hash.   These two properties are used to generate a “Message Authentication Code” (MAC) that is used in validating data in secure networks as we will see soon.      Given a digest, the input length and other parameters like input encoding, the input can be computed from a hash. One of the very popular methods used for this is called a rainbow table. It is a list of precomputed hashes of all alpha numeric combinations of a given length which can be used to reverse map the input that resulted in the string       A hashing function is only one of the methods that can be used to generate a MAC    Encryption and Decryption   Encryption and decryption are conceptually simple operations though they are extremely complex as we get into the implementation details. For a given piece of data $d$ and an encryption key $k$ , we can define two operations $E$ the encryption function and $D$ the decryption function where:   $E(d,k) \\rightarrow e$   $D(e,k) \\rightarrow d$   Here, $e$ is the encrypted data and $d$ can be obtained from $e$ only if the key $k$ is known.   pardon the inaqurate mathematical notations. style of representation is my only requirement here.   In this case where there is a single key for encryption and decryption, the operation is broadly classified as “symmetric-key cryptography”. Some of the popular examples of symmetric-key algorithms are AES, DES, RC4 etc.   The challenge in this type of cryptography is the secure transmission and storage of the key. In case of network communication, if this is the only type of cryptography available, then the key will have to be transmitted through some medium and this opens up an attack surface where an adversary would be able to get hold of the key.   This is addressed in the case of the next class of cryptographic algorithms called asymmetric key cryptography. In this case, we define a key pair which includes:      a public key $k_{pu}$   a private key $k_{pr}$   Here, any data encrypted using $k_{pu}$ can be drypted only using $K_{pr}$. So:   $E(d,k_{pu}) \\rightarrow e$   $D(e,k_{pr}) \\rightarrow d$   So, your public key can be published so that anyone interested in sending data to you can use it to encrypt it and be assured that it can be decrypted only using your private key that that you have securely stored. Since the private key is never transmitted, it greatly reduces the attack surface.   This approach is generally called a static key based asymmetric cryptography since it involves a stored key and it is not completely secure. When the key pair is generated on the fly and not reused, the pair is called an ephemeral key pair.   Popular algorithms in asymmetric/public-key world are RSA,DSA,ECDSA,DH etc. Of this, only RSA is capable of key exchange as well as practical data encryption. Algorithms like DH are used only for key exchange and DSA algorithms are used for signing digital certificates. We will visit these concepts soon.   In comparison to asymmetric algorithms, symmetric algorithms :      are faster and more efficient   are safer for a given key length   has a fixed over head in terms of increase in the size of encrypted data . (on the contrary RSA for instances increases the size of encrypted data by 10% of input)   key exchange   Key exchange is the process by which two parties can derive a common secret key over an unsecure network while an eavesdropper listening to the exchange process will not be able to derive the same key.   Diffie-Hellman (DH) and RSA are both popular algorithms that can be used for key exchange and the later involves the use of a pre-shared public-key.   One of the popular variants of DH is ephemeral DH (referred to as DHE in the context of TLS) where it does not involve the use of a stored private key for generation and exchange. This ensures “forward secrecy” which means that even if one of the exchanged keys are compromised, it does not lead to compromising other keys generated by the same system.   “Trapdoors” and one way functions   All these cryptographic algorithms revolve around the complexity of reversing some mathematical operation called a one-way function or a trapdoor. An example of such a trap door function is prime factorisation as used in RSA. If two prime numbers are “multiplied” to obtain a composite number, it is computationally exhaustive to generate those exact prime numbers if just the composite is known - provided that the primes are very large. Here multiplication is a modulus based operation and not normal arithmetic multiplication.   Digital Signature   A cryptographic signature or a digital signature is the encrypted MAC of the signed content. For example, when we say that a code file has been digitally signed, it means that a hash of the contents of the file has been generated and the hash has been encrypted with the private key of the signer. This encrypted hash is the signature of the file. The signers public key and the hash are attached to the original file so that a third party can verify the signature by decrypting the signature using the signers public key and comparing it to the hash of the file that he computed independently.   Sometimes, the private key is stored in a special hardware device like a USB dongle and can never be retrieved from it. Any signature operation to be performed will involve the hash being sent to the hardware and the signature being returned by the device.   More on this can be understood once you read about digital certificated below.   keys! keys!! keys!!!   All this while, we have been talking about keys. Essentially,as seen in certificates or key files, they would look like a very large string. However, these are encoded numbers that contains the various parameters that are needed by the algorithm to operate. The encoding is as per the RFC relevant to the algorithm being used.   For example, RSA private key, when decoded can give the following fields:   version           Version, modulus           INTEGER,  -- n publicExponent    INTEGER,  -- e privateExponent   INTEGER,  -- d prime1            INTEGER,  -- p prime2            INTEGER,  -- q exponent1         INTEGER,  -- d mod (p-1) exponent2         INTEGER,  -- d mod (q-1) coefficient       INTEGER,  -- (inverse of q) mod p otherPrimeInfos   OtherPrimeInfos OPTIONAL   Implementations like OpenSSL has functions that can take the encoded key string and extract these pieces of information from it to operate the corresponding algorithm.   To do this in a terminal, use the following command while passing the key to it.   openssl rsa -text -noout   Digital certificates   A digital certificate is a cryptographically backed proof that can authenticate a resource. Resource can be a domain, a server, a document, a piece of code etc.   Digital certificates are issued by trusted authorities and will contain the following information:   Serial Number  Subject Signature Algorithm Signature Issuer Valid-From Valid-To Key-Usage Public Key Fingerprint Algorithm Fingerprint    Here, subject is the entity for which the certificate is issued. This information will be verified by the certificate issuing authority (CA).   The most commonly used formate to store these details is the X509 specification. Once these pieces of information is passed through a X509 encoder, the output will be a string that can be safely stored and passed through the network.   A sample certificate stored in the browser looks like this:                                               Google's certificate example         As an entity (a server, a company or even a person), the following steps are to be followed to get a digital certificate.      Generate a public and private key pair and secure the private key.   Generate a certificate signing request (CSR) and send it to the CA along with proof of identity on the subject information in the request and certificate fee.This request would also contain the public key algorithm and public key from step 1.   The CA will add the signer details (including the CA public key, CA ID etc), signature and hashing (fingerprint) algorithms, validity dates, usage etc into the CSR and form a TBS (to-be-signed) certificate.   The CA will compute the hash of TBS, encrypt it using his private key and signature algorithm to form the fingerprint and attach it to the TBS to form a digital certificate.   This digital certificate will now be returned to the requester and he can now use it for the purposes listed in the key-usage field.   Going back to the example of signing a code file, when the digital certificate owner wants to sign a document, he uses his securely stored private key to encrypt the hash of the document and attach it to the document along with the digital certificate itself. When the party who received the document wants to verify the signature, he follows the following steps:      Ignore the fingerprint of the certificate. Now this is the same as the TBS that the CA used to generate the fingerprint. Compute the hash of this TBS using the finger print algorithm.   Decrypt the certificate fingerprint using the CAs signing algorithm and public key to get back the hash of the TBS computed by CA.   Compare the two hashes from 1 and 2. They will match only if the certificate issued by the CA is intact. If any bit in the certificate including keys, identity information etc has changed, the hash would be different.   Now that the validity of the certificate has been ensured, we can trust the identity of the signer.   Compute the hash of the document and keep aside the digest.   Using the public key of the signer in the certificate, that we just validated, decrypt the signature of the document.   compare the hashes in steps 5 and 6. They will match only if the contents of the document has not been altered after it was signed.   The point to note here is that we have trust the CA blindly and assume that he will not issue certificates to malicious entities or without proper proof of identity. Most operating systems and browsers will have an internal key-store that stores offline copies of root certificates that the CAs that the OS vendor trusts. Public keys from these built in certificates will be used to validate certificates that comes into the system.   Digital certificates and networking   Now that we know the foundational concepts used in digital certificate based authentication and encryption, let us understand how a digital certificate is used in the case of loading a web page and exchanging sensitive data with a server using the example of a browser.   When a browser loads a webpage using the https protocol, it first requests for the digital certificate held by the server. When the server passes on the certificate, it is validated as described in the Digital certificates section of this article. All major browsers will update its internal certificate store of trusted CAs with the latest and greatest root certificates. This list of certificates can be viewed from the browser properties window. These certificates are built into the browser at the time of compilation and will be updated and enhanced on every release.                                               firefox certificate store for builtin authorities            Here we are effectively offsetting our trust from the CAs to the browser vendor. The impractical alternative is to store certificates of all websites in the internet as a local copy in the browser. This shows the importance of downloading browsers from trusted sources.    Once this validation is complete, the browser will show an indication that the server can be trusted. In case of firefox, it is the green padlock icon in the address bar () .   As we discussed before, we need to rely on symmetric cryptographic algorithms for actual encryption of data stream. For this, a secure key needs to be negotiated between the server and the browser. The key exchange algorithm to be used for this will determines using a combination of cypher suite selected for the transaction and the algorithm mentioned in the certificate. For example, in the yahoo certificate shown below, RSA algorithm is to be used.                                               key exchange algorithm of yahoo.com            cypher suite exchange is a process in TLS/SSL handshake where the server and client exchanges the list of cypher suites that they can support and the highest common suite is selected for the session.    In case of mutual authentication, the server application will request a certificate from the client and validate it before giving it access to server resources.   ","categories": ["Articles","Tutorial"],
        "tags": ["IoT","Digital Certificates","Security","Networking"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/introduction-to-digital-certificates/",
        "teaser": null
      },{
        "title": "Understanding X.509 Certificate Structure",
        "excerpt":"  NOTE: If you are totally new to digital certificates and the ecosystem, please read my article introducing the basics of Digital certificates here before continuing with this article.   Introduction   Digital certificates are now prevalent and its significance is growing in the current market where an increasing number of embedded devices are deployed into secure networks.          Table of contents     Introduction   Basics           Abstract Syntax Notation One (ASN.1)       Object identifier (OID)                   OID encoding (for DER)                       Distinguished Encoding Rules (DER)       Base64 encoding       Privacy Enhanced Mail format (PEM)       Key identifiers and usage       Public Key Cryptography Standards (PKCS)       Key formats           Parsing a certificate        Even with increasing significance of Digital certificates, engineers are largely unaware of underlying concepts and standards that dictate certificate formats and contents. This is primarily because of the availability of hardened and stable software like OpenSSL that abstracts many things under friendly top level APIs. However, with the new wave of embedded devices using these technologies, we need to understand and re-visit the underlying technology to optimize them for new-age requirements.   When it comes to parsing and understanding a X.509 certificate by hand, information is scattered all over the place. This article aims at bringing in the basics together without going too much into specification details so that you can use it as a starting point to understand where and what to look for.   Basics   First a little bit of glossary and background. Some seemingly disconnected terminology and concepts will be introduced in this section.   Abstract Syntax Notation One (ASN.1)   ASN.1 is an abstract syntax that defines a machine encoding independent way of representing ( encoding, transmitting and decoding) data. Being an abstract syntax, it only defines the structure of a data tree and leaves actual data representation (encoding) to application specific implementations. In other (simple) words, ASN.1 is a schema language. To give an idea of what this means, ASN.1 can be used to define the schema for formats like JSON, XML etc.   A tip for later: In the schema representation , a SEQUENCE models an ordered collection of variables of different type   Object identifier (OID)   In the context of digital certificates, OID refers to ITU-T maintained tree based object identifier hierarchy that allows unambiguous representation of information in the form of a dot separated number.   For example 2.5.4.8 is the doted OID representation of {joint-iso-itu-t(2) ds(5) attributeType(4) stateOrProvinceName(8)} . A certificate will use this OID as the tag of a string tthat represents the state or province of the entity to which a certificate was issued.   OID Repository is a good place to lookup entities in the OID tree.   OID encoding (for DER)   Special encoding rules have been defined to represent OIDs inside the ASN.1 tree . Note that ASN.1 by itself is agnostic of encoding rules. The rule illustrated below is part of DER specification . (More on DER in the next session)   For directly encoded OID components, the individual octets should have first bit as 0.   Consider a dummy OID 1.2.62329.4. OID binary encoding for octet representation is done using the following rules.   Step 1: The first two components (A.B) are encoded as 40*A+B . In this case, it would be 2A (Hex of 42)   Step 2: Since the third component (62329) has more than 7 bits in it, we need to split it into multiple octets with only leading octet having MSB =1.   For this, first convert the number into binary and split into groups of 7 bits (Pad 0s to form a leading set to form an octet) 000_0011 110_0110 111_1001   Set MSB of all octets except last to 1. Set MSB of last octet to 0. Now, convert resulting decimal into hex  1000_0011 1110_0110 0111_1001 = 0x83E679   Step 3: since the last component has a value with less than 7 bits, it can be converted to hex directly.   Final HEX encoded OID of 1.2.62329.4 would be 2A 83 E6 79 04   Distinguished Encoding Rules (DER)   DER defines rules for unambiguous encoding of data into ASN.1. This is one of the formats commonly used in cryptographic systems.   A portion an ASN.1 tree within a certificate is given below:   Here a SET contains a SEQUENCE that contains an IUT-T OID ObjectIdentifier and a “value” for the identified object. In the example here, all OIDs have been mapped to corresponding string notation as well. DER defines that for OID 2.3.4.6, the next element in SEQUENCE should be of type PrintableString and further goes on into definition of how a PrintableString appears in ASN.1 tree.   SET   SEQUENCE     ObjectIdentifier countryName (2 5 4 6)     PrintableString 'IN'   The HEX notation of this portion of the tree encoded as per DER would be:   31 0B 30 09 06 03 55 04 06 13 02 49 4E   This means:   31 - Type is SET 0B - length of immediate contents is 11 bytes. 30 - Type is SEQUENCE 09 - Length is 9 bytes 06 - Type is ObjectIdentifier 03 - length is 3 bytes 55 04 06 - OID is 2.5.4.6 (see OID encoding section above for explanation)  13 - Type is  PrintableString 02 - length is 2 49 4E - ASCII of \"IN\"    Essentially ASN.1 DER encoding is a tag, length, value encoding system for each element in the certificate.   Base64 encoding   Base64 is an encoding scheme used to represent binary data as an ASCII string using radix64 (just like binary representation uses radix2). Characters chosen to represent encoding depends on the standard used . For example, MIME uses A-Z, a-z , 0-9 and +/.   There are well deifined methods to handle corner cases and there are variants (like URL safe Base64) to handle different scenarios.   Privacy Enhanced Mail format (PEM)   Though the e-mail securing proposal in PEM was not a success, PEMs certificate and key encoding format described by PEM is one of the most widely used for certificate storage and transmission. PEM files are used for storage of single certificates, complete certificate chain or just keys.   Essentially PEM is a BASE64 encoded DER file.   Key identifiers and usage   X.509 PKI certificate format defines some methods to generate unique identifiers for a public key. This is especially useful to identify the relevant keys when a CA has multiple active Public keys that can be used to sign certificates. These IDs appear in two forms in a certificate as part of certificate extensions.      authorityKeyIdentifier - key ID of CA public key used to sign the certificate.   subjectKeyIdentifier - key ID of public key present in the certificate. This is a mandatory field for a certificate marked as CA   Two recommended methods for generating unique key IDs from public keys as per the x.509 RFC are:      compute 160 bit SHA-1 hash of the BIT STRING value of subjectPublicKey (excluding tag and length)   type indicator 0100 followed by least significant 60 bits of subjectPublicKey (excluding tag and length)   Other methods for generating unique IDs can also be used.   Each key will be marked with a usage bitmap in the “extension” portion of the certificate. The defined bitmap and meanings are :                  Bit position       Name       Description                       0       digitalSignature       subject public key is used for verifying digital signatures (except CRL and its own)                 1       nonRepudiation       used to provide a non-repudiation service that protects against the signing entity falsely denying some action                 2       keyEncipherment       subject public key is used for enciphering private or secret keys                 3       dataEncipherment       subject public key is used for directly enciphering raw user data without the use of an intermediate symmetric cipher                 4       keyAgreement       subject public key is used for key agreement (like in the case of D-H                 5       keyCertSign       subject public key is used for verifying signatures on public key certificates. This also mandates marking the certificate as a CA certificate                 6       cRLSign       subject public key is used for verifying signatures on certificate revocation lists                 7       encipherOnly       When the encipherOnly bit is asserted and the keyAgreement bit is also set, the subject public key may be used only for enciphering data while performing key agreement                 8       decipherOnly       When the decipherOnly bit is asserted and the keyAgreement bit is also set, the subject public key may be used only for deciphering data while performing key agreement.           So, for exmple, 0xa0 for keyUsage (2.5.29.15) would mean that the key can be used for digitalSignature and keyEncipherment   Public Key Cryptography Standards (PKCS)   PKCS is a set of asymmetric crypto standards initially published by RSA and later enhanced by Microsoft. It describes a series of Public key infrastructure (PKI) assisting standards for items like certificate syntax, token interface, certificate signing request, private key storage etc.   The pkcs12 standard defines a password protected container format for storing private and public keys.   Key formats   Each cryptographic algorithm requires a set of instance specific parameters to be fed into it for operation. The generalized term used for these set of parameters is called a “Key”.   In case of certificates and keys stored in certificates, there are well defined formats for storing key parameters for every supported cipher suite .   The generalised ASN.1 Schema for public key storage in a certificate as per TLS 1.2 RFC is :   SubjectPublicKeyInfo ::= SEQUENCE {     algorithm AlgorithmIdentifier,     publicKey BIT STRING }   For example, ECCDSA public key is essentially a set of two points in the curve. These points are encoded as an octet string and placed inside the certificate. Section 2.3.4 (OctetString-to-EllipticCurvePoint Conversion) of SEC 1: Elliptic Curve Cryptography V1 spec defines how to decode an octet string representation of the public key (that is generally seen within the certificate) into curve points (a,b).   In a sample certificate using this algorithm, the following will appear in its ASN.1 tree:   SEQUENCE   ObjectIdentifier ecPublicKey (1 2 840 10045 2 1)   ObjectIdentifier secp256r1 (1 2 840 10045 3 1 7)   BITSTRING 0004664fca46ef4c681b74760bd89534588794d177252788432e63f8a7d81d287d9e474e5402beb90a75ae4b2759ff35799223e1ff14aa00c7489bb68c21142edccd   DER has defined rules for decoding BIT STRING which is essentially a padded ASN.1 representation. In this case, it will be decoded into an octet string that can further be decoded using the aforementioned SEC 1 spec.   Parsing a certificate   In “Server Certificate” stage of TLS handshake, the server presents a “certificate_list” to the client. As per RFC5246 (TLS 1.2), a certificate list is a sequence (chain) of certificates.  The sender’s certificate MUST come first in the list.  Each following certificate MUST directly certify the one preceding it. This chain transmission happens in  ASN.1 schema defined in the rfc and can have up to a maximum of 2^24-1 certificates in the chain.   The root certificate can be omitted as it does not make sense to validate a chain with a root sent in the chain.   When exported, certificates are generally stored between string tags:   -----BEGIN CERTIFICATE----- &lt;encoded certificate&gt; -----END CERTIFICATE-----   We will use the certificate of https://example.com to understand the basic structure of an X.509 certificate.   Following command will fetch the certificate chain:   openssl s_client -showcerts -connect www.example.com:443 &lt;/dev/null   This will show the servers certificate as well as the intermediate CAs certificate . We will focus on the first certificate (server certificate) for this exercise   The certificate contents is :   -----BEGIN CERTIFICATE----- MIIF8jCCBNqgAwIBAgIQDmTF+8I2reFLFyrrQceMsDANBgkqhkiG9w0BAQsFADBw MQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3 d3cuZGlnaWNlcnQuY29tMS8wLQYDVQQDEyZEaWdpQ2VydCBTSEEyIEhpZ2ggQXNz dXJhbmNlIFNlcnZlciBDQTAeFw0xNTExMDMwMDAwMDBaFw0xODExMjgxMjAwMDBa MIGlMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5pYTEUMBIGA1UEBxML TG9zIEFuZ2VsZXMxPDA6BgNVBAoTM0ludGVybmV0IENvcnBvcmF0aW9uIGZvciBB c3NpZ25lZCBOYW1lcyBhbmQgTnVtYmVyczETMBEGA1UECxMKVGVjaG5vbG9neTEY MBYGA1UEAxMPd3d3LmV4YW1wbGUub3JnMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A MIIBCgKCAQEAs0CWL2FjPiXBl61lRfvvE0KzLJmG9LWAC3bcBjgsH6NiVVo2dt6u Xfzi5bTm7F3K7srfUBYkLO78mraM9qizrHoIeyofrV/n+pZZJauQsPjCPxMEJnRo D8Z4KpWKX0LyDu1SputoI4nlQ/htEhtiQnuoBfNZxF7WxcxGwEsZuS1KcXIkHl5V RJOreKFHTaXcB1qcZ/QRaBIv0yhxvK1yBTwWddT4cli6GfHcCe3xGMaSL328Fgs3 jYrvG29PueB6VJi/tbbPu6qTfwp/H1brqdjh29U52Bhb0fJkM9DWxCP/Cattcc7a z8EXnCO+LK8vkhw/kAiJWPKx4RBvgy73nwIDAQABo4ICUDCCAkwwHwYDVR0jBBgw FoAUUWj/kK8CB3U8zNllZGKiErhZcjswHQYDVR0OBBYEFKZPYB4fLdHn8SOgKpUW 5Oia6m5IMIGBBgNVHREEejB4gg93d3cuZXhhbXBsZS5vcmeCC2V4YW1wbGUuY29t ggtleGFtcGxlLmVkdYILZXhhbXBsZS5uZXSCC2V4YW1wbGUub3Jngg93d3cuZXhh bXBsZS5jb22CD3d3dy5leGFtcGxlLmVkdYIPd3d3LmV4YW1wbGUubmV0MA4GA1Ud DwEB/wQEAwIFoDAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUHAwIwdQYDVR0f BG4wbDA0oDKgMIYuaHR0cDovL2NybDMuZGlnaWNlcnQuY29tL3NoYTItaGEtc2Vy dmVyLWc0LmNybDA0oDKgMIYuaHR0cDovL2NybDQuZGlnaWNlcnQuY29tL3NoYTIt aGEtc2VydmVyLWc0LmNybDBMBgNVHSAERTBDMDcGCWCGSAGG/WwBATAqMCgGCCsG AQUFBwIBFhxodHRwczovL3d3dy5kaWdpY2VydC5jb20vQ1BTMAgGBmeBDAECAjCB gwYIKwYBBQUHAQEEdzB1MCQGCCsGAQUFBzABhhhodHRwOi8vb2NzcC5kaWdpY2Vy dC5jb20wTQYIKwYBBQUHMAKGQWh0dHA6Ly9jYWNlcnRzLmRpZ2ljZXJ0LmNvbS9E aWdpQ2VydFNIQTJIaWdoQXNzdXJhbmNlU2VydmVyQ0EuY3J0MAwGA1UdEwEB/wQC MAAwDQYJKoZIhvcNAQELBQADggEBAISomhGn2L0LJn5SJHuyVZ3qMIlRCIdvqe0Q 6ls+C8ctRwRO3UU3x8q8OH+2ahxlQmpzdC5al4XQzJLiLjiJ2Q1p+hub8MFiMmVP PZjb2tZm2ipWVuMRM+zgpRVM6nVJ9F3vFfUSHOb4/JsEIUvPY+d8/Krc+kPQwLvy ieqRbcuFjmqfyPmUv1U9QoI4TQikpw7TZU0zYZANP4C/gj4Ry48/znmUaRvy2kvI l7gRQ21qJTK5suoiYoYNo3J9T+pXPGU7Lydz/HwW+w0DpArtAaukI8aNX4ohFUKS wDSiIIWIWJiJGbEeIO0TIFwEVWTOnbNl/faPXpk5IRXicapqiII= -----END CERTIFICATE-----   This is in base64 encoded format. This can be decoded into the follwoing hex string:   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   When this is passed through an ASN.1 decoder, we get the following tree:   SEQUENCE   SEQUENCE \t[0] \t  INTEGER 02 \tINTEGER 0e64c5fbc236ade14b172aeb41c78cb0 \tSEQUENCE \t  ObjectIdentifier SHA256withRSA (1 2 840 113549 1 1 11) \t  NULL \tSEQUENCE \t  SET \t\tSEQUENCE \t\t  ObjectIdentifier countryName (2 5 4 6) \t\t  PrintableString 'US' \t  SET \t\tSEQUENCE \t\t  ObjectIdentifier organization (2 5 4 10) \t\t  PrintableString 'DigiCert Inc' \t  SET \t\tSEQUENCE \t\t  ObjectIdentifier organizationalUnit (2 5 4 11) \t\t  PrintableString 'www.digicert.com' \t  SET \t\tSEQUENCE \t\t  ObjectIdentifier commonName (2 5 4 3) \t\t  PrintableString 'DigiCert SHA2 High Assurance Server CA' \tSEQUENCE \t  UTCTime 151103000000Z \t  UTCTime 181128120000Z \tSEQUENCE \t  SET \t\tSEQUENCE \t\t  ObjectIdentifier countryName (2 5 4 6) \t\t  PrintableString 'US' \t  SET \t\tSEQUENCE \t\t  ObjectIdentifier stateOrProvinceName (2 5 4 8) \t\t  PrintableString 'California' \t  SET \t\tSEQUENCE \t\t  ObjectIdentifier locality (2 5 4 7) \t\t  PrintableString 'Los Angeles' \t  SET \t\tSEQUENCE \t\t  ObjectIdentifier organization (2 5 4 10) \t\t  PrintableString 'Internet Corporation for Assigned Names and Numbers' \t  SET \t\tSEQUENCE \t\t  ObjectIdentifier organizationalUnit (2 5 4 11) \t\t  PrintableString 'Technology' \t  SET \t\tSEQUENCE \t\t  ObjectIdentifier commonName (2 5 4 3) \t\t  PrintableString 'www.example.org' \tSEQUENCE \t  SEQUENCE \t\tObjectIdentifier rsaEncryption (1 2 840 113549 1 1 1) \t\tNULL \t  BITSTRING 003082010a0282010100b340962f61633e25c197ad6545fbef1342b32c9986f4b5800b76dc06382c1fa362555a3676deae5dfce2e5b4e6ec5dcaeecadf5016242ceefc9ab68cf6a8b3ac7a087b2a1fad5fe7fa965925ab90b0f8c23f13042674680fc6782a958a5f42f20eed52a6eb682389e543f86d121b62427ba805f359c45ed6c5cc46c04b19b92d4a7172241e5e554493ab78a1474da5dc075a9c67f41168122fd32871bcad72053c1675d4f87258ba19f1dc09edf118c6922f7dbc160b378d8aef1b6f4fb9e07a5498bfb5b6cfbbaa937f0a7f1f56eba9d8e1dbd539d8185bd1f26433d0d6c423ff09ab6d71cedacfc1179c23be2caf2f921c3f90088958f2b1e1106f832ef79f0203010001 \t[3] \t  SEQUENCE \t\tSEQUENCE \t\t  ObjectIdentifier authorityKeyIdentifier (2 5 29 35) \t\t  OCTETSTRING, encapsulates \t\t\tSEQUENCE \t\t\t  [0] 5168ff90af0207753cccd9656462a212b859723b \t\tSEQUENCE \t\t  ObjectIdentifier subjectKeyIdentifier (2 5 29 14) \t\t  OCTETSTRING, encapsulates \t\t\tOCTETSTRING a64f601e1f2dd1e7f123a02a9516e4e89aea6e48 \t\tSEQUENCE \t\t  ObjectIdentifier subjectAltName (2 5 29 17) \t\t  OCTETSTRING, encapsulates \t\t\tSEQUENCE \t\t\t  [2] www.example.org \t\t\t  [2] example.com \t\t\t  [2] example.edu \t\t\t  [2] example.net \t\t\t  [2] example.org \t\t\t  [2] www.example.com \t\t\t  [2] www.example.edu \t\t\t  [2] www.example.net \t\tSEQUENCE \t\t  ObjectIdentifier keyUsage (2 5 29 15) \t\t  BOOLEAN TRUE \t\t  OCTETSTRING, encapsulates \t\t\tBITSTRING 05a0 \t\tSEQUENCE \t\t  ObjectIdentifier extKeyUsage (2 5 29 37) \t\t  OCTETSTRING, encapsulates \t\t\tSEQUENCE \t\t\t  ObjectIdentifier serverAuth (1 3 6 1 5 5 7 3 1) \t\t\t  ObjectIdentifier clientAuth (1 3 6 1 5 5 7 3 2) \t\tSEQUENCE \t\t  ObjectIdentifier cRLDistributionPoints (2 5 29 31) \t\t  OCTETSTRING, encapsulates \t\t\tSEQUENCE \t\t\t  SEQUENCE \t\t\t\t[0] \t\t\t\t  [0] \t\t\t\t\t[6] http://crl3.digicert.com/sha2-ha-server-g4.crl \t\t\t  SEQUENCE \t\t\t\t[0] \t\t\t\t  [0] \t\t\t\t\t[6] http://crl4.digicert.com/sha2-ha-server-g4.crl \t\tSEQUENCE \t\t  ObjectIdentifier certificatePolicies (2 5 29 32) \t\t  OCTETSTRING, encapsulates \t\t\tSEQUENCE \t\t\t  SEQUENCE \t\t\t\tObjectIdentifier (2 16 840 1 114412 1 1) \t\t\t\tSEQUENCE \t\t\t\t  SEQUENCE \t\t\t\t\tObjectIdentifier (1 3 6 1 5 5 7 2 1) \t\t\t\t\tIA5String 'https://www.digicert.com/CPS' \t\t\t  SEQUENCE \t\t\t\tObjectIdentifier (2 23 140 1 2 2) \t\tSEQUENCE \t\t  ObjectIdentifier authorityInfoAccess (1 3 6 1 5 5 7 1 1) \t\t  OCTETSTRING, encapsulates \t\t\tSEQUENCE \t\t\t  SEQUENCE \t\t\t\tObjectIdentifier (1 3 6 1 5 5 7 48 1) \t\t\t\t[6] http://ocsp.digicert.com \t\t\t  SEQUENCE \t\t\t\tObjectIdentifier (1 3 6 1 5 5 7 48 2) \t\t\t\t[6] http://cacerts.digicert.com/DigiCertSHA2HighAssuranceServerCA.crt \t\tSEQUENCE \t\t  ObjectIdentifier basicConstraints (2 5 29 19) \t\t  BOOLEAN TRUE \t\t  OCTETSTRING, encapsulates \t\t\tSEQUENCE {}   SEQUENCE \tObjectIdentifier SHA256withRSA (1 2 840 113549 1 1 11) \tNULL   BITSTRING 0084a89a11a7d8bd0b267e52247bb2559dea30895108876fa9ed10ea5b3e0bc72d47044edd4537c7cabc387fb66a1c65426a73742e5a9785d0cc92e22e3889d90d69fa1b9bf0c16232654f3d98dbdad666da2a5656e31133ece0a5154cea7549f45def15f5121ce6f8fc9b04214bcf63e77cfcaadcfa43d0c0bbf289ea916dcb858e6a9fc8f994bf553d4282384d08a4a70ed3654d3361900d3f80bf823e11cb8f3fce7994691bf2da4bc897b811436d6a2532b9b2ea2262860da3727d4fea573c653b2f2773fc7c16fb0d03a40aed01aba423c68d5f8a21154292c034a220858858988919b11e20ed13205c045564ce9db365fdf68f5e99392115e271aa6a8882   The basic tree structure and relevent information in this tree are annoted below:   TLS version (02 for V03) Serial Number (0e64c5fbc236ade14b172aeb41c78cb0) Certificate signature Algorithm (SHA256withRSA) Issuer \tcountryName [CN] - (US) \torganization [O] - (DigiCert Inc) \torganizationalUnit [O] - (www.digicert.com) \tcommonName [CN] - (DigiCert SHA2 High Assurance Server CA) Validity \tnotBefore (151103000000Z -Tuesday, November 03, 2015 5:30:00 AM) \tnotAfter (181128120000Z -  Wednesday, November 28, 2018 5:30:00 PM) Subject \tcountryName [C] - (US) \tstateOrProvinceName [ST] - (California) \tlocality [L] - (Los Angeles) \torganization [O] - (Internet Corporation for Assigned Names and Numbers)     organizationalUnit [OU] - (Technology) \tcommonName [CN] - (www.example.org) SubjectPublicKeyInfo \tSubject Public Key Algorithm - (rsaEncryption) \tSubject Public Key - (represented as BITSTRING) Extensions \tAuthorityKeyIdentifier  - (5168ff90af0207753cccd9656462a212b859723b) \tsubjectKeyIdentifier  -  (a64f601e1f2dd1e7f123a02a9516e4e89aea6e48) \tsubjectAltName - (list of identities bound to the same subject entity) \tkeyUsage - (digitalSignature and keyEncipherment) \textended key usage - (serverAuth and clientAuth) \tcRLDistributionPoints - (2 links to get the revoked serial numbers) \tcertificatePolicies \t\tssl-server-certificates \t\t\tcertificate policy statement link provide by CA \t\torganization-validated (legal) \tauthorityInfoAccess \t\tOnline Certificate Status Protocol (OCSP)  - (CA OCSP URL) \t\tCertificate Authority Issuers (caIssuers)  - (link pointing to CA cert for verification) \tbasicConstraints (used to identify whether this is a CA . Not a CA in this case) certificate signature algotighm - (SHA256withRSA) certificate signature value - (bitstring)   You can see the same set of information in Firefox certificate viewer.                                               example.com cert as parsed by Firefox         ","categories": ["Articles","Tutorial"],
        "tags": ["IoT","Digital Certificates","Security","Networking"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/understanding-X.509-certificate-structure/",
        "teaser": null
      },{
        "title": "JTAG basics and usage in microcontroller debugging",
        "excerpt":"  Introduction   JTAG stands for Joint Test Access Group and is an association that was formed initially to derive a specification to test connectivity between chips in a PCB. Later this spec was formalized into an IEEE standard (1149.1), entitled “Standard Test Access Port and Boundary-Scan Architecture”. The latest update to this spec was done in 2013.   Colloquially JTAG refers to the debug and programming dongle that is used to communicate to a microcontroller during development/hacking.   The original intention of this specification was to have standard “boundary scan” provisions in an IC that will allow monitoring and controlling the I/O pads so as to test connectivity between devices in a PCB. However, open provisions of the standard was soon exploited by chip manufacturers to use it for internal circuitry testing as well as controlling hardware and achieve effective programming and debugging over a serial interface.   JTAG standard is more like XML that defines a lingo that allows a lot of implementation flexibility. This has lead to large variety of implementations over the JTAG standard that caters to needs of individual chip makers.   In this article we will be focusing on the debug and programming aspects of JTAG than boundary scan aspect of it.   JTAG Operation   JTAG interface includes 5 lines:      Test data input (TDI)   Test data output (TDO)   Test mode select (TMS)   Test clock input (TCK)   Test reset input (TRST*) - optional   These pins connects into a Test Access Port (TAP) hardware within a device.TAP controller hosts a state machine that is defined in the spec.                                               JTAG TAP controller statemachine         This state machine can be used to request the TAP controlled to talk to a device specific debug engine. This is illustrated in the figure below:                                               TAP and device debug interface         Examples of device debug engines include “EJTAG” in case of MIPS, “OCI” in case of some 8051s etc. They tie into the CPU logic to enable provisions like stepping through, PC redirection, instruction and data trace etc. We will later look into the specifics of one of these. For now, we will understand how JTAG can be used to communicate to these implementations in a generalized way.   To understand how JTAG TAP state-machine, operates, we need to first look into some more details of a generalized TAP pseudo implementation.   TAP controller contains an “Instruction Register (IR)” and one or more “Data Registers (DR)” that can be accessed over the 4-wire JTAG interface. At any given point in time of operation, the TAP controller will place one of these between the TDI and TDO lines. Data will be shifted into the register from TDI on the raising edge of TCK and data will be captured into TDO in the falling edge of TCK. So, to read data from a particular register between TDI and TDO, some dummy bits can be shifted into TDI that causes data present in the register to be pushed out via TDO.   TMS line is used to navigate the state-machine. State transition values shown in the state-machine are TMS values at the raising edge of TCK. So, for instance, to go into shift-IR state from Test-logic-reset, TMS should have the following values at the raising edges of subsequent TCKs : 01100 . Once the tap reaches shift-IR state, it will stay there as long as TMS is 0 . That means, TCK can be used for other purposes , without moving out of the state.      The state-machine is designed in such a way that holding TMS high for 5 subsequent raising edges of TCK will take it into “test-logic-reset” state irrespective of the current state. This is a quick escape hatch in case you screw-up :grimacing:    When the state-machine is in any of the IR states (like “shift-IR”, “capture-IR”, “update-IR” etc), the instruction register will be placed between TDI and TDO. Depending on the instruction pushed into the IR (and later captured), one of the data registers will be placed between TDI and TDO in the next DR state.   Length of the instruction register, binary encoding of an instruction and the instructions that are supported in a device are implementation dependent. However, certain instructions like IDCODE are recommended to be implemented on all devices. Implementation details are generally documented in the devices programming manual as well as in the device’s BSDL file. BSDL stands for boundary scan description language and is a subset of VHDL specification. Most JTAG based tools like Lauterbach Trace32 IDE has a provision to import a BSDL file and the instruction mnemonics can be used in a portable manner.   Assuming a 5 bit IR, and the binary encoding 00010 for IDCODE, placing the pattern in the TDI pin serially (LSB first) on subsequent raising edges of TCK while in shift-IR state will push in the IDCODE instruction into the IR that is between TDI and TDO in that state.      while in Test-log-c-reset state, device ID register will be the register placed between TDI and TDO in the next data phase.    While the last bit is being latched in, TMS need to be modified to go into capture-IR state. It is in this state that the TAP controlled decodes the instruction that has just been placed in IR. Next clock will push the state-machine out of capture-IR state into exit1-IR or “shift-IR” based on TMS state.      exit1-IR state will terminate the scanning process and moving into subsequent states to reach capture-IR from shift-IR will not alter the contents that were just shifted into IR    Once an instruction has been captured, the corresponding data register will be placed between TDI and TDO in the next “capture-DR” state. In this case, a data register containing the device identifier will be placed between TDI and TDO. So, once the TAP enters “capture-DR” state, contents of the ID register can be serially shifted out into TDO line at subsequent falling edges of TCK.      while the data register is being shifted out at falling edges of TCK, data in TDI will be shifted into the same register. So, assuming that TDI is held low during the process of reading out data via TDO, the register will end up with all 0s at the end of read.    MIPS debug subsystem   Since JTAG has a lot of implementation specific provisions, a specific processor family should be selected to understand how debugging (and programming) is achieved using JTAG.   For this study, we will be selecting  MIPS32 family that has a debug subsystem named EJTAG that ties into the TAP and CPU debug features. The spec is available here.   EJTAG extensions and external virtual memory access   One of the primary features of MIPS with EJTAG extensions is its ability to virtually map the processors address space into the debugger. This essentially means that the processor will be able to execute instructions and process data coming from the debug probe instead of its own in-device memory (RAM/executable flash etc ). One of the ways to trigger this mode of operation is the EJTAGBOOT JTAG instruction that will cause the processor to reset and start accessing instructions from kseg3 that is virtually mapped into the TAP controller. (another mechanism is the SDBBP instruction).   Reset following EJTAGBOOT would would put the system in debug mode where all access restrictions are removed. The following bits in EJTAG Control Register (ECR)  will be set following this reset.      Debug Interrupt Exception Request bit (EjtagBrk) indicating that a debug interrupt exception request is pending   Debug Exception Vector Control Location bit (ProbTrap) indicating that the debug exception vector is at 0xFF200200 that falls within DMSEG region instead of 0xBFC00480 in normal boot case   Processor Access Service Control bit (ProbeEn) indicating that the “probe” services processor accesses to DMSEG.      EJTAG control register is a 32 bit JTAG data registers that can be accessed using EJTAG CONTROL instruction)       Rocc bit in ECR remains 1 following a reset until cleared by the probe. This is to force the probe to acknowledge a device reset. Until it is cleared, update-DR will not update the control register.    CONTROL JTAG instruction will place the ECR between TDI and TDO for access in data phase of the TAP controller. When an EJTAG memory access is triggered following a debug reset with probe access the following is applicable:      PrAcc bit in control register will be set to 1   ADDRESS JTAG instruction can be used to shift out the 32-bit address that the CPU is trying to access   DATA JTAG instruction can be used to shift in/out the 32-bit data that the CPU is trying to read/write   Psz will indicate size of data (0 for byte through 3 for triple word)   PRnW bit in control register will indicate whether it is a read (0) or write(1) operation pending on the address in ADDRESS register by the CPU.   CPU will be stalled until PrAcc bit is cleared . EJTAG probe (and debug software like the IDE) periodically polls the TAP control register for PrAcc and handles the request accordingly by shifting in/out data and clearing PrAcc once data access requested by CPU is ready.   This set of provisions lets the MIPS CPU execute instructions that are provided over JTAG. When this is used in conjunction with the CPU debug features that we will be seeing soon, a full fledged debugging system can be developed over the JTAG interface.      CPU stalls with PrAcc only when access to dmseg addresses (starting at 0xFF20_0000) are made. So, for instance if an execution passed from the probe tries to access a RAM location, CPU will go ahead and fetch data from RAM and stall only for next instruction to be passed on via the probe (when PC starting at a DMSEG address is incremented by 1). In case of a jump to a non-dmseg address, CPU will jump to that location and continue execution and not stall until the next access to dmseg address is made.    CPU debug features   To enable extensive debug support, MIPS CPUs define a “debug” mode where the system enters a un-restricted access mode. dseg (where DMSEG and DRSEG resides) is active mapped only in this mode and this allows access to all hardware resources used for debugging as well as to all memory regions (Generally, some memory regions are accessible in kernel mode and not in user mode).   Debug mode is entered via a debug exception and the processor will be in debug mode when DM bit of CPU debug register is set to 1. DRET instruction makes the system exit from debug mode. DEPC register contains the PC address for return from debug exception   MIPS CoProcessor 0 (CP0) contains a Debug register (reg 23) that provides information about behavioural aspects of the system in debug mode. Only DM bit that shows current debug mode state and VER bit that shows EJTAG version are valid for read in non-debug mode. Other than the imprecise exception configuration bits, none of the other bits are writable in this register.   more details to follow  ","categories": ["Articles","Tutorial"],
        "tags": ["Microcontrollers","FPGA","JTAG"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/JTAG-basics-and-usage-in-microcontroller-debugging/",
        "teaser": null
      },{
        "title": "Secure firmware upgrade for embedded systems",
        "excerpt":"  If you want more clarity on digital certificates before reading this article, please go through previous articles on digital certificates and X.509 certificates   Introduction   One of the features of connected devices that attract businesses to it is the capability to do a remote device firmware update. However, this is a double edged sword that can open up a range of hostile activities targeting your product if this capability is not carefully architected. This can range from IP theft to bringing down half the internet (ref: IoT DDoS attack on Dyn) to threats to life (like the Jeep hack) and personal information leak. The threat of your product line becoming an army of zombie warriors is no joke.   This entry captures general principles, architecture, guidelines and good practices associated with secure over the air (OTA) and over the host (OTH) firmware upgrade for an embedded device. Final implementation could have device specific adaptations.   Secure firmware upgrade refers to the ability to ensure E2E credibility and integrity of update software being downloaded into a device.   Terminology:      Source:            The origin point of firmware to be upgraded (w.r.t the device). Some examples are:                    A host software that can serve an image over interfaces like SPI, UART, USB etc in case of OTH           A server from which the device can “pull” an upgrade image over protocols like HTTP file transfer, FTP etc           A web interface (hosted on the device) that accepts a file upload           An SD card or a USB flash drive containing an upgrade image.                           Code protection (CP)            The ability of a device to prevent readout of any component of its memory by un-intended parties like a debug probe.           TPM and hardware firewalls            Trusted platform modules are essentially crypto co-processor sub-systems with self-contained memories and processing engines that can execute trusted routines without exposing critical information to untrusted parts of the system.       Hardware firewalls enables storage of keys and certificates in special (factory) modes into the system. Once out of the assembly line, these locations will be accessible only to crypto engines and TPMs and information cannot be extracted out of these locations.           Secure Boot            While we are concentrating more on secure transmission and authentication of an image, secure boot is worth mentioning.       Secure boot is the process by which a bootloader operating out of a TPM or a read only partition validates the signature of an image from which a device is about to boot from. This will be done every time the device reboots and will ensure that the firmware is un-tampered.           Architecture primitives   Assuming a code protected device with appropriate TPMs and memory firewalls, the overall requirements for secure firmware upgrade would include (but not limited to):      Images should be downloaded from authenticated sources.            Source can be a host software (OTH) , a server or a web page for firmware upload (OTA).           Image source should confirm device identity before allowing download.   Images transmitted from source to device should be encrypted using a session specific key.   Images stored in the source should be encrypted using a transport key.   Images stored in external storage should always be encrypted using a device specific key.   Downloaded image should be authenticated before being applied to the device.   Upgrade software (that applies the image) should work in principle of least privilege and should monitor addresses being indicated in the image            Eg1: Boot  indicator regions (that indicates whether an image is good to boot) should not be addressed in an image       Eg2: Ideally, certificates should never be upgraded OTA/OTH.                    CRLs can be upgraded OTA/OTH based on business logic.                           Depending on business logic, there can be a provision to write to certificate stores. If this is to be implemented, it should be done by a least privilege, read-only, secure firmware.   What this essentially means is: Once firmware leaves source memory it will always be encrypted using a device specific key until it is applied (flashed) into a code protected device. Additionally, an image will be applied to the device only if it can be authenticated to be from a trusted source.                                               Secure upgrade load flow         Caveats   Please keep the following caveats in mind before reading further / implementing a device specific design.      Public key cryptography is notoriously slow (especially without hardware acceleration). Decrypting an image will consume more resources (time, CPU, mem etc).            If image integrity is the only concern, a digital signature will suffice.           Image might have to be split into smaller signed chunks in case of resource constrained systems            Eg: in case of a dual panel flash without an external image store , chunks of the image will have to be downloaded into device RAM, decrypted, authenticated and written directly into the device flash before continuing.           The whole design (as in TLS and other network security measures) revolves around the following system conditions:            The device is capable of storing and operating on an uncompromised chain of trust.       Device keys can remain un-exposed and ephemeral keys are used whenever possible.       Use of un-compromised crypto algorithms and methodologies.           Image source mutual authentication   Mutual authentication refers to the process by which a device and server (/source) validates each other’s identity. The corner stone to this process is PKI and is the same as that used in network based financial transactions.   The first step in this process is for a device to fetch and validate the server’s certificate as part of TLS handshake. Next server requests device certificate as part of the client authentication process of TLS (CertificateRequest stage of TLS handshake process).  The client response will contain its certificate as well as a session signature. A session signature is the encrypted (using device private key) hash of all previous hand shake messages which includes session specific random numbers exchanged between server and client. This signature enables a server to validate that the device is in possession of a private key corresponding to the device certificate and thereby authenticate it. Additionally server checks the certificate validity (whether it was issued to a qualified device) and its usage (whether it is authorized to download images) and should entitle the connection to proceed with firmware access.   For this reason, devices should have dedicated certificates with securely stored private keys that would never leave device’s TPM area. They are generally generated and stored in the device as part of manufacturing process along with trusted CA chains.   Device specific encryption   Data stored in the image store of a device should be encrypted using a device specific key. This is to deter storage of master keys into devices.   As part of mutual authentication, the device’s public key is sent to the server. This can be used to encrypt images that will be sent to the device for storage into an external device like an SD card or flash.  Signature (refer section below) can be a part of the encrypted chunk.   In case of a resource constrained device (read: lower RAM) , firmware will have to be broken into smaller chunks and individually signed and encrypted and then packed into a larger structure that can link the encrypted chunks together.   Host software and web-upload considerations   Web upload should not be encouraged since the uploaded image should be in clear. This is because it is not practical to store copies of images signed for all possible devices out there and we do not want to use master keys.  However, it can be used in cases where only a signature validation is required for the update process (based on business logic)   Host (e.g. a PC) software should mimic server behavior by performing certificate mutual authentication and image encryption using device specific keys as it is sent to the device.   Images stored within the host software should be encrypted using a transport key so that the firmware is not available in clear within the software package. (It will be available in clear within the host memory while transitioning from transport key to device key). However, there is a risk associated with storing private transport keys as part of the host software package.   This can be worked around by using an external crypto box with the PC like host or using a host with application level TPM based security (e.g: an update box that can act as the host)   Signing the firmware and Authentication:  Digital signature refers to a digital equivalent of hand written signature that can verify the authenticity of a document. From a digital signatures perspective, the document by itself will be in clear test and not encrypted.   Digital Signatures revolves around the following cryptographic capabilities:     Ability to create a non-reversible (one-way) and unique message digest (Hash)   Ability to implement public key cryptography that can ensure authenticity of a signing certificate            Signer certificate will contain info/details including signer`s public key and signature algorithm and hash function to be used.       Signer certificate will be signed by a trusted Root CA that authenticates the holder and integrity of the signer certificate           To sign a firmware blob, a hash of the blob is computed and is then encrypted using signer’s signing (private) key. This encrypted hash is called the signature. Signature can be decrypted using the signer’s public key available in the signing certificate to get back the expected hash of the blob.                                               Firmware signature verification         To verify authenticity of firmware, a hash of the received blob is computed and compared to the decrypted signature. They will match only if contents of the firmware blob are intact as signed.   Root keys and signing keys   Since signature verification is a resource intensive process, we could use a weaker but faster signing algorithm as long as we can ensure that algorithm used to sign the signing key (signing certificate) is strong. This is an optimization compromise possibility.   This approach also assists in preventing rollback attacks   Secure source of time   One of the requirements in case of a trusted system design is availability of a trusted source of time (and date) to compare the certificate validity (dates) against.  This can either be RTCs with in build batteries or a secure time server or maybe even GPS based time modules for systems of extreme criticality.   Rollback prevention   To prevent attacks based on rolling back to a compromised firmware version, an image key chain approach can be utilised. In this approach, each update image is signed with a different signer key and the associated signing certificate that can be pulled along with an update image will include an incremental firmware version number or index. If a strong certificate signing algorighm is used, the certificates would be intact.   Once a certificate is recieved, it should be securely commited to the TPM memory (key chain) and any upgrade request would be processed only if certificate validation using the latest signer keys pass.  ","categories": ["Articles","Tutorial"],
        "tags": ["Microcontrollers","Security","Upgrade"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/Secure-Firmware-upgrade-for-embedded-systems/",
        "teaser": null
      },{
        "title": "Setting up my blogging environment",
        "excerpt":"  I use jekyll with minimal mistakes for publishing the blog. Here are the steps to setup a local server to write new pages in the existing framework.   In this case I am starting off with a fresh install of Ubuntu17.04 in a virtual machine. It came with ruby installed so, gem command was already available, but ruby-dev had to be installed along with couple of other tools.   sudo apt-get install build-essential patch ruby-dev zlib1g-dev liblzma-dev jekyll bundler    Once the tools are installed, got to the cloned blog folder and run the following command to install missing gems and dependencies.   bundle config build.nokogiri --use-system-libraries     --with-xml2-lib=/usr/lib     --with-xml2-include=/usr/include/libxml2     --with-xslt-lib=/usr/lib     --with-xslt-include=/usr/include bundle install    nokogiri setup is required for latest version to compile with system versions of xml libs.   Once all installations are successful, run the following command to serve the blog in localhost   bundle exec jekyll serve   Use the --incremental flag when writing pages since the process will be faster   ","categories": ["Tutorials"],
        "tags": ["Blogging"],
        "url": "https://www.embeddedinn.xyz/tutorials/Setting-up-my-blogging-environment/",
        "teaser": null
      },{
        "title": "Hijacking openssl renegotiated keys for server wiretaps",
        "excerpt":"  Introduction and problem statement   I was recently working on setting up an Azure IoT hub  based system for MQTT messaging and had to explore some low level stuff while debugging communication issues. It was then that I came across the fact that Azure does TLS re-negotiation while authenticating clients that use X.509 certificates.   To re-create the scenario, let us use openssl’s s_client to post a message to the event hub using HTTP post. The command to use would be:   openssl s_client -host vysakh-testHub1.azure-devices.net -port 443 -cert selfsigned.crt -key selfsigned.key   This command will connect to Azure IoT hub over a secure channel, do a full ssl handshake and derive a session specific Master secret. This key will be printed in the console and it can be used to decrypt application logs using wireshark. At this point, device certificate is not requested.   When the actual request is posted, if authentication mode of the requesting device is X.509, Azure IoT hub will perform a TLS re-negotiation by sending hello request TLS handshake. This triggers a full handshake between the device and server but this time, it would include a certificate request handshake. At the end of this re-negotiation, a new Master secret is derived and it would be used for further application data transfer. However, s_client does not print this re-negotiated key and using default setups, it would be impossible to decrypt further application data exchange between the device and client.                                               default s_client Master Secret                                                     `Hello Request` followed by encrypted data in case of re-negotiation         Another issue is that while using a standard MQTT client like mosquitto_pub over secure channels there is no way to observe application level data once secure channel is established.   I will be describing how to decrypt TLS traffic using wireshark in upcoming sections   overview of the hack   Even without detailed understanding of openssl, it is easy to understand that the sure-shot way to hijack/print keys from the stack is to modify SSL_read() or SSL_write(). These functions will definitely be called by all code written on top of openssl when it has to send or receive data over a TLS socket.   These API takes an argument (s) of type struct ssl_st. On digging a bit through the code, it was clear that this can be used to print the master key using the member: s-&gt;session-&gt;master_key   modifying and compiling libssl   Latest release version of openssl can be downloaded from its website. To compile, use following commands after unpacking the archives. At the time of writing, latest release version was openssl-1.1.0e   ./config make   Before compiling, we will modify the read function to print master_key of the calling session with the following patch in ssl/ssl_lib.c                                               SSL_read() patch to print key         We can now run the freshly compiled openssl app using the modified library by running the following command:   LD_LIBRARY_PATH=. apps/openssl s_client -connect google.com:443   Once the session is established, send a dummy GET / request to see the patch in action                                               SSL_read() patch printing master_key         using the hack with re-negotiating servers   Now that we have the capability to print out session master_key using our modified version of openssl, we can use it with Azure to see the re-negotiated key.   I have already created a device that uses X.509 certificates for authentication. Issue the following command to connect to Azure IoT hub and POST  an event request as this device:   LD_LIBRARY_PATH=. apps/openssl s_client -connect vysakh-TestHub1.azure-devices.net:443 -cert ../selfsigned.crt -key ../selfsigned.key   Once a connection is established, post an event using the below HTTP request:   POST /devices/testDevice2/messages/events?api-version=2016-02-03 HTTP/1.1 Host: vysakh-TestHub1.azure-devices.net User-Agent: testDevice Content-Length: 8 Accept: application/json  testData   From the console output of our patch, you can see that a new key has been negotiated                                               SSL_read() patch printing re-negotiated master_key         decrypting re-negotiated session using wireshark   Wireshark comes with an inbuilt capability to decrypt TLS traffic if master_key can be provided. Master key is matched to the sessions random send as part of Client hello using a “master-secret log file”   You can tell Wireshark where to find the key file via Edit→Preferences→Protocols→SSL→(Pre)-Master-Secret log filename. Key log file format is :   CLIENT_RANDOM &lt;space&gt; &lt;64 bytes of hex encoded client_random&gt; &lt;space&gt; &lt;96 bytes of hex encoded master secret&gt;   when there are more than one CLIENT_RANDOMs involved, add more similar lines to the file and Wireshark will select the appropriate master key for decryption.   Initial client_random can be obtained form the logs itself. In this case, we will first log the entire transaction using wireshark, fetch the initial client_random from the logs and then match them to the master_keys printed by our patch.                                               copying first client ramdom         In the encrypted logs we will be able to see only the first Client Hello since the re-negotiated hello will be under the encryption of the first session. Copy the random as a “hex stream” by right clicking random as shown in the image. Paste it along with the first master secret printed by our patch into a file in the format described above.   After saving the file, point wireshark to it and reload the file by clicking    Upon reloading, the re-negotiation client hello will be available in clear.                                               copying first client ramdom         Now, copy the new random, append it to the key log file along with the re-negotiated master key from our patch and re-load the file to see the entire log in clear.   Decryption of re-negotiated traffic was recently fixed in wireshark. I was not able to see it in action in release Version 2.2.6 (v2.2.6-0-g32dac6a). Instead, I used automated build version Wireshark-win64-2.3.0-3548-gc30bb2c                                               decrypted re-negotiated traffic         Wifi Decryption   In case you are sniffing WiFi traffic from a secure AP, the packets in air themselves will be encrypted.   In case of WPA authenticated network, first generate the PSK by entering SSID and passphrase in the online Wireshark Raw key geenrator tool here   Next, open wireshark and go to Edit&gt;Preferences&gt;IEEE 802.11 and click “Edit” next to Decryption keys”. Add a new key under wpa-psk and enter the generated PSK there.   decrypting secure MQTT traffic   Now that we have a modified TLS library that can provide us required information, we can use it to have more fun. For instance, I used it to decrypt MQTT traffic that is going through a secure channel. An alternate option would have been to modify the MQTT client source to print the session keys. But where is the fun in that??   However, I faced some issues in doing this using the library that we just compiled. At first, ldd command showed that default mosquitto_pub in my system was linked against v1.0.0 of libssl. So, I downloaded sources of openssl 1.0.0k adn compiled it with the patch. However, upon running the tool against the newly compiled library, I faced another issue where it complaints about missing version information   mosquitto_pub: ./libcrypto.so.1.0.0: no version information available (required by /usr/local/lib/libmosquitto.so.1) mosquitto_pub: ./libssl.so.1.0.0: no version information available (required by /usr/local/lib/libmosquitto.so.1) mosquitto_pub: ./libssl.so.1.0.0: no version information available (required by /usr/local/lib/libmosquitto.so.1)   I figured out that this is because debian patches for openssl that is expected by my mosquitto_pub client is missing in the official sources. So, I downloaded the official debian version sources and patches and compiled it.   applying debian patches and compiling sources   debian use quilt for applying patches to official sources. For applying these patches and compiling, download the following files from ubuntu repository:      original sources: openssl_1.0.2g.orig.tar.gz   patches and rules: openssl_1.0.2g-1ubuntu11.debian.tar.xz   unpackage both the archives and copy debian folder from the patch package into original souces folder. Now, to apply the patch, issue the following commands:   export QUILT_PATCHES=debian/patches quilt push -a   instlall debhelper package using sudo apt install debhelper before building.   Now, compile the package using the following command.   debian/rules   Once compiled, make the same changes to SSL_read() as we did before and re-compile the library by issuing make command.   Compiling using debian\\rules will cause tests that are run as part of the script to fail since we are adding additional prints. This way, we just re-compile the files effected by ssl_lib.c changes and re-package the lobrary.   Once compilation is through, run mosquitto_pub using the following command to see the patch in action. Wireshark based decryption uses the same method above   mosquitto_pub -h iot.eclipse.org -p 8883 -t \"testTopic\" -m \"testMessage\" --capath /etc/ssl/certs/ -d                                               mosquitto_pub over secure channel                                                     decrypted MQTT traffic         In case MQTT server uses a non-standard port (e.g. 8884 in case of test.mosquitto.org for mutual authentication), the captured logs will not be identified as TLS. To get this detected as TLS, add the port into a comma separated list in  Edit→Preferences→Protocols→HTTP→SSL/TLS Ports   The above step would enable us to get CLIENT_RANDOM out of the dissected handshake. However, application data after TLS negotiation is not detected as MQTT packets. For this, right click on the application data packet and click Decode As... and enter MQTT in the Current field.                                               decode packet As MQTT         Note added on 10-Mar-2020   In case you want to modify mosquitto-client source to print the complete SSLKEYLOGFILE contents and you are using a newer version of openssl, follow the steps below:      Newer versions of openssl (1.1.1 and bove) have exposed a SSL_CTX_set_keylog_callback() function for this purpose.   Download mosquitto source.   In net_mosq.c, under net__init_ssl_ctx() add the following code after as an else condition to the check if(!mosq-&gt;ssl_ctx)   SSL_CTX_set_keylog_callback(mosq-&gt;ssl_ctx, SSL_CTX_keylog_cb_func_cb);      Define the callback function as below:   void SSL_CTX_keylog_cb_func_cb(const SSL *ssl, const char *line){ \tprintf(\"%s\\r\\n\",line); }   The complete SSLKEYLOGFILE contents will be printed out whenever TLS key material is generated or received.  ","categories": ["Articles","Tutorial"],
        "tags": ["openssl","Security","Azure","MQTT"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/hijacking-openssl-renegotiated-keys-for-server-wiretaps/",
        "teaser": null
      },{
        "title": "letsencrypt+namecheap+Apache in DigitalOcean",
        "excerpt":"  Introduction   The little green icon  that you see in your browser that tells the world that your webserver is backed by a chain of trust and a that you can connect to it using cryptographically backed security always gives me a warm feeling. So, I wanted to try out and setup such a server with the lowest $$ I can spend.   The basic stuff I need for this are      A public IP.   A server to host a service using that public IP.   A (sub) domain name.   A trusted CA issued certificate.   Digitalocean solves the problem of a cheap server with a public IP address with their $5/month Linux SSD virtual machines. If you are new to the service, use this link to sign-up  and get $10 credit to start off.   Domain setup   I bought an ultra cheap .pro domain name from namecheap.com for $0.88 + $0.11 ICANN fees. This namem will be valid for one year and comes with free whois protection and basic DNS support. To make the deal even more better, I was charged in INR and I did not have to pay a foreign currency markup fee on my credit card. I ended up paying just INR 69 for a .pro domain valid for a year.   Once the domain was purchased, log in to namecheap dashboard and click the  button. In the  tab, go to name server and select “Custom DNS” and enter the following three nameservers and apply the changes. This will allow us to manage domain name mappings directly from digital ocean portal.      ns1.digitalocean.com   ns2.digitalocean.com   ns3.digitalocean.com                                               enter custom nameservers in namecheap portal         Once nameservers are setup, login to  Digitalocean portal and create a new droplet. I choose the $5 ubuntu server hosted in bangalore for all my experiments.   In the portal, go to Networking &gt; Domains and enter the domain name you just purchased. If the domain name is already taken, DO will not allow you to register it again. I assume, they check they local cache for this since the check is quiet fast and I was able to do a cross registration while I was still new and with namecheap basicDNS.   Enter the domain name and click . Once the domain is added, enter the following two minimum records            Optionally, you can host your own mailinator domain by adding the below MX record:         This will result in 1 A, 1 CNAME and 3 NS records for this domain. Once the DNS controllers sync up, the domain will be accessible for ping.   Now that we have setup a domain name and mapped it to our droplet, we can go ahead and setup a web server in the instance.   Initial server setup   To start with, we need to create a non-root user and install Apache server. Run the following commands for this   apt-get update apt-get install apache2 adduser webadmin gpasswd -a webadmin sudo   For now, we are not going to setup key based entry . However, this is preferred.   Once these steps are done, the domain we setup in the previous step can be used to see the ubuntu Apache test page.   Apache virtual host setup   For these steps , we need to log in as the non-root user we created. Use su webadmin for this. We will setup our web server to have multiple virtual hosts. For this, follow these steps:      Create the document root folder folder with sudo mkdir -p /var/www/ test.com/public_html   Grant permissions with sudo chown -R $USER:$USER /var/www/test.com/public_html   Ensure read access with sudo chmod -R 755 /var/www   Create a test page with vi /var/www/test.com/public_html/index.html and enter the following contents   &lt;html&gt;   &lt;head&gt;       &lt;title&gt;Welcome to Test.com!&lt;/title&gt;   &lt;/head&gt;   &lt;body&gt;       &lt;h1&gt;Success!  The test.com virtual host is working!&lt;/h1&gt;   &lt;/body&gt; &lt;/html&gt;      Create a Virtualhost configuration template with sudo cp /etc/apache2/sites-available/000-default.conf /etc/apache2/sites-available/example.com.conf   Enter below contents into the config file:   &lt;VirtualHost *:80&gt;     ServerAdmin admin@test.com     ServerName test.com     ServerAlias www.test.com     DocumentRoot /var/www/test.com/public_html     ErrorLog ${APACHE_LOG_DIR}/error.log     CustomLog ${APACHE_LOG_DIR}/access.log combined &lt;/VirtualHost&gt;      save the config and enable the virtual host with sudo a2ensite test.com.conf followed by sudo service apache2 restart   Now, try navigating to the domain from a browser and the ubuntu test page will be replaced with our new page   setting up a certificate with Letsencrypt   [Letsencrypt]{https://letsencrypt.org/}{:target=”_blank”} provides an automated way to fetch free and valid 2048 bit RSA domain certificates.   The automation tool used in this process is certbot. To install certbot, follow these steps:      Add the repo with sudo add-apt-repository ppa:certbot/certbot   Update with sudo apt-get update   Install certboot with sudo apt-get install python-certbot-apache   Run the following command to fetch a certificate for the domain we setup with sudo certbot --apache -d example.com -d www.example.com   At a stage you will be asked to choose between redirecting all traffic to HTTPS and allowing HTTP as well as HTTPS . It is preferred to redirect to HTTPS.   Let’s Encrypt’s certificates are only valid for ninety days. To setup auto renewals, execute: sudo crontab -e. You will be asked to select an editor and once the files open, enter the following line to the end of the file.   15 3 * * * /usr/bin/certbot renew --quiet   This will automatically run the renewal step daily and renew the certificate when it is due. The --apache command we used during certificate setup will ensure that apache will reload when the certificate is renewed.   At the end of this process, we get the glorious    Eleptic curve certificates   Letsencrypt signs certificates using RSA. However, it allows the client certificates to include public keys for ECC. Since certbot tool does not provide a mechanism for automatically fetching ECC certificates, we will have to manually generate a CSR and get it signed by letsencrypt.   letsencrypt uses alternate subject names to issue the same certificates for multiple (alternate) domains. However, default openssl configuration does not ask for adding alternate subject names while generating signing requests using openssl req. So, as the first step, we need to set up a temporary configuration to do this. This will allow us to use the same certificate for test.com as well as www.test.com      copy the default openssl configuration file into the current working directory cp /etc/ssl/openssl.cnf .   uncomment req_extensions=v3_req line in the config file. This will let openssl read the v3_req section while creating requests.   go to [v3_req] section in to config file and add subjectAltName = @alt_names to the existing lines.   next add a new section [alt_names] with the following contents    [ alt_names ]  DNS.1 = test.com  DNS.2 = www.test.com   Now that the setup is ready to create a CSR for EC* certificates, we need to first create a key pair to place the public key in our CSR. Issue the following command to create a key pair.   openssl ecparam -genkey -name secp384r1 | openssl ec -out ec.key   Now, issue the CSR request command as follows to generate the CSR including the public key we just generated.   openssl req -new -sha256 -key ec.key -nodes -out ec.csr -outform pem -config openssl.cnf   To use this CSR and generate a certificate, issue the following certbot command.   certbot certonly -w /var/www/test.com/ -d test.com -d www.test.com --email \"admin@test.com\" --csr ./ec.csr --agree-tos   This will create files (0000_cert.pem, 0000_chain.pem, 0001_chain.pem) corresponding to the signed certificate, chain and full chain that certbot generates.   To make Apache use these manually generated certificates,      identify the SSLCertificateFile and SSLCertificateKeyFile configured in /etc/apache2/sites-available/test.com-le-ssl.conf.   Mostly, this will be a softlink to some files in /etc/letsencrypt/archive/test.com/fullchain.pem   replace the corresponding files with the newly generated certificates.   To enable strong server side TLS, we follow the recommendations from mozilla and edit the  configuration in /etc/letsencrypt/options-ssl-apache.conf to:    SSLProtocol             all -SSLv2 -SSLv3  SSLCipherSuite          ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256  SSLHonorCipherOrder     on  SSLCompression          off   Restart apache with sudo service apache2 restart   This configuration can be checked by running an ssltest on the domain using the online service provided by                                                AES256 negotiated with firefox (shown using cipherFox plugin on firefox)         The renewal crontab we setup will renew the certificate to use a default (RSA) certificate which will break the ECC certificate we setup and fallback to the default RSA certificate.                                               ssllab analysis before renewal                                                     ssllab analysis after renewal         Multiple certificates in Apache   Now that we have a RSA certificate generated by certbot and an ECC certificate generated with our own CSR, we can setup apache to use both in parallel and serve the corresponding certificate to the client based on suites presented in client hello.   For this, add the following 2 extra pointers to the new (ECC) files along with the default certbot configuration in /etc/apache2/sites-available/test.com-le-ssl.conf   SSLCertificateFile /etc/letsencrypt/live/test.com/fullchain_rsa.pem SSLCertificateKeyFile /etc/letsencrypt/live/test.com/privkey_rsa.pem   After apache restart, an analysis with the openssl tool shows that both ECDSA and RSA based ciphers are supported by the  server now in the order we configured in the server.                                               ssllab analysis showing ECC and RSA support         A firefox plugin names toggle-cipher-suites allows us to disable certain ciphers and see corresponding cipher selection in the server we just configured.                                               toggle-cipher-suites firefox plugin lets us try the 2 certificate configuration         SNI considerations for multiple named hosts   By default the initial SSL connection handshake (client hello) will not indicate the host name to which the request is being made. This is a problem when we have different certificates and SSL settings for different virtual hosts behind the same IP address. TLS has an extension named Server Name Indication that allows client hello to include the host name to which the connection request is being made. Apache uses this to select the corresponding server certificate to use in setting up the session.   Most modern browsers supports SNI by default. When using clients like openssl s_client make sure to use the -servername argument to connect to the appropriate virtual host. If this is not provided, connection will be made to the default virtual host.   Client authentication using certificates   If you need to enable client authentication using certificates add the following configuration into the SSL configuration of the corresponding virtual host   #Enable client authentication SSLVerifyClient require SSLVerifyDepth 1 SSLCACertificateFile \"/etc/letsencrypt/archive/&lt;host&gt;/&lt;expected Root CA&gt;\" SSLOptions +StdEnvVars   Optionally, SSLCACertificatePath can be used to support multiple CAs. StdEnvVars helps with getting the connecting client’s certificate information.  ","categories": ["Articles","Tutorial"],
        "tags": ["Server Setup","Security","letsencrypt","Digital Ocean"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/letsencrypt+namecheap+Apache_in_DigitalOcean/",
        "teaser": null
      },{
        "title": "Developing an Alexa Smart Home Skill",
        "excerpt":"  Introduction   Though it might sound complex, developing a smart home skill for Alexa is in fact very straightforward. You just need to get a grasp of some underlying principles of developing systems on AWS platform.   While there are many in-depth guides available out there, this write-up is meant to be a quick start guide to experience the complete development cycle.   To start with, you need an AWS account as well as an Amazon developer account.   I operate in AWS console as an IAM user with following permissions enabled:      AWSLambdaFullAccess   AWSIoTDataAccess   Creating an Alexa smart home skill   Go to Amazon developer site, login and click on  button under  tab. Select Smart Home Skill API radio button, English (U.S) as language and select an appropriate skill name.   After saving initial entries, there is nothing to be done for interaction model since Smart Home Skills rely on pre-build interaction models that are build into the ecosystem.   Now, we need to create a back-end Lambda function that will process smart home requests coming from our skill. Make a note of skill ID from the top bar of Alexa developer console before logging in to AWS console as a user with sufficient privileges to create a Lambda function.   Creating a Lambda function   After logging into console, select Ireland region and go to Lambda service. Click  and select alexa-smart-home-skill-adapter blueprint for nodeJs. This will select Alexa Smart Home as the Lambda trigger. Now enter skill ID that we copied from developer console into Application Id and select Enable trigger before clicking  .   Lambda function code for a dummy household would have been already populated. We will look into the relevant parts later on. For now, just provide a function name and select a role. If you do not have an existing role that you have created, create a custom service-role/execute_my_lambda role. Make sure that this role has the follwoing permissions enabled in IaM console.      AWSLambdaFullAccess   AWSIoTDataAccess   Review settings in next screen and create the function. You can test it by configuring a test event of type Alexa Smart Home - Discovery from . Once Discovery test is successful, copy the Lambda ARN and go back to developer console.   Configuring Account Linking   In Configuration page of the skill we where working on, select Europe as the lambda region and enter the ARN of Lambda function we just created in AWS console.   Alexa Smart Home Skills uses account linking feature of Alexa ecosytem to let our backend lambda function interact securely with a variety of cloud or standalone solutions handling actual device control functions. Only prerequisite is that the service should support OAuth based authentication.   When an end-user of our skill install it using Alexa mobile/web app, they will be asked to log into the device-cloud vendor’s web site by re-directing to a login page in their system. Once logged in, Alexa will get the Authentication_token that was derived as part of OAuth transactions. This will in turn be passed on to our Lambda function each time it is invoked. The Lambda function can then use this token to authenticate with device-cloud vendor’s service and call relevant device control APIs based on the Alexa intent that is being handled in that session.   In this case, to understand the account linking process, we will link our skill using Amazon credentials itself. So, when we enable the skill in our Alexa app, we will be presented an Amazon login page into which we can login using regular Amazon credentials. Since we will be setting up only Profile access as part of account linking, the Lambda will not be able to access anything other than user profile from the end users Amazon account.   In developer console go to  and select Login with Amazon. Select  and enter details in the next page. I selected Amazons privacy policy page http link since this is a dummy example.   Once a profile has been created, click on Show Client ID and Client Secret and make a note of Client ID and Client Secret and go back to the Alexa skill that we where developing.   Under Account Linking in Configuration, enter the following details:      Authorization URL : https://www.amazon.com/ap/oa   ` Client ID`        : Client ID from security profile   ` Access Token URI : https://api.amazon.com/auth/o2/token`   ` Client Secret`    : Client Secret from security profile.   Scope             : profile   You can enter the same privacy policy URL as we did while creating security profile here as well.   Now we need to provide the unique redirect URLs for this skill in the security profile. Click on  corresponding to your security profiles, go to web settings and add the two redirect URLs from Alexa Skill into Allowed Return URLs.   Now that all the required details and setup has been done, we will see a  in the Test section of this skill. this means that our skill is ready to be tested in a real Alexa capable device. I use an Amazon Echo dot. In case you do not have a device, you can use  echosim.io. Make sure that the device language is set to English (U.S) since we developed our skill for only that language.   Testing the Skill   Navigate to alexa.amazon.com and go to your skills under skills. Here the new skill will be visible with an explicit banner statting that account linking is required. . Click on it and enable it.   When enable is clicked, you will be taken to the amazon login page for this skill. You should login using your regular Amazon user ID and password here.                                               Amazon login page for account linking         Upon successful login, you will be shown a prompt to discover all devices in your household that can be handled by this skill.                                               device discovery prompt         Following this, the dummy devices from our lambda blueprint will be visible under                                                discovered Smart devices         Now, you can issue voice commands like :   Alexa, discover devices  Alexa, switch on smart light  Alexa, set dimmable light to fifty percent    Code flow   When a discover device is invoked either through the app or by asking Alexa to do so, the lambda handlers of all smart home skills will be called with DiscoverAppliancesRequest . The payload of this request json will contain the access token negotiated with actual device-cloud using OAuth.   Now that the lambda function has an authentication token, it can request the end-service (Server/device cloud) for devices that are available for this user. This data is passed back to the skill and it is populated under the smart devices tab.   A lot of device specific details are passed back to Alexa Smart Home as part of device discovery. This will include unique device and manufacturer identifier namespaces, device capabilities etc.   In the case of our blueprint code, two devices are reported back without contacting any service providers. The samrt light device just has just turnOn and turnOff capabilities while  Dimmable light has turnOn, turnOff, setPercentage, incrementPercentage and decrementPercentage capabilities.   When an actual action on one of the devices is requested by the user, the lambda handler will be called with the corresponding request (e.g. TurnOnRequest) with enough details to uniquely identify the device for which the action is intented.   One way to take this dummy implementation to the next level is to attach devices to AWSIoT and have the lambda function modify device shadows based on intent calls from the trigger point.   ","categories": ["Articles","Tutorial"],
        "tags": ["AWS","Alexa","IoT","Smart Home"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/Developing-an-Alexa-Smart-Home-Skill/",
        "teaser": null
      },{
        "title": "Developing a smart home application for Google Assistant",
        "excerpt":"  Introduction   Last time, I wrote about developing a smart home skill for Alexa. (It is available here in case you missed it). Now let us look at how Google smart home controls can be integrated into this ecosystem.   Google Assistant is the Alexa equivalent from Google. While Alexa is available through a variety of Echo devices and AVS apps, Google Assistant is available in almost all new Android devices as well as through Google Home - a voice activated speaker from Google. In this tutorial, we will look at how to build a back end to control smart home devices using Google Assistant. Since I have already build up a “Device cloud” using AWS IoT as the back end for my previous Alexa experiments, I will e trying to integrate Google Assistant control into that as an additional control plane instead of building a totally new device back end.   At this stage the code written for this is a proof of concept and many of the parameters are hardcoded. However, there will be incremental improvements made to this code.   Current state of this integration can be seen in the demo video below:     Terminology   First, let us get used to some of the standard terminology that will be used in this article. These are terms that you will be seeing frequently in official Google documentation as well.      Actions are Google equivalents of Alexa skills. Actions lets developers build additional capabilities that add on to existing capabilities of Google Assistant. Currently there are two ways to build actions - api.ai and Actions SDK. Currently, smart home development iis not supported in api.ai   api.ai is a web based application that can be used to develop basic actions without writing code. Using UI based development flow, an action can be built, linked to a Google account and activated in all Google Assistant enabled devices linked to that account.   An Action Package is a JSON file that describes the characteristics, capabilities and fulfilment mechanisms of an action. In case of api.ai based flow, a developer need not generate this file whereas it needs to be hand coded for SDK based flow. Though this flow is a bit more complex, it gives more flexibility to the developer and is the only flow currently supported for smart home actions development.   A Fulfillment is an HTTP based back end that will be invoked by Google Assistant when a request is placed to an action. Request details are passed on in JSON format as part of a POST request to the HTTP endpoint (webhook) and response is expected in JSON format.   There are some more minor terms and they will be covered as part of the rest of this article.   Google Assistant smart home flow   Unlike general actions that can be invoked from Assistant without any setup stage, smart home actions requires the user to perform account linking to specific device vendor actions so that the user specific devices can be reported to Google Assistant by the corresponding actions. For this, the user needs to select the device provider’s action from a list of published smart home actions and login to it and it is very much similar to Alexa account linking.   Once account linked, devices from all vendors will be visible in Google Home app under the smart home tab and each device can be assigned to rooms in the house. This is because once devices are reported by different actions, google stores the information in an internal “Home Graph” and is frequently refreshed. Devices will be presented using a real name and nickname that can be used to “talk” to the device via assistant.   When a device is to be controlled talk to Google Assistant asking for the action to be performed and the device to perform it on. E.g. OK Google, switch on the kitchen-light   Smart home actions development   We will first create a webhook API to handle our smart home requests and then use it in an action package that will be uploaded into actions console. I decided to use a combination of AWS Lambda and AWS API to build this since this would integrate well with my existing AWS IoT based device back end.   Building a device control API   First step in this process is to build an AWS Lambda function. I chose to build a lambda in eu-west-1 region as a user with the following permissions:      AWSLambdaFullAccess   IAMFullAccess   AmazonAPIGatewayAdministrator   AWSIoTFullAccess   My lambda has a role with the following permissions:      CloudWatchLogsFullAccess   AWSIoTFullAccess   The code for this function can be accessed here. However, it has many parameters hard coded and is currently just a reference proof of concept. As with the Alexa skill, I have two of the below environment variables set to communicate with AwsIoT and discover my devices.      thingTypeName : HouseholdDevices   region : eu-west-1   While creating the lambda, I did not setup any triggers since we will be doing this from the API console later.   To make sure that my lambda is working as expected, I use the following test trigger. (However, I did not test this when there are no devices populated in AWS IoT)   Execution Trigger:   {   \"inputs\": [{     \"intent\": \"action.devices.EXECUTE\",       \"payload\": {         \"commands\": [{           \"devices\": [{             \"id\": \"KitchenLight1\"           },           {             \"id\": \"PorchLight1\"           }           ],           \"execution\": [{             \"command\": \"action.devices.commands.OnOff\",             \"params\": {               \"on\": true             }           }]         }]       }   }],     \"requestId\": \"13973399895754995660\" }   Now that this lambda has been built and tested, we can build an API around it. For this, head to the Amazon API console and click create API. Give an API name and select “New API” before clicking “Create API”.   Under Actions, click Create resource and name it DeviceControl and click create resource. Now click create method under actions and select post from the drop down menu. Once you click the tick mark next to the selected method, we can now choose the lambda to be linked to this resource method. Select eu-west-1 region and select the lambda function that we just created. (You will be presented a list once you start typing the lambda name). Click save and accept the permissions prompt.   Now to make this API useable, we need to deploy it under a stage. To do this, click Actions and select Deploy API. Enter a deployment stage name (e.g. beta) and click Deploy. Now, Navigate to the POST method in this deployed API and copy the Invoke URL. This is what we will be entering into our action package in the next stage.      Some header mappings needs to be performed to pass on information like authorization token to handle user identification. However, we are skipping it here since we are implementing a test app that will be linked to only one account.    Understanding the lambda code   Google actions for smart home has just three intents that needs to be handled by the webhook. These are:      action.devices.SYNC   action.devices.EXECUTE   action.devices.QUERY   SYNC is used to discover devices, EXECUTE for controlling device states and QUERY for fetching device states.   When a SYNC is received, the lambda function queries for all devices of type householdDevices to AWSIoT back end and populates the returned information into the JSON response. This will be stored in google home graph by actions.   The EXECUTE intent can be quiet elaborate with a set of packed instructions for a combination of devices. This is to support controlling multiple devices with a single command like “OK google, turn off all lights”. This is the sample used in our lambda test trigger.   Though QUERY is mandatory, the current implementation above does not implement it yet.   Linking the back-end to Actions   Now that we have a back-end API to control our smart home devices using JSON based POSTs in formats prescribed bu Google Actions, we can go ahead with writing an actions package. Unlike generic actions, smart home action packages are rather simple and is indented for two main purposes      To indicate to the actions project that this is a smart home action   Provide the fulfillment webhook endpoint address   An action package for this purpose would look lihe :    {   \"actions\": [{     \"name\": \"actions.devices\",       \"deviceControl\": {       },       \"fulfillment\": {         \"conversationName\": \"automation\"       }   }],     \"conversations\": {       \"automation\" :       {         \"name\": \"automation\",         \"url\": \"&lt;webhookEndpoint&gt;\"       }     } }    actions.devices indicates that this is a smart home skill and WebhookEndpoint is the fulfillment endpoint that needs to be replaced with the API Invocation URL that we just created.   To get this into actions console, first navigate to actions console and click Add/Import Project. After giving a project name and selecting the country a blank project can be created. This is a common starting point for all actions project and here select actions SDK from the first section. The update app section in the pop-up will give the required command to upload our action package into actions console.   Before this can be done, you need to download gactions CLI for your platform of choice. This is a single file that does not need any installation and can be used form command line.   Store the above JSON in a file (e.g. action.json) in the same folder as that of the downloaded CLI and run the command that was copied from actions console pop-up. In my case it was   ./gactions update --action_package action.json --project blogtestproject-e815f   During this step, you will be prompted to visit an oauth2 page and present the authorization code after providing required permissions. You should use the same account to which your Assistant enabled devices are linked to. Once this is done, the action package will uploaded to your project and we can continue with a test deployment.   Go back to actions console web page and add some of the app informations like name and pronunciation, description etc. these are not too crucial at this stage since this app is going to be for personal test use. However, they need to comply with google standards if it is to be published.   Account Linking   Account linking is mandatory for home control apps since it is the only way for the app backend to identify a user trying to access and control devices. For this, we will be using login with amazon as we did for the Alexa smart home skill.   Log on to Amazon developer console and click on Login with Amazon under Apps &amp; services and create a new security profile. Make a note of the client ID and secret. Now, go to the web settings of this profile and add the following URL under Allowed Return URLs. (replace with your project name from actions console.)   https://oauth-redirect.googleusercontent.com/r/&lt;ActionsProjectName&gt;   Go back to Actions Console and click Add under account linking. Select Implicit as grand type and click Next. Enter ClientId from the security profile in previous step (from Amazon) and enter the Authorization URL as https://www.amazon.com/ap/oa. Add profile as scope and complete this step. Since this app is not going to be published, enter some dummy test instructions.   Now the actions app is ready for testing. So, click Test Draft   Testing the smart home app   Once the actions draft has been deployed for testing, go to the Google Home app in your smart phone and navigate to Home Control. Click the + sign and click on the app that we just deployed for testing. (It will have a [test] prefix to its name). Once clicked, you will be re-directed to the Amazon login page and once account linking is complete, the smart home devices in our AWSIoT console with type ‘householdDevices’ will be listed in the app. Now, we are ready for testing as seen in the introductory video.  ","categories": ["Articles","Tutorial"],
        "tags": ["AWS","Google Home","IoT","Smart Home","MQTT"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/Developing-a-smart-home-application-for-Google-Assistant/",
        "teaser": null
      },{
        "title": "A refresher of new C coding features for embedded system developers",
        "excerpt":"  Introduction   As embedded system developers, we are often baned to work with archaic vendor compilers and more often than not, to base our work on historic code bases. One of the major flip sides of this is that we do not get to use and slowly lose touch with some of the relatively new and exciting C standard and compiler features. We will look at some of them in this article.   Note you might have to use the --std=c11 flag with your compiler to try out some of these features. I will be using clang throughout this article. Since clang uses an extended version of C11, the explicit flag is not required. To keep things simple, I am not enabling any extra compilation flags unless otherwise specified.   Link time optimzsation (LTO)   Traditionally the C compiler compiles each file in to an object file and does optimization within these compilation units. Potential cross file optimizations where not visible to the compiler until recently. For a while, the gcc and Clang compilers have been supporting LTO. With LTO, the compiler compiles each file into a source aware annotated object file that has enough information to perform an additional round of optimization at link time. In gcc use the -flto flag to enable this.   data types   Gone are the days where you write long declarations like unsigned long long int . The best way to do this now is to use the standard definitions like int8_t, uint32_t etc. that is offered by stdint.h. But then, this is not very new to embedded system developers since we already had this convention going for a while.   In addition to the fixed length types, there are fast and least types defined in stdint.h spec. Fast types like int_fast8_t or uint_fast32_t guarantees at least x bits of storage while it might upgrade to a larger type if it has faster performance in the platform.least types like uint_least16_t or int_least64_t provides the most compact number of bits to accommodate the request.   The use of char to indicate 8 bit data is frowned upon. You should instead use uint8_t.   For pointer math, stdint.h provides uintptr_t,intptr_t etc. This is in addition to ptrdiff_t from stddef.h   intmax_t/uintmax_t is the safest cast to hold the largest integer that a given platform can hold.   variable declaration   Since C99, you can declare variables right before they are used and not just at the beginning of a file or function. Loop variables can also be defined within the loops scope like:    for(int i=0;i&lt;10;i++){}    Use of this feature makes the code more readable.   the once pragma   Most new compilers lets you place a #pragma once at the top of your (header) file to make sure that the file is included only once so that you do not have to use the header file name definition guard in your files.   array initialization   Arrays can be initialized to known values without having to do memset by using the following :    uint32_t testVector[32]={0};    The same holds true for structures. However, padding bits might not be initialized.  ","categories": ["Articles","Tutorial"],
        "tags": ["C programming"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/A-refresher-of-new-C-coding-features-for-embedded-system-developers/",
        "teaser": null
      },{
        "title": "Understanding JSON Web Tokens",
        "excerpt":"  Introduction         Table of contents     Introduction   Intro to basic JWT   JSON web Signatures (JWS)   JSON Web Encryption (JWE)   JSON Web Keys (JWK)           Reference                JSON Web Token, or JWT (“jot”) for short, is a standard for safely passing claims securely in memory constrained environment. It is part of the JSON Object Signing and Encryption group (JOSE) set of standards.   Usecase: For instance upon logging into a server initially using a user name and password, the server can provide a JWT to be used for subsequent requests for a defined period of time. The users authorizations and claims can be encoded into the JWT itself. This is especially useful for stateless systems like HTTPS requests. In this case, JWT will use JSON web signature (JWS) and optionally JSON web encryption (JWE) from JOSE along with JWT itself.   Intro to basic JWT   Basic structure of a JWT is HEADER.PAYLOAD.SIG/ENC where:          HEADER and PAYLOAD are the base64url encoded form of the flattened JSON header and payload.       {\"alg\":\"HS256\",\"typ\":\"JWT\"}            Header defines the basic type and algorithm used in the token. Payload is application defined.       {\"sub\":\"1234567890\",\"name\":\"John Doe\",\"admin\":true}            SIG/ENC is the optional signature and/or encryption info. We will investigate this in detail below.       A typical JWT in the encoded and decoded form is given below. In this case, the SIG/ENC part is omitted.                                               basic JWT decoded         The JOSE header in the JWT contains claims about itself. It provides indicators on how rest of the token is encoded and signed. In the case above, the header mentions that the token uses HS256 for signature. Spec provides provisions for a type (typ) and content type (cty) headers. But these are rarely used. An unsecure JWT will look like the one below. Note that the trailing . represents an empty signature part. Though the trailing = is present in the PAYLOAD in this case, it is typically removed.                                               unsecure JWT example         PAYLOAD typically contains user claims. But this is not defined by the spec beyond the fact that it should be valid JSON. However, following items are registered claims as per the spec and they have special meaning      iss : a case sensitive URI or string that uniquely identifies the JWT issuer.   sub : a  case sensitive URI or string that uniquely identifies the party about which the JWT carries info.   aud : an array of intended audience for the JWT   exp: expiration epoch time   nbf: epoch time “not before” which the JWT is valid   iat: “ issued at “ time.   jti: a unique JWT ID   Beyond these, users can use either public or private claims. Public claims are registered with IANA JSON Web Token Claimsregistry to avoid collisions. Private claims are specific to the use-case.   JSON web Signatures (JWS)   A signature is required to cryptographically establish the authenticity of a JWT. Algorithms used for signature are designated identifiers in the JSON Web Algorithms (JWA) spec (RFC 7518). For instance HMAC using SHA-256, called HS256 in the JWA spec.   For beginners HMAC is an algorithm that accepts a hash algorithm, an octet input and a secret and generate an octet output using the HMAC algorithm. Typically HMAC in the JWT context uses a shared secret.   The process of attaching a signature to an unsecure JWT is given below:                                               Signing Process of JWT         JWS would require the HEADER to carry more claims to assist the signature validation. Major ones  include:      jku : URI pointing to a set of JSON encoded public keys that can be used to verify the signature.   jwk : JSON web key used to sign the JWT. More on JWK in the next section   kid: a unique , user defined identifier representing the key   x5u: URL pointing to the X.509 cert chain used to sign the JWT   x5c: JSON array of Base64 encoded DER format cert chain used to sign the JWT   x5t: SHA-1 thumbprint of the X.509 cert used to sign the JWT   x5t#S256: like x5t, but uses SHA256   JWS spec defines an alternate way to represent secure tokens. While the HEADER.PAYLOAD format is called the compact form, the following is a non-compact form called JWS JSON Serialization form.   In JWS JSON Serialization form, a signed JWT is represented in a printable JSON format. This allows multiple signatures to be present for the same payload. Consequently, the structure is defined as:      The top most JSON object will include the following key value pairs            payload: Base64 encoded string of the actual JWT payload object.       signatures: an array of JSON objects carrying the signatures. These objects are defined below.           Signature array contains:            protected: a Base64 encoded string of the JWS header. Claims contained in this header are protected by the signature. This header is required only if there are no unprotected headers. If unprotected headers are present, then this header may or may not be present.       header: a JSON object containing header claims. This header is unprotected by the signature. If no protected header is present, then this element is mandatory. If a protected header is present, then this element is optional.       signature: A Base64 encoded string of the JWS signature           A sample is given below:  {   \"payload\": \"eyJpc3MiOiJqb2UiLA0KICJleHAiOjEzMDA4MTkzODAsDQogImh0dHA6Ly9leGFtcGxlLmNvbS9pc19yb290Ijp0cnVlfQ\",   \"signatures\": [     {       \"protected\": \"eyJhbGciOiJSUzI1NiJ9\",       \"header\": {         \"kid\": \"2010-12-29\"       },       \"signature\": \"cC4hiUPoj9Eetdgtv3hF80EGrhuB__dzERat0XF9g2VtQgr9PJbu3XOiZj5RZmh7AAuHIm4Bh\"     },     {       \"protected\": \"eyJhbGciOiJFUzI1NiJ9\",       \"header\": {         \"kid\": \"e9bc097a-ce51-4036-9562-d2ade882db0d\"       },       \"signature\": \"DtEhU3ljbEg8L38VWAfUAqOyKAM6-Xx-F4GawxaepmXFCgfTjDxw5djxLa8ISlSApmWQxfKTUJqPP3-Kg6NU1Q\"     }   ] }   In a visual representation, this looks like:                                               JWS JSON Serialization form         JWS JSON serialization defines a simplified form for JWTs with only a single signature. This form is known as flattened JWS JSON serialization. Flattened serialization removes the signatures array and puts the elements of a single signature at the same level as the payload element.   JSON Web Encryption (JWE)   While JWS provides only data integrity and authenticity checks, JWE adds confidentiality to the equation. However, unless special algorithms are used, JWE guarantees only confidentiality. Therefore, in an asymmetric scheme, JWE and JWS are complementary to each other.   Encrypted JWE has a different compact representation when compared to a regular (signed) JWT. JWE Compact Serialization has five elements. As in the case of JWS, these base64 encoded elements are separated by dots. These elements are:      protected header: a header analogous to the JWS header. Contains info required to process the JWT.   Encrypted key: Key used to encrypt data in the token . This key is encrypted with a key specified by the user.   Initialization vector: IV sued by the crypto algorithm   Encrypted data: actual data being encrypted   Auth tag: algorithm generated data for validation   In addition to compact serialization, JWE also defines a non-compact and flattened JSON representation.   JSON Web Keys (JWK)   Even though there are existing specifications talking about how keys can be represented, JOSE includes a unified representation for all keys supported in JWA spec.   A sample JWK:   {     \"kty\": \"EC\",     \"crv\": \"P-256\",     \"x\": \"MKBCTNIcKUSDii11ySs3526iDZ8AiTo7Tu6KPAqv7D4\",     \"y\": \"4Etl6SRW2YiLUrN5vfvVHuhp7x8PxltmWWlbbM4IFyM\",     \"d\": \"870MB6gfuTJ4HtUnUvYMyJpr5eUZNP4Bk43bVdj3eAE\",     \"use\": \"enc\",     \"kid\": \"1\" }   Reference      RFC7515 - JSON Web Signature   RFC7516 - JSON Web Encryption   Registered JOSE Headers   jwt.io   The JWT Handbook by Sebastián E. Peyrott, Auth0 Inc.  ","categories": ["Articles","Tutorial"],
        "tags": ["Networking","Security"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/understanding-JSON-web-tokens/",
        "teaser": null
      },{
        "title": "Decrypting embedded TLS communication",
        "excerpt":"  Introduction         Table of contents     Introduction   Modus operandi   Code changes   Decrypting TLS traffic with Wireshark   updated method (24-Mar-20)        A typical cloud connected device communicates with the server over a TLS connection. All data in this communication will be encrypted using a set of session specific keys derived at the time of connection. Even if the network traffic is sniffed, this data cannot be decrypted unless you get access to either the negotiated key or the server’s private key. Getting access to the server’s private key in the case of a cloud service like AWS IoT is impractical. So we resort to the second (and most foolproof) method.   In the case I am describing below, the communication is happening between a PIC32 device running WolfSSL and AWS IoT core. The communication uses MQTT protocol and we need to decrypt the application level exchanges happening over the TLS1.2 transport layer. I will not be going into the details of how the Wi-Fi traffic was sniffed. We start from the point where we have a setup to capture and view the network traffic in Wireshark (v3.0.6-0-g908c8e357d0f) .   Modus operandi   To decrypt TLS traffic we need access to the master secret that is negotiated as part of the TLS connection process. Wireshark can accept a NSS Key Log Formatted file containing the client random and corresponding master secret for the session and use it to decrypt TLS traffic.   The simple file structure that we would be using looks like:    CLIENT_RANDOM &lt;Client Hello Random&gt; &lt;master secret&gt;   More details about the file format can be found here.   Code changes   We get this dumped out of the firmware by adding the following lines of code in WolfSSL’s tls.c file  under the SHOW_SECRETS flag.   printf(\"CLIENT_RANDOM \");  //printf(\"\\r\\nclient Random (tls.c): \"); for (i = 0; i &lt; RAN_LEN; i++)   printf(\"%02x\", ssl-&gt;arrays-&gt;clientRandom[i]); printf(\" \");          //printf(\"\\r\\nmaster secret (tls.c): \"); for (i = 0; i &lt; SECRET_LEN; i++)   printf(\"%02x\", ssl-&gt;arrays-&gt;masterSecret[i]); printf(\"\\r\\n\");         This will generate a print like the one below for each session it establishes.    CLIENT_RANDOM 1d38a12474f12447cff5013d88a85b1afc83b100a7ff92e5a1fc5171f6dfe101  331fd5c8d5e52fce44e4cd13a34beadd14c533b40f3a1839de0feb04d069ea58045cf04ef25f22e71dbcbe00b88ef4e2   Copy these into a text file. If there are multiple sessions , copy each of the prints into a newline within the file.   Decrypting TLS traffic with Wireshark   In your capture , right click on one of the TLS lines and click on the menu item to provide a master secret log file as shown in the menu below. (you can filter the capture with the tls expression to reduce clutter.)                                               Log file context menu                                                     Log file menu and options         Once this file is provided, Wireshark will decrypt all traffic in the stream that has a matching random from the client hello phase of the session.                                               Client Random                                                     Decrypted traffic         updated method (24-Mar-20)  Wolfssl aready has a function to dump SSLKEYLOGFILE by enabling the SHOW_SECRETS flag under MakeTlsMasterSecret of tls.c. However this function tries to write to a file. On an embedded system without a FS, you can modify the function to print pmsBuf to the console.  ","categories": ["Articles","Tutorial"],
        "tags": ["Networking","Security"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/Decrypting-embedded-TLS-communication/",
        "teaser": null
      },{
        "title": "Setting up a Github Actions based CI system for embedded development using PIC32",
        "excerpt":"  Introduction  GitHub Actions is a relatively new workflow automation offering from GitHub. While there are a lot of tutorials out there talking about using GitHub Actions on your web/software project, there are not many on using it for CI/CD workflows for embedded firmware development. In this article, we will go through the steps involved in creating a build test environment for a PIC32 based projects hosted in GitHub. We will create a simple project using MPLABX and Harmony 3 for a PIC32MZ device , commit it to GitHub and create all the framework and backend required to perform a build test on the project each time a change is committed.   While a build test in itself might sound trivial, the steps we follow here will equip you to enhance the setup further to perform advanced tests including Hardware in the loop tests.   Creating your project and repo   This is the relatively trivial part of this project which most of you would already be familiar with. Without going into the details, the outline of steps that needs to be followed are :      Create a GitHub repo to host your project.   Push your PIC32 project to the repo.            Add a .gitignore to omit *.o , *.d, *.tmp etc. I typically omit the dist folder as well.           I have kept the project configuration simple to avoid confusion. It is just a typical Hello World project with default configs.                                               Create a repo to host your project                                                     Device project configuration         Setting up the build runner   GitHub Actions essentially runs a bunch of scripts to clone your repo and execute commands that you provide in a .yaml file every time a git action is performed on the repo. For mainstream software project configurations, GitHub provides free “runners”. Runners are essentially sandboxed environments where the commands execute.   In the case of an embedded project, it will be tricky to find a build environment in the standard offering. (Unless you are using a hobbyist platform like Arduino). Even if the environment is available, for serious projects, we would want to have tight control over the toolchain versions and configurations. So, in this case we will build our own runner and register it with GitHub Actions. If you really want to go with a GitHub remote Runner, you can spin up a docker with the required tools installed.   The docker file for this would look like this:    FROM ubuntu:20.04  ENV XC32VER v2.40 ENV MPLABXVER v5.40  MAINTAINER Vysakh P Pillai   # Install dependencies RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends apt-utils  RUN dpkg --add-architecture i386 \\     &amp;&amp; apt-get update -yq \\     &amp;&amp; apt-get install -yq --no-install-recommends ca-certificates wget unzip libc6:i386 git \\         libx11-6:i386 libxext6:i386 libstdc++6:i386 libexpat1:i386 \\         libxext6 libxrender1 libxtst6 libgtk2.0-0 make \\     &amp;&amp; rm -rf /var/lib/apt/lists/*  # Install MPLAB RUN wget https://ww1.microchip.com/downloads/en/DeviceDoc/MPLABX-${MPLABXVER}-linux-installer.tar -q --show-progress --progress=bar:force:noscroll -O MPLABX-${MPLABXVER}-linux-installer.tar \\     &amp;&amp; tar xf MPLABX-${MPLABXVER}-linux-installer.tar &amp;&amp; rm -f MPLABX-${MPLABXVER}-linux-installer.tar \\     &amp;&amp; USER=root ./*-installer.sh --nox11 \\     -- --unattendedmodeui none --mode unattended \\     &amp;&amp; rm -f MPLABX-${MPLABXVER}-linux-installer.sh  # Install XC32 RUN wget https://ww1.microchip.com/downloads/en/DeviceDoc/xc32-${XC32VER}-full-install-linux-installer.run -q --show-progress --progress=bar:force:noscroll -O xc32-${XC32VER}-full-install-linux-installer.run\\     &amp;&amp; chmod a+x xc32-${XC32VER}-full-install-linux-installer.run \\     &amp;&amp; ./xc32-${XC32VER}-full-install-linux-installer.run \\     --mode unattended --unattendedmodeui none \\     --netservername localhost --LicenseType FreeMode \\     &amp;&amp; rm -f xc32-${XC32VER}-full-install-linux-installer.run  ENV PATH $PATH:/opt/microchip/xc32/${XC32VER}/bin ENV PATH $PATH:/opt/microchip/mplabx/${MPLABXVER}/mplab_platform/bin    Compiling your MPLAB X (windows) project in a Linux shell   One of the key features of MPLABX that I like the most is its ability to dynamically generate Makefiles for the project in such a way that it can be used in automated build systems with ease. It does take some getting used to. However, once you figure out the bits and pieces, you will realize that there are quiet a few powerful features built into the system. However, one pain point is that the generated Makefiles rely on some IDE utilities which makes IDE installation in the build machine mandatory. It would have been better if we can get away with just installing the Compiler.   Setting up a build VM   I prefer to use a docker or VM to prepare sandboxed environments. To keep it simple for a beginner , we will use some UI tools here. We will use Oracle VitrualBox to setup a Ubuntu VM and install the required build tools into it. Steps follow:      Install Oracle VirtualBox and create a VM .   In my workstation, I configured for 2 Processors, 8GB RAM , 30GB Disk Space and Bridged networking   Download Ubuntu ISO and install the minimal configuration into the VM.   Issue following commands to download and install the IDE and tools via the commandline.   # install dependencies sudo dpkg --add-architecture i386 sudo apt-get update -yq &amp;&amp; sudo apt-get upgrade -yq sudo apt-get install build-essential lib32z1 libc6:i386 libx11-6:i386 libxext6:i386 libstdc++6:i386 libexpat1:i386 wget -y  #Download and install IDE wget http://ww1.microchip.com/downloads/en/DeviceDoc/MPLABX-v5.30-linux-installer.tar -q --show-progress --progress=bar:force:noscroll -O MPLABX-v5.30-linux-installer.tar  tar xf MPLABX-v5.20-linux-installer.tar  rm -f MPLABX-v5.20-linux-installer.tar sudo ./MPLABX-v5.30-linux-installer.sh --nox11 -- --unattendedmodeui none --mode unattended rm -f MPLABX-v5.20-linux-installer.sh  # Download and install Compiler wget http://ww1.microchip.com/downloads/en/DeviceDoc/xc32-v2.30-full-install-linux-installer.run -q --show-progress --progress=bar:force:noscroll -O xc32-v2.30-full-install-linux-installer.run chmod a+x xc32-v2.30-full-install-linux-installer.run  ./xc32-v2.30-full-install-linux-installer.run --mode unattended --unattendedmodeui none --netservername localhost --LicenseType FreeMode &amp;&amp; rm -f xc32-v2.20-full-install-linux-installer.run    Testing the setup with a build test   To test the installation and setup, clone the repo into the new VM, navigate to the project location (were the Makefile resides ) and issue the following command. This command overrides the local tool location variables in the project with the settings of the VM we just setup.   make \\ SHELL=bash \\ MKDIR=\"mkdir -p\" \\ PATH_TO_IDE_BIN=\"/opt/microchip/mplabx/v5.35/mplab_platform/platform/../mplab_ide/modules/../../bin/\" \\ PATH:=\"/opt/microchip/mplabx/v5.30/mplab_platform/platform/../mplab_ide/modules/../../bin/\":$PATH \\ OS_CURRENT=\"$(uname -s)\" \\ MP_JAVA_PATH=\"/opt/microchip/mplabx/v5.30/sys/java/jre1.8.0_181/bin/\" \\ MP_CC=\"/opt/microchip/xc32/v2.30/bin/xc32-gcc\" \\ MP_CPPC=\"/opt/microchip/xc32/v2.30/bin/xc32-g++\" \\ MP_AS=\"/opt/microchip/xc32/v2.30/bin/xc32-as\" \\ MP_LD=\"/opt/microchip/xc32/v2.30/bin/xc32-ld\" \\ MP_AR=\"/opt/microchip/xc32/v2.30/bin/xc32-ar\" \\ DEP_GEN=\"${MP_JAVA_PATH}java -jar /opt/microchip/mplabx/v5.30/mplab_platform/platform/../mplab_ide/modules/../../bin/extractobjectdependencies.jar\" \\ MP_CC_DIR=\"/opt/microchip/xc32/v2.30/bin/\" \\ MP_CPPC_DIR=\"/opt/microchip/xc32/v2.30/bin/\" \\ MP_AS_DIR=\"/opt/microchip/xc32/v2.30/bin/\" \\ MP_LD_DIR=\"/opt/microchip/xc32/v2.30/bin/\" \\ MP_AR_DIR=\"/opt/microchip/xc32/v2.30/bin/\" \\ DFP_DIR=\"/opt/microchip/mplabx/v5.30/packs/Microchip/PIC32MZ-EF_DFP/1.1.45\" -j4   This will compile your project using the newly installed toolchain.  If you don’t want to do this step manually, the easiest is to execute the tool in the following path. This will re-generate all the Makefiles for you just as if you opened the project in the IDE.   /opt/microchip/mplabx/v5.30/mplab_platform/bin/prjMakefilesGenerator.sh &lt;proj.X&gt;    Attaching the build runner to your repo   Navigate to the GitHub repo settings and click on “Actions”&gt;”Add Runner”. The pop-up will provide a bunch of steps to add the actions runner package into your VM. Essentially, this is a script that listens to invocations from GitHub Actions and execute some steps. In the case of the image below, I have attached 2 runners to the repo. This means 2 workflows can be triggered in parallel.                                               Adding self hosted runner.         It is recommended to install the runner as a service into your VM so that it starts seamlessly every time. To do this, issue the following command.   sudo ./svc.sh install   Once the runner is executing in the background, you can check its status using sudo ./svc.sh status   Creating a build test workflow   Now that we have attached a runner to the repo, we need to create a workflow. A workflow is essentially the set of steps and execution conditions that are given to GitHub actions. When the condition is met, the steps in the workflow will be executed on the runner.                                               Create a new workflow.         Go to “Actions” in your repo. Github would have already identified that this is a C/C++ project with Makefile based builds. Click on “Set up this workflow”. This will create a new template .yaml script within .github/workflows of your project. A sample .yaml script that compiles the PIC32 project and upload the artifact back to the workflow results is given below. Once the contents are pasted, commit the file to your repo. Note that we are specifying self-hosted   name: HelloCIWorld-pic32mz  on: [push]  jobs:   build:      runs-on: self-hosted          steps:     - uses: actions/checkout@v2     - name: clean       run: make SHELL=bash MKDIR=\"mkdir -p\" PATH_TO_IDE_BIN=\"/opt/microchip/mplabx/v5.35/mplab_platform/platform/../mplab_ide/modules/../../bin/\" PATH:=\"/opt/microchip/mplabx/v5.30/mplab_platform/platform/../mplab_ide/modules/../../bin/\":\"/opt/microchip/xc32/v2.30/bin/\":$PATH OS_CURRENT=\"$(uname -s)\" MP_JAVA_PATH=\"/opt/microchip/mplabx/v5.30/sys/java/jre1.8.0_181/bin/\" MP_CC=\"/opt/microchip/xc32/v2.30/bin/xc32-gcc\" MP_CPPC=\"/opt/microchip/xc32/v2.30/bin/xc32-g++\" MP_AS=\"/opt/microchip/xc32/v2.30/bin/xc32-as\" MP_LD=\"/opt/microchip/xc32/v2.30/bin/xc32-ld\" MP_AR=\"/opt/microchip/xc32/v2.30/bin/xc32-ar\" DEP_GEN=\"${MP_JAVA_PATH}java -jar /opt/microchip/mplabx/v5.30/mplab_platform/platform/../mplab_ide/modules/../../bin/extractobjectdependencies.jar\" MP_CC_DIR=\"/opt/microchip/xc32/v2.30/bin/\" MP_CPPC_DIR=\"/opt/microchip/xc32/v2.30/bin/\" MP_AS_DIR=\"/opt/microchip/xc32/v2.30/bin/\" MP_LD_DIR=\"/opt/microchip/xc32/v2.30/bin/\" MP_AR_DIR=\"/opt/microchip/xc32/v2.30/bin/\" DFP_DIR=\"/opt/microchip/mplabx/v5.30/packs/Microchip/PIC32MZ-EF_DFP/1.1.45\" clean     - name: make       run: make SHELL=bash MKDIR=\"mkdir -p\" PATH_TO_IDE_BIN=\"/opt/microchip/mplabx/v5.35/mplab_platform/platform/../mplab_ide/modules/../../bin/\" PATH:=\"/opt/microchip/mplabx/v5.30/mplab_platform/platform/../mplab_ide/modules/../../bin/\":\"/opt/microchip/xc32/v2.30/bin/\":$PATH OS_CURRENT=\"$(uname -s)\" MP_JAVA_PATH=\"/opt/microchip/mplabx/v5.30/sys/java/jre1.8.0_181/bin/\" MP_CC=\"/opt/microchip/xc32/v2.30/bin/xc32-gcc\" MP_CPPC=\"/opt/microchip/xc32/v2.30/bin/xc32-g++\" MP_AS=\"/opt/microchip/xc32/v2.30/bin/xc32-as\" MP_LD=\"/opt/microchip/xc32/v2.30/bin/xc32-ld\" MP_AR=\"/opt/microchip/xc32/v2.30/bin/xc32-ar\" DEP_GEN=\"${MP_JAVA_PATH}java -jar /opt/microchip/mplabx/v5.30/mplab_platform/platform/../mplab_ide/modules/../../bin/extractobjectdependencies.jar\" MP_CC_DIR=\"/opt/microchip/xc32/v2.30/bin/\" MP_CPPC_DIR=\"/opt/microchip/xc32/v2.30/bin/\" MP_AS_DIR=\"/opt/microchip/xc32/v2.30/bin/\" MP_LD_DIR=\"/opt/microchip/xc32/v2.30/bin/\" MP_AR_DIR=\"/opt/microchip/xc32/v2.30/bin/\" DFP_DIR=\"/opt/microchip/mplabx/v5.30/packs/Microchip/PIC32MZ-EF_DFP/1.1.45\"  -j4 -C HelloCIWorld/firmware/HelloCIWorld.X     - uses: actions/upload-artifact@v1       with:         name: noEthBuild_$.$_$.zip         path: HelloCIWorld/firmware/HelloCIWorld.X/dist/HelloCIWorld/production    If prjMakefilesGenerator was used, you can just provide the make command.   As soon as the file is committed, you can see that the runner will be executing a build. This is because the script calls for the workflow to be executed as soon as a push is made into the repo.                                               Runner executing.         Results can be observed in the “Actions” tab of GitHub repo by clicking on the specific workflow.                                               Workflow Results.         ","categories": ["Articles","Tutorial"],
        "tags": ["Embedded","devops"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/Setting-up-a-github-actions-CI-system-for-embedded-systems/",
        "teaser": null
      },{
        "title": "Analyzing size of ELF sections and symbols to generate a modular report.",
        "excerpt":"  Source code for this article is at vppillai/MPLABXMemoryAnalyzer.   Introduction   If you are working on any serious embedded systems project, you will come across the need to optimize the memory footprint of your code sooner or later. When the crude methods like high levels of compiler optimization and the standard tips and tricks wont cut it any more, you will have to break out your work boots and the hardhat and get to digging. A map file typically gives you a fair idea of what pieces in your code is taking up the most memory. There are even wonderful tools like amap that gives you a graphical interface to look into the map file output and analyze the memory distribution in your image. However, IDEs tend to have their own ways of dealing with how your code is rearranged to facilitate a build. This means, the names of source files that you see in the map file will be modified and will not be indicative of your original source code structure. MPLABX is particularly notorious for doing this.   When you compile an MPLABX project, it creates an _ext folder with a bunch of compilation unit numbers based on how the IDE sees your code. In large projects, you will be importing a lot of pre-written code from SDKs and libraries that you want to group together for your initial memory analysis. The IDE re-grouping code renders the tools like amap unusable to perform modular analysis of the memory footprint of your code.   The implementation we are discussing in this article gives an output like:                                               Summary of memory usage         Source code and instructions are available in Github.   Problem Statement   I had to write a tool to address this very specific issue while working on a PIC32 based project. I had a bunch of third-party tools and Harmony3 code that were being included in my project along with my application code. I wanted to get a report of the modular distribution of static memory. I wanted:           Reporting of memory elements as given by the gnu size command.            Grouping of known modules like TCP/IP , Peripherals etc. to give an overview for further analysis.            Reporting of symbol-file-size mapping in such a way that I can trace back the file corresponding to the symbols.       Existing tools   There are a bunch of existing tools that could satisfy parts of my requirement. These were the tools that I eventually used to piece together my own implementation.   amap   First in the group is amap. It is a cross-platform Map File Browser that gives a visual interface to navigate and filter map file entries. It is a wonderful tool for manual analysis. However, the issues I faced were:           The map file generated by xc32 includes a bunch of non-linked sections like .gnu.attributes or .comment. These are not included in the final image but there is no straight forward way to filter these out from the report in one-shot.            Because of the _ext folder based mechanism that MPLABX has, the modules reported by amap is limited to components included from a pre-compiled library.       MapViewer   From its Github page, MapViewer is a Windows (C#/.NET)application that displays information extracted from the MAP file (generated by the GNU linker LD) and from the ELF executable image.   This tool has a mechanism to filter out the un-used sections and reads the elf file to generate a more accurate report. It also uses the platform specific tools like xc32-readelf and xc32-nm to read the elf. So, the report generated is more accurate. However,           The tools got hit by MPLABX’s bloody _ext build folder. So, there is no way to map the symbols to the actual source files.            It takes an awful lot of time to generate the report. It took around 6 minutes in my case with a fairly large image.       Dwex   A cross-platform GUI utility for visualizing the DWARF debugging information in executable files, built on top of pyelftools and filebytes. Runs on Windows, MacOS X, and Linux. Supports parsing the following file types for DWARF data:   While not a tool for parsing map files or to generate size information, dwex gave me a lot of insight into the DWARF assortment in object files. I had a decent understanding of DWARF. But dwex solidified the understanding greatly and gave me a breakthrough in my implementation. I will explain this in a later section.   This tool is a goldmine for anyone who wants to explore elf, DWARF or even pyelftools.   gcc tools   gcc comes with all the tools required to analyze and manipulate all aspects of code and image. However, there is a learning curve.   Implementation Details   Source code and releases can be found in Github. There is a single file names mapfileParse.py. The following explanations are based on the code in this file.           A lot of the heavy lifting is done using amap and xc32-readelf. This is to avoid reinventing the wheel.            Do basic sanity checks and cleanup using setPaths() and checkTools().       a. setPaths() makes sure that irrespective of where the tool is executed from, absolute paths are used. For some reason, relative paths were very slow to execute with xc32-readelf       b. checkTools() simply uses xc32-size to see of the required PATHs are set.            Intermediate and stale files are deleted in cleanupOutput()            cleanupMapFile() cleans the map file by removing all unlinked sections. I am using plain old regex for this. This creates a new map file that can be used directly with amap. The code as well passes this to amap and generates parsed csv compatible output.            parseMap() takes amap output and maps existing modules that has been identified by amap. This is typically just available for symbols coming from pre-compiled libs.            attachFileNames() then uses xc32-addr2line to find actual source file names of symbols that has not been assigned a module name by amap. However, some of the source files cannot be identified by addr2line. This is typically for .S files . But I have seen this happening for other files as well. I did not really go digging to find the reason for this.              addr2elf can accept multiple addresses and provide the corresponding file names and line numbers in one shot. We pass 2000 addresses in one shot . This minimizes the number of times xc32-addr2elf needs to be invoked. These calls are very expensive in terms of execution time.                finalizeFileNames() does something interesting to fill in the gaps where addr2line failed to map a file name. amap gives the compiled object file name inside the _ext folder. We use xc32-readelf to dump out the DIE0 of the DWARF info in this object file. This includes the DW_AT_name entry that points to the actual source file that was used to generate the object file inside _ext.              Once we identify the source file of an object, we store this in an internal dictionary to re-use it for all other symbols inside that object file. This saves a ton of time since calling the gcc elf tools are very expensive in terms of execution time.                filewiseSize() adds up all the sizes corresponding to a file name. This gives us an aggregate of the components reported by xc32-size on the whole elf - but now split file wise.              This includes .text, .rodata, .data &amp; .bss.                summarizeComponents() maps components into modules based on a set of paths:module relationship defined in compDefinition.      ","categories": ["Articles","Tutorial"],
        "tags": ["Embedded","devops"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/analyzing-size-of-ELF-sections-and-symbols/",
        "teaser": null
      },{
        "title": "A hands-on approach to measuring microcontroller performance using Coremark.",
        "excerpt":"  Source code for this article is at vppillai/CoremarkH3.   Introduction  Measuring the performance of a microcontroller is not a trivial task. There are too many architectural nuances to consider. Parameters like the number of cores, execution memory pipelines, code optimization, and toolchain increase analysis complexity even further. All this makes comparing the performance of different microcontrollers a challenging task. Numerous benchmarking stacks have been developed over time to address this challenge. However, porting these to a bare-metal system as part of silicon or board bring-up often consumes much effort. This article will go through the steps required to port, compile, and execute the Coremark benchmark tests on a PIC32 microcontroller. However, the steps enumerated here are generic enough to be used with any microcontroller. Let’s go.   The Embedded Microprocessor Benchmark Consortium website says, “CoreMark is a simple, yet sophisticated benchmark that is designed specifically to test the functionality of a processor core. Running CoreMark produces a single-number score allowing users to make quick comparisons between processors.”. It replaces the antiquated Dhrystone benchmark and contains implementations of the algorithms such as list processing (find and sort), matrix manipulation (common matrix operations), state machine (determine if an input stream contains valid numbers), and CRC (cyclic redundancy check). It is designed to run on devices from 8-bit microcontrollers to 64-bit microprocessors. To ensure compilers cannot pre-compute the results at compile-time, every operation in the benchmark derives a value that is not available at compile-time. Furthermore, all code used within the timed portion of the benchmark is part of the benchmark itself and does not rely on library calls. The result is a single number that can quantify a CPU’s ability.   The code for Coremark is available on Github. However, since the benchmark can run on bare-metal and Linux systems alike, porting it to a new bare-metal platform can be confusing.   Platform Code   The main components required to port and execute Coremark on a bare-metal platform are           System initialization code.            A mechanism to measure and report time.            A UART driver to report the results.       In the case of PIC32, we can quickly generate these using the Harmony 3 framework. The project graph for this setup would look like this:                                               Project Configuration         The source code for Coremark includes the main() routine. So, we configure the harmony project not to create application files and the main source file. Besides this, there are no changes made to the default device configuration.                                               Disable main file generation                                                     Disable app file generation         With these configurations, we generate the base code framework to start porting Coremark on a PIC32 device. In case you are using a different platform, there are for sure steps equivalent to this that you can perform to create a base project that includes the system initialization sequence. In the next section, we look at how the platform code can is hooked up to the Coremark porting layer.   Porting layer   The default Coremark package is straightforward to use with a system with ‘Make.’ However, in most cases like ours, we would have an existing compilation system. To make it easy to integrate Coremark with our platform code, we include the following essential files into the project:                                               Include Coremark Source files         core_portme files are copied from the base template is available in the ‘simple’ folder of Coremark source.   To initialize the system, we include a call to SYS_Initialize ( NULL ); the first item in portable_init() function within core_portme.c. It ensures that the system clock and peripherals initialize to the proper state before we start any operation.   Since we include the Harmony 3 time system service, all the required timing-related structures and functions are hooked-up automatically. However, we need to implement the time() function manually. We first register a periodic callback with SYS_TIME from within the portable_init() function with :   SYS_TIME_CallbackRegisterMS(MyCallback, (uintptr_t)0, 1000, SYS_TIME_PERIODIC);   Within the callback, we increment a variable to keep track of the number of seconds elapsed since init.   uint32_t timeSec=0;  void MyCallback ( uintptr_t context){   timeSec++; }   Then we implement the time() function with   time_t time(time_t *tod)  {   if (tod != NULL)     *tod = timeSec;   return timeSec; }       Note: The code compiles even without the implementation of time() using a stub. However, functionality is impaired, and execution fails.    Next, we define the execution parameter macros under project properties.                                               Define execution variables         ITERATIONS determines the number of cycles the benchmark executes. It should be tweaked to let the benchmark execute for at least 10s for valid results.   PERFORMANCE_RUN is required to get results for reporting back to EEMBC.   Next, we update COMPILER_FLAGS and MEM_LOCATION (STACK) in core_portme.h.   Additional compiler flags   The following compiler flags can be enabled to improve the overall score. Some of these options might require a pro compiler.           Enable top-level reordering (-ftoplevel-reorder)            O3 optimization (-O3)            Loop Unrolling (-funroll-loops)            Pre and post-optimization instruction scheduling (-fschedule-insns -fschedule-insns2)       Execution   Compile the program, flash it into your device, and wait for the results to appear in the UART console. How long you need to wait depends on your value of ITERATIONS. I had to wait for 10s with ITERATIONS=6000.   The results are quite good, with a score of 600 or 3 Coremark / MHz. It can be compared with other devices on the official coremark scores website.    2K performance run parameters for coremark. CoreMark Size : 666 Total ticks : 10 Total time (secs): 10.000000 Iterations/Sec : 600.000000 Iterations : 6000 Compiler version : GCC4.8.3 MPLAB XC32 Compiler v2.40 Compiler flags : -O3 Memory location : STACK seedcrc : 0xe9f5 [0]crclist : 0xe714 [0]crcmatrix : 0x1fd7 [0]crcstate : 0x8e3a [0]crcfinal : 0xa14c  Correct operation validated. See README.md for run and reporting rules. CoreMark 1.0 : 600.000000 / GCC4.8.3 MPLAB XC32 Compiler v2.40 -O3 / STACK   ","categories": ["Articles","Tutorial"],
        "tags": ["Embedded","performance"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/a-hands-on-appraoch-to-Measuring-microcontroller-performance-using-Coremark/",
        "teaser": null
      },{
        "title": "Linux & Python on RISC-V using QEMU from scratch",
        "excerpt":"  Introduction  As of today, RISC-V hardware is not always easy to come by on a hobbyist budget. But this need not stop you from exploring the ecosystem. This article explores how to set up RISC-V on QEMU, boot a simple application on it using the PK kernel, and boot a cross-compiled Linux on the emulation platform. We top it by booting a standard Linux distro with python support.   We will be compiling all the tools from scratch to get a good familiarity with the pieces involved. There is quite a lot of info already available out there. So, it might be overwhelming for a beginner. This article brings together the end to end flow into a single document.   Familiarizing the toolchain using spike   We need a bunch of tools to get started.           riscv-gnu-toolchain is the toolchain to compile applications for RISC-V. Two flavors run on Linux:                       riscv64-­unknown-­elf-gcc that uses newlib and is used for small statically linked standalone programs and embedded targets.                        riscv64-unknown-­linux-­gnu-­gcc that uses glibc and can be used to build programs that can be dynamically linked and executed on an OS like Linux.                          if you compile with -nostartfiles -nostdlib -nostdinc both the toolchains will work the same way.                 spike is a RISC-V ISA Simulator that is the golden reference for the ISA. It provides full system emulation or proxied emulation (using HTIF/FESVR). It is the universal starting point to explore RISC-V targeted software.            RISC-V Proxy Kernel, commonly known as pk is an application execution environment that can host statically linked RISC-V ELF binaries. Besides initialization and basic system setup, pk primarily traps I/O system calls in a tethered I/O limited environment and proxies them to the host. If the RISC-V core runs tethered to an actual host over FESVR, pk send it back to the host. In the case of spike, the requests are relayed back to the OS hosting spike.       I thought the easiest way to get all the tools in one-shot is to using the riscv-tools repo.   So, I started by installing the dependencies with:   sudo apt install autoconf automake autotools-dev curl libmpc-dev libmpfr-dev libgmp-dev \\                  libusb-1.0-0-dev gawk build-essential bison flex texinfo gperf libtool \\                  patchutils bc zlib1g-dev device-tree-compiler pkg-config libexpat-dev  \\                  libncurses5-dev libncursesw5-dev   Then clone the repo, init submodules, export the install location and trigger the build with :   git clone https://github.com/riscv/riscv-tools.git git submodule update --init --recursive export RISCV=~/riscv ./build.sh   But since the riscv-tools repo is not actively maintained, I ended up getting an error during compilation                                               riscv-tools build error         So, I decided to compile each tool manually.   For the GCC tools,   git clone https://github.com/riscv/riscv-gnu-toolchain cd riscv-gnu-toolchain ./configure --prefix=$RISCV   Pass the option --enable-multilib to build the toolchain with 32-bit and 64-bit support.   To compile the embedded toolchain simply do make -j $(nproc). For the linux version, give make -j $(nproc) linux .   This will clone all required submodules and compile the toolchain and install it to the $RISCV folder. At the end of the process, we will have both the toolchains.                                               our newly-compiled GCC toolchain         Next, we compile spike from the riscv-isa-sim repo. From the cloned repo, give:   mkdir build cd build ../configure --prefix=$RISCV make -j $(nproc) make install   This will generate spike tools:                                               Generated spike tools         We now need to compile riscv-pk to bootstrap bare-metal programs in spike. Make sure that the path where you installed the GCC toolchain is available in PATH. From the cloned repo, give:   mkdir build cd build ../configure --prefix=$RISCV --host=riscv64-unknown-elf make -j $(nproc) make install   This will generate the following tools:                                               Generated `PK` and bbl         bbl is the Berkeley bootloader, which is a supervisor execution environment for tethered RISC-V systems.   Moment of truth. Let us compile and run a hello world application with RISC-V simulation in spike.                                               Successful first bare-metal program         This gives us confidence that the toolchain we just built is working.   Spike also has a bunch of debug capabilities. For now, we are not going into the details of a debugging setup since we want first to set up QEMU and run riscv-linux.   Setting-up QEMU for RISC-V emulation   What we saw with spike is essentially bare-metal simulation with support from pk. While it can be done, running whole Linux operating systems in spike is not efficient. We will use system emulation with QEMU for that.   riscv-qemu has been mainstreamed a while back. So, we will use the upstream QEMU repository.   First, install the dependencies.   sudo apt install ninja-build libglib2.0-dev libpixman-1-dev   Clone the repo and move to the latest release tag:   git clone https://git.qemu.org/git/qemu.git git checkout v5.2.0 ./configure --target-list=riscv64-softmmu,riscv64-linux-user --prefix=$RISCV make -j $(nproc) make install   This will generate the following files.                                               newly built QEMU tools         Compiling the latest Linux kernel, busybox and creating a rootFS   First, install some more deps   sudo apt install libncurses5-dev libncursesw5-dev   Compiling the kernel   We need to cross-compile the kernel and busybox separately. To get the latest Linux kernel and compile it with the toolchain we compiled, follow these steps:   git clone https://github.com/torvalds/linux.git cd linux git checkout v5.10 make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- defconfig make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- -j $(nproc)   Note: In this iteration, we are building a Linux kernel with the default options.   At the end of this build, we will have the compressed, bootable RISC-V kernel in linux/arch/riscv/boot/Image   Compiling busybox.   For the uninitiated, busybox clubs, many userland tools into a single binary and can finally give us the shell at boot.   git clone https://git.busybox.net/busybox cd busybox git checkout 1_32_1 CROSS_COMPILE=riscv64-unknown-linux-gnu- make defconfig CROSS_COMPILE=riscv64-unknown-linux-gnu- make menuconfig   Enable static linking under settings to make it easier to prepare the rootFS later.                                               Build options for statically linked `busybox`         CROSS_COMPILE=riscv64-unknown-linux-gnu- make -j $(nproc)   Creating a rootFS   First , we will create a NULL disk and format it as an ext2   dd if=/dev/zero of=root.bin bs=1M count=64 mkfs.ext2 -F root.bin    We will then locally mount the newly created disk image, create the bare minimum files and copy the busybox we compiled.   mkdir mnt sudo mount -o loop root.bin mnt cd mnt  sudo mkdir -p bin etc dev lib proc sbin tmp usr usr/bin usr/lib usr/sbin sudo cp ~/busybox/busybox bin sudo ln -s ../bin/busybox sbin/init sudo ln -s ../bin/busybox bin/sh cd .. sudo umount mnt   Now the rootFS looks like this:                                               rootFS tree         We have intentionally avoided creating an inittab since this manual process is more for understanding the process. We are relying on Kernel defaults. A full-fledged rootFS can be created using buildroot / Yocto.   First QEMU boot   Boot the Linux image in our QEMU using   qemu-system-riscv64 -nographic -machine virt \\                     -kernel linux/arch/riscv/boot/Image \\                     -append \"root=/dev/vda rw console=ttyS0\" \\                     -drive file=root.bin,format=raw,id=hd0 \\                     -device virtio-blk-device,drive=hd0   Note: to exit QEMU, type CTRL+a then x.   At boot, execute the following command to make busybox install all the required utility mappings   /bin/busybox --install -s                                               SUCCESS!!         Running an actual OS   To build an embedded target rootFS, we typically use buildroot or Yocto. But, I decided to use a standard OS to start a complete RISC-V exploration.      If buildroot is used use the qemu_riscv64_virt_defconfig config and boot with the command below:    qemu-system-riscv64 -nographic -machine virt -kernel output/images/Image \\                     -append \"root=/dev/vda rw console=ttyS0\"             \\                     -drive file=output/images/rootfs.ext2,format=raw,id=hd0\\                     -device virtio-blk-device,drive=hd0   We start by installing virt-builder, a tool that lets us quickly build virtual machines.   sudo apt install libguestfs-tools   By default RISC-V images wont appear in the list. So, we configure the builder repos to be able to pull fedora images.   mkdir -p ~/.config/virt-builder/repos.d/ cat &lt;&lt;EOF &gt; ~/.config/virt-builder/repos.d/fedora-riscv.conf [fedora-riscv] uri=https://dl.fedoraproject.org/pub/alt/risc-v/repo/virt-builder-images/images/index EOF   Now, we can see the RISC-V images by giving the command   virt-builder --list | grep riscv64                                               looking for fedora images         We now get the raw disk image and unzip it with :   wget https://dl.fedoraproject.org/pub/alt/risc-v/repo/virt-builder-images/images/Fedora-Developer-Rawhide-20200108.n.0-sda.raw.xz unxz -k Fedora-Developer-Rawhide-20200108.n.0-sda.raw.xz   This will create the file Fedora-Developer-Rawhide-20200108.n.0-sda.raw   We also need the bootloader. For this, find and download the image using:   virt-builder --arch riscv64 --notes fedora-rawhide-developer-20200108.n.0 | grep fw\\_payload   wget https://dl.fedoraproject.org/pub/alt/risc-v/repo/virt-builder-images/images/Fedora-Developer-Rawhide-20200108.n.0-fw\\_payload-uboot-qemu-virt-smode.elf   Now boot fedora with:   export VER=20200108.n.0 qemu-system-riscv64 -machine virt                      -nographic \\                     -smp 4 \\                     -m 8G \\                     -kernel Fedora-Developer-Rawhide-${VER}-fw_payload-uboot-qemu-virt-smode.elf \\                     -object rng-random,filename=/dev/urandom,id=rng0 \\                     -device virtio-rng-device,rng=rng0 \\                     -device virtio-blk-device,drive=hd0 \\                     -drive file=Fedora-Developer-Rawhide-${VER}-sda.raw,format=raw,id=hd0 \\                     -device virtio-net-device,netdev=usernet \\                     -netdev user,id=usernet,hostfwd=tcp::3333-:22   You can see the system info after boot                                               RISC-V QEMU cpuinfo                                                     Python3 on RISC-V + Linux         ","categories": ["Articles","Tutorial"],
        "tags": ["QEMU","Linux","python","RISC-V"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/Linux-Python-on-RISCV-using-QEMU-from-scratch/",
        "teaser": "https://www.embeddedinn.xyz/images/posts/riscv_qemu/image13.png"
      },{
        "title": "Adding a custom peripheral to QEMU RISC-V machine emulation & interacting with it using bare-metal C code",
        "excerpt":"  Introduction   QEMU is an excellent platform to emulate hardware platforms. But, we often end up using ready-made platforms without thinking twice about how QEMU emulates them. This article dives into the depth of how a new peripheral can be added to an existing QEMU machine and how to interact with it using bare-metal C code. Ultimately, we will build a RISC-V machine that has our custom peripheral and driver for that peripheral.   Compiling QEMU and understanding the build system:      For detailed steps on setting up the tooling and compiling all the required pieces, refer to my previous article: Linux &amp; Python on RISC-V using QEMU from scratch.    Before we add things to QEMU, we need first to understand the development ecosystem. I am compiling the latest 5.2.0 version of QEMU for this.   QEMU has a build flow similar to the autoconf tools based flow – we first do a configure and then make. However, recently, QEMU started using uses Meson build system under the hood instead of GNU autotools. The main difference we note at first is that only VPATH builds are supported. Configuration environment properties are identified by the configure script, recipes by Kconfig, Meson takes over at that point to tie together the build. Even though Kconfig is used, there is no UI to manage the configurations. This info will come in handy when we later add our peripherals into the machine emulation.   To build QEMU for the RISC-V machine,   git clone https://github.com/qemu/qemu.git git checkout v5.2.0 cd qemu mkdir build cd build  ../configure --target-list=riscv64-softmmu,riscv64-linux-user --prefix=$RISCV  make -j$(nproc) install      $RISCV is where we want the built images to be stored       Note that we are building the softmmu version as well as the linux-user version.    From the QEMU manual, we know that :      new targets and boards can be added without knowing in detail the architecture of the hardware emulation subsystems. Boards only have to list the components they need, and the compiled executable will include all the required dependencies and all the devices that the user can add to that board.    This is what we will leverage to add our first peripheral.   Executing bare-metal RISC-V C code in QEMU   We will use riscv-probe as the base to start executing bare metal code. From the official repo readme, we know that:      riscv-probe contains libfemto which is a lightweight bare-metal C library conforming to a reduced set of the POSIX.1-2017 / IEEE 1003.1-2017 standard. libfemto can be used as a starting point for bare metal RISC-V programs that require interrupt handling, basic string routines, and printf().    We clone the repo and compile it with:   git clone https://github.com/michaeljclark/riscv-probe.git      the toolchain we compiled in the previous article was not compiled with --enable-multilib. So, I had to recompile it.    The default did not build !!   So, I had to define riscv_excp_names and riscv_intr_names as extern to get it to compile. After digging around, I found that PR#9 fixes a whole bunch of these issues. So, I cloned the patch branch (patch1 of axel-h / riscv-probe) and used it instead   git clone &lt;https://github.com/axel-h/riscv-probe.git&gt; cd riscv-probe git checkout patch1 make   Then execute the “hello world” code in the 64-bit virt machine using:   qemu-system-riscv64 -nographic -machine virt -kernel build/bin/rv64imac/virt/hello -bios none      The -bios none option is not present in the official riscv-probe documentation. But with newer QEMU virt machines, you need to pass this option since the bios FW is loaded by default, and this overlaps with our bare-metal code.                                                bare-metal hello world         Understanding the libfemto code flow   Here are the initialization and execution sequence of the riscv-probe femto ecosystem. This understanding is essential to generate a stripped out version of riscv-probe for us to use.           The C runtime (crt.s -&gt; crtm.s) contains the _start symbol that is the entry point to the C program post compilation. This function sets a generic trap handler and stack pointer and disables all hardware threads except hart 0. Next it calls ` libfemto_start_main() that is defined in libfemto\\arch\\riscv\\start.c`            libfemto_start_main() initializes the memory required to load the C code as per the ABI and clears/inits the device handlers before calling arch_setup() and malloc memory initialization.            arch_setup() is a platform-specific function that defines how the console and power-off sequence functions. For now, we will focus on the virt QEMU machine for RISC-V that supports VirtIO.              This board’s UART console is based on ns16550a that is modeled in libfemto/drivers/ns16550a.c. We will soon realize the reason behind why we are using this specific part.                Finally, the main() from our code will be executed.                                                   libfemto code flow         Stripping down riscv-probe   Since the Makefile is dynamic, we start by deleting all env components other than virt and common. Then I deleted all the examples other than hello and cleaned up the make file to build just the 64-bit hello program. For now, this is what we will use as our “SDK.” I can now modify and run bare metal code.                                               custom bare-metal code         Understanding the QEMU UART flow   In the previous experiment, we did a printf()—a relatively simple task in the application software realm. But, there are a few moving pieces.   From the fundamental code flow analysis, we know that UART is emulated after a ns16550a device. Let us see how a single character passed to printf() reach QEMU frontend. I am beginning this analysis with an assumption that finally, a character gets written into a memory-mapped register processed by the UART emulation in QEMU.   Here is the code flow of printf() within libfemto.                                               printf() code flow         The short version of the info present in the sequence chart is:           The ns16550a driver in libfemto writes each character from printf() into address 0x10000000, where the UART transmit hold register (UART_THR) resides.            qemu\\hw\\riscv\\virt.c file maps VIRT_UART0 to the same address.       The QEMU serial device   Now, the question is, how does QEMU know that the contents of UART_THR have changed and that it needs to be printed on the console?   To understand this, we need first to understand how the UART device at 0x10000000 is mapped in QEMU. By looking at riscv\\virt.c we know that the device is added as a flattened device tree (FDT). This info can also be pulled out of the pre-compiled QEMU instance by doing:   qemu-system-riscv64 -machine virt,dumpdtb=virt.dtb dtc -I dtb -O dts virt.dtb \\&gt;virt.dts    More on this in the next section                                               the QEMU device tree         In virt.c, we also see a call to serial_mm_init() where the VIRT_UART0 address is passed on. Following this trail, we get to know how a printf() in the application code ends up in the emulated QEMU console.           As part of the init function, serial_mm_realize() registers serial_mm_read() and serial_mm_write() functions as callbacks for I/O operations in the VIRT_UART0 address range.              The function memory_region_init_io() is used for this. This is in line with what the QEMU documentation says :              a range of guest memory that is implemented by host callbacks; each read or write causes a callback to be called on the host. You initialize these with memory_region_init_io(), passing it a MemoryRegionOps structure describing the callbacks.       You can see this in action by passing the -trace serial_ioport_write flag while executing QEMU.       Depending on the emulated register to which the write happens, ` serial_mm_write() takes the relevant action. In case of a character to transmit, it calls  qemu_chr_fe_write ()` that ultimately results in the character to be pushed out of the relevant console interface.                                               QEMU serial code flow         QEMU &amp; Device Trees   In the previous experiment, we dumped the device tree blob and converted it into a device tree source using the device tree compiler ( dtc). Then we also saw that the memory I/O callbacks are registered using ` memory_region_init_io()`. Let us take a closer look at how these are related.   The following is my empirical understanding. However, some parts of the system might break if you remove the FDT calls.   QEMU passes on the flattened device tree to the kernel as part of the boot loading process. This is for the kernel to understand the systems architecture – the usual device tree process. However, for bare-metal code to work, the flattened device tree need not be fully populated – unless you want to do some interrupt-based processing. To prove this point, I removed the fdt calls creating the UART0 entry from virt.c, recompiled qemu, and executed the hello world code – It works as expected. One thing to note is that the chosen entry is mandatory. So, I hardcoded the string instead of passing the name argument.                                               Modified device tree for test         Adding a simple, custom peripheral – The butter robot.   Now that we have a relatively good understanding of how a new peripheral can be added in QEMU and controlled via SW let us put that understanding into practice.   We will design a relatively simple memory-mapped peripheral device. The target is to make it appear in the system memory map and access it through C code. The simplest peripheral would be a static register set with pre-defined contents that we can read. This rather useless peripheral’s purpose is to ensure that our understanding of the system architecture is accurate. This will let us graduate into more complex peripherals in the future. Let us call this peripheral “The Butter Robot.”   The life purpose of this peripheral is to host a read-only register that pass (the characters) “BUTTER”                                               The butter robot system architecture         Here are the steps to follow to create the peripheral:      You can see the full diff of changes in my github QEMU fork.            Add a butter_robot folder under hw            Make it visible to the build system by editing hw/meson.build            Create hw/butter_robot/Kconfig and include a symbol that can be used to control the inclusion of our peripheral into various machines.              And include it in hw/Kconfig                Create a build config for our peripheral in hw/butter_robot/meson.build and include the core files based on the Kconfig.            Now, build the core logic in [hw/butter_robot/butter_robot.c](https://github.com/vppillai/qemu/blob/theButterRobot/hw/butter_robot/butter_robot.c) based on our understanding so far.            Include BUTTER_ROBOT into the RISCV_VIRT machine configuration in ` hw/riscv/Kconfig`            Map butter robot into the system’s memory map. I used the location 0x50000000 with a 256-byte address space. This is defined in hw/riscv/virt.c            Instantiate the peripheral by calling br_create()            make and make install the new machine.       I used the following code within the probe SDK we created before to test out the implementation.   #include &lt;stdio.h&gt;  int main(int argc, char **argv) { char *ptr = (char *)(void *)0x50000000; //BUTTER_ROBOT is mapped to this address in the RISCV_VIRT machine printf(\"\\n\\nButter Robot is giving you: %.6s\\n\\n\",ptr); }                                                SUCCESS!!         Debug notes:           The min and max access size defined in the ops structure is critical.              I left this at 4 and faced a bug that was very tricky to find. Finally, I executed QEMU with the -d guest_errors flag and it gave me the reason very clearly. Until then, I was just getting an unhandled trap error.                                                       Exception trap debug         ","categories": ["Articles","Tutorial"],
        "tags": ["QEMU","Butter_Robot","RISC-V"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/Adding-a-custom-peripheral-to-QEMU/",
        "teaser": "https://www.embeddedinn.xyz/images/posts/riscv_qemuPeriph/image14.png"
      },{
        "title": "Exploring virtualization in RISC-V machines",
        "excerpt":"  Virtualizing RISC-V   “RISC-V is classically virtualizable” – We come across this statement very often when we read about the merits of the RISC-V ISA. This article looks at what this statement means and how practical virtualization works with the RISC-V architectures.      Introduction   Virtualization is the key to cloud computing since that is the technology allowing us to partition a highly specced out machine into smaller independent and isolated virtual machines. However, virtualization is not just for enterprise or general-purpose computing. It is advantageous in embedded systems – especially the safety-critical ones, where we can leverage the computing power of a single modern CPU to implement smaller, independent systems. An implementation that would require multiple physical CPUs (chips) in a non-virtualized environment can all be “crammed” into a single chip/single-board solution while still maintaining the standalone nature of each system. The main driving factor for this approach is the need to minimize Size, Weight, Power, and Cost (SWaP+C) in system design.   Understanding the Terminology   Let us first look at some terminology related to virtualization before going into the details of RISC-V virtualization.   The first term to look at is virtualization itself. Simply put, it is the technology that lets you run multiple operating systems (or privileged bare-metal software) from a single physical machine (Host Machine). The virtual instances of the hardware on which the operating systems execute are often referred to as Virtual Machines (VM / Guest VM). These virtual machines are managed by a firmware called the hypervisor that arbitrates (and at times emulates) the Virtual Machine’s access to the underlying hardware.   Classic Virtualization requirements, also known as Popek and Goldberg virtualization requirements, states three main characteristics of a Virtual Machine.           Equivalence / Fidelity: The SW should behave similarly in the VM and the actual HW.            Resource control / Safety: The VM must have absolute control of the virtualized resources.            Efficiency / Performance: The majority of the machine instructions should be executed directly on the hardware.       In the context of classic virtualization, the ISA is further classified into three:           Privileged instructions: Those that trap if the processor is in user mode and do not trap if it is in system mode (supervisor mode).            Control sensitive instructions: Those that attempt to change the configuration of resources in the system.            Behavior sensitive instructions: Those whose behavior or result depends on the configuration of resources (the content of the relocation register or the processor’s mode).       The excerpt below from [2] summarizes the classic virtualizable nature of RISC-V well:   The RISC-V ISA was designed from the ground-up to be classically virtualizable by allowing to electively trap accesses to virtual memory management control and status registers (CSRs) as well as timeout and mode change instructions from supervisor/user to machine mode. Furthermore, RISC-V provides fully precise exception handling, guaranteeing the ability to fully specify the instruction stream state at the time of an exception. The ISA simplicity coupled with its virtualization-friendly design allow the easy implementation of a hypervisor recurring to traditional techniques(e.g., full trap-and-emulate, shadow page tables) as well as the emulation of the hypervisor extension, described further ahead in this section, from machine mode   Motivation for virtualization in embedded systems   The popularity of cloud ecosystems and user VM software like virtual box makes it easy to visualize the concept of a VM. However, another critical area where virtualization plays a critical role in developing modern, cost-effective, resilient systems.   It is a common practice to build redundancy into fault-tolerant embedded systems by having multiple physical computational blocks (say, MCUs) doing the same / monitoring work to derive the final results using a voting and consensus mechanism.   Faults in these systems are often classified broadly into hardware errors and software errors. Hardware errors are caused due to bugs and faults in the hardware or run time faults such as memory read errors in the system bus due to system-level interference or even external causes like radiation. However, the technology involved in building fault-tolerant hardware has significantly evolved in the last couple of decades. Most major semiconductor manufacturers now carry rad-hard, automotive, space, and military-grade silicon that goes through specialized processes to ensure robustness at the silicon level. While these technologies do not prevent the need for hardware redundancy, parts of the system that are relatively less critical can blindly trust these advances. (I know, this is an exaggeration).   Software errors, on the other hand, are evolving into an untamable beast. Irrespective of the amount of testing and defensive coding we do, the sheer scale of modern systems and accelerated development cycles result in corner cases and errors. The difficulty of doing complete integration testing at a system level has resulted in critical systems following a distributed architecture where software systems with relatively lesser complexity will run on dedicated hardware. With this, a failure in one module will not bring the whole system to a grinding halt.   An excellent example to understand this concept is a modern car. A critical system like the ECU, EBS, etc., runs on their dedicated hardware. But, by extension, even systems like climate control, lighting control, infotainment, etc., also run on their hardware. This is where virtualization can be leveraged.   We can potentially run smaller software sub-systems isolated away in their virtual machines while hosting all of them in a single hardware. The primary advantage of this approach is a reduction in hardware cost while keeping software development complexity relatively the same. Each piece of software runs as if it is executing in its dedicated hardware, with the hypervisor taking care of the orchestration. The disadvantage is that there is lower redundancy to protect against hardware failure.   Virtualization in RISC-V   Virtualization is part of the key design goals of RISC-V. This is clearly spelled out right from the design thesis by Waterman [1]   Key RISC-V design goals are:           Separate the ISA into a small base ISA and optional extensions            Support both 32-bit and 64-bit address spaces            Facilitate custom ISA extensions            Support variable-length instruction set extensions            Provide efficient hardware support for modern standards            Orthogonalize the user ISA and privileged architecture, allowing full virtualizability and enabling experimentation in the privileged ISA while maintaining user application binary interface (ABI) compatibility.       The RISC-V reference privileged architecture (RPA) naturally supports classical virtualization and can be extended to support hardware-accelerated virtualization. RPA is orthogonal to the user ISA, and this enables classical virtualization. Suppose privileged features (like interrupt-enable) are exposed to unprivileged (user/guest mode) software. In that case, it can make classical virtualization impossible since there will be no way for a hypervisor or host OS to intercept/tap and emulate the operation.   In a full-fledged implementation, the architecture will look like this:                                               Layers in the RISC-V spec         Applications make requests to the application execution environment (typically an OS) using the ABI.   In a RISC-V system, we often see a Supervisor Binary Interface (SBI) between the hardware and the operating system. This layer, along with the supervisor execution environment (SEE), takes care of implementing privileged instructions. This might be something as simple as a bootloader or a complex Hypervisor. The bottom line is that the SBI abstracts primitive I/O makes the OS more portable and enables virtualization.   A system needs just 2 modes to facilitate a practical operating system that executes applications in isolation from each other. A user mode for regular operations and a privileged mode (set of instructions) that trap when executed in user mode. This is also the minimum criteria to support classic virtualization. RISC-V introduces 2 additional modes specifically to assist virtualization. So, the 4 RISC-V modes privilege modes are:           User mode (U), where the applications generally executes with the least privilege.            Supervisor mode (S) that provides operating systems basic exception processing and virtual memory support.            Hypervisor Mode (H) for Virtual machine monitors with support for I/O access virtualization            Machine Mode (M) with full hardware access. This is the only mandatory mode and is sufficient for simple embedded systems.       Practically there is an HS mode, which is a hypervisor extended supervisor mode that.   From a Virtual memory standpoint, having a Hypervisor mode means that there are 2 levels of address translations:           Application virtual address to guest physical address (VS level page table)            Guest physical address to machine physical address (HS level page table)       The V bit controls the virtualization mode. When V is set, either VU- or VS-mode are executing, and 2nd stage translation is in effect. When V is low, the system may execute in M-, HS- or U- mode. The extension defines a few new hypervisor instructions and CSRs and extends existing machine CSRs to control the guest virtual memory and execution. A hypervisor executing in HS-mode can directly access these registers to inspect the virtual machine state and efficiently perform context switches. The hypervisor must manually manage the original supervisor registers, which are not banked.   The three types of Hypervisors   There are primarily three types of hypervisors:           Type 1 / Complete Monolithic: Native or Bare metal hypervisor with a common software for Host hardware access, CPU virtualization, and Guest IO emulation (e.g. Xvisor, Xen,VMWare ESX Server, Microsoft HyperV, OKL4 Microvisor etc.)            Type 2 / Partially Monolithic: Hosted on an OS as an extension of the base OS kernel (e.g. Linux KVM, FreeBSD Bhyve, VMWare Workstation, Oracle VirtualBox, etc.)            Micro-kernelized: Micro-kernelized hypervisors (e.g. Xen) are usually light-weight micro-kernels providing basic Host hardware access + CPU virtualization in kernel and for rest it depends on a Managment Guest (e.g. Dom0 of Xen) which provides complete Host hardware access, Management interface, and Guest IO emulation.                                                   Type 1 Hypervisor                                                     Type 2 Hypervisor         Getting your hands dirty   We will try out a both type 1 and type 2 hypervisor using QEMU for RISCV. For type 1, we will use Xvisor and For type 2, we will use Linux KVM.   KVM on RISC-V   Compiling the toolchain:   We can either install the RISC tooling that is available on most OS platforms using a package manager, or compile it from the source. I prefer compiling it from the source experiments like these since I will have a better control over the version and feature set of the compiler I am using.   A lot of the dependencies we are installing here will be used even in subsequent steps. So, it is better to install them even if you are installing a pre-packaged compiler.   mkdir ~/riscv export RISCV=~/riscv  sudo apt install autoconf automake autotools-dev curl libmpc-dev libmpfr-dev libgmp-dev \\                  libusb-1.0-0-dev gawk build-essential bison flex texinfo gperf libtool \\                  patchutils bc zlib1g-dev device-tree-compiler pkg-config libexpat-dev  \\                  libncurses5-dev libncursesw5-dev git \t\t\t\t  git clone https://github.com/riscv/riscv-gnu-toolchain cd riscv-gnu-toolchain ./configure --prefix=$RISCV --enable-multilib make -j $(nproc) make -j $(nproc) linux  export PATH=$PATH:$RISCV   The compiler will now be available in $RISCV\\bin.   Alternatively, if you want to use the packaged compiler (in ubuntu), install it with:   sudo apt-get install gcc-riscv64-linux-gnu      In case you are doing this, use the prefix riscv64-linux-gnu- in the steps below, instead of riscv64-unknown-linux-gnu-    Compiling QEMU for RSIC-V with virtualization extensions.   Install additional dependencies   sudo apt install ninja-build pkg-config libglib2.0-dev libpixman-1-dev libtirpc-dev unzip   Compile and install QEMU from official sources. Virtualization support is enabled by default in teh RISC-V configuration in QEMU mainline.   git clone https://github.com/kvm-riscv/qemu.git cd qemu ./configure --target-list=\"riscv32-softmmu riscv64-softmmu\" make -j $(nproc)   The QEMU executable will be available in the build folder.   M-mode runtime for boot: OpenSBI Firmware   For the M mode bootup firmware, we will use openSBI   git clone https://github.com/riscv/opensbi.git cd opensbi export CROSS_COMPILE=riscv64-unknown-linux-gnu- make -j $(nproc) PLATFORM=generic   compile Linux with RISC-V KVM support   RISC-V KVM support is not fully merged into the Linux mainline at the moment. So, we will be using a fork of the kernel to play around with KVM.   git clone https://github.com/kvm-riscv/linux.git mkdir build-riscv64 export ARCH=riscv export CROSS_COMPILE=riscv64-unknown-linux-gnu- make -C linux O=`pwd`/build-riscv64 defconfig make -C linux O=`pwd`/build-riscv64 -j $(nproc)      The same .config will work with the mainline kernel though all options are not available in the RISC-V Kconfig.    The KVM tool to make our lives easier      kvmtool is a lightweight tool for hosting KVM guests. As a pure virtualization tool it only supports guests using the same architecture, though it supports running 32-bit guests on those 64-bit architectures that allow this.”. KVM tool requires libfdt to be cross compiled.    KVM tool depends on dtc at runtime. So we first compile it.   git clone git://git.kernel.org/pub/scm/utils/dtc/dtc.git cd dtc export ARCH=riscv export CROSS_COMPILE=riscv64-unknown-linux-gnu- export CC=\"${CROSS_COMPILE}gcc -mabi=lp64d -march=rv64gc\" TRIPLET=$($CC -dumpmachine) SYSROOT=$($CC -print-sysroot) make libfdt make EXTRA_CFLAGS=\"-mabi=lp64d\" DESTDIR=$SYSROOT PREFIX=/usr LIBDIR=/usr/lib64/lp64d install-lib install-includes   Now, build kvmtool   git clone https://github.com/kvm-riscv/kvmtool.git export ARCH=riscv export CROSS_COMPILE=riscv64-unknown-linux-gnu- cd kvmtool make lkvm-static -j $(nproc) ${CROSS_COMPILE}strip lkvm-static   Now, lets build a root FS for our OS   Download busybox:   wget https://busybox.net/downloads/busybox-1.33.1.tar.bz2   Configure buildroot to re-use our toolchain and make:   tar -C . -xvf busybox-1.33.1.tar.bz2 export ARCH=riscv export CROSS_COMPILE=riscv64-unknown-linux-gnu- make -C busybox-1.33.1 defconfig make -C busybox-1.33.1 install   Populate the FS and create an image to boot from:   git clone https://github.com/kvm-riscv/howto.git mkdir -p busybox-1.33.1/_install/etc/init.d mkdir -p busybox-1.33.1/_install/dev mkdir -p busybox-1.33.1/_install/proc mkdir -p busybox-1.33.1/_install/sys mkdir -p busybox-1.33.1/_install/apps ln -sf /sbin/init busybox-1.33.1/_install/init cp -f ./howto/configs/busybox/fstab busybox-1.33.1/_install/etc/fstab cp -f ./howto/configs/busybox/rcS busybox-1.33.1/_install/etc/init.d/rcS cp -f ./howto/configs/busybox/motd busybox-1.33.1/_install/etc/motd cp -f ./kvmtool/lkvm-static busybox-1.33.1/_install/apps cp -f ./build-riscv64/arch/riscv/boot/Image busybox-1.33.1/_install/apps cd busybox-1.33.1/_install; find ./ | cpio -o -H newc &gt; ../../rootfs_kvm_riscv64.img; cd -   Boot the host OS   ./qemu/build/riscv64-softmmu/qemu-system-riscv64 -cpu rv64,x-h=true -M virt -m 512M -nographic -bios opensbi/build/platform/generic/firmware/fw_jump.bin -kernel ./build-riscv64/arch/riscv/boot/Image -initrd ./rootfs_kvm_riscv64.img -append \"root=/dev/ram rw console=ttyS0 earlycon=sbi\"      note the argument -cpu rv64,x-h=true    Then the guest OS over KVM   ./apps/lkvm-static run -m 128 -c2 --console serial -p \"console=ttyS0 earlycon=uart8250,mmio,0x3f8\" -k ./apps/Image --debug   Xvisor on RISC-V   Dependency installation:   sudo apt-get install python genext2fs   clone and build Xvisor   git clone https://github.com/xvisor/xvisor.git cd xvisor CROSS_COMPILE=riscv64-unknown-linux-gnu- make ARCH=riscv generic-64b-defconfig make      Note: I had to apply the patch from https://github.com/xvisor/xvisor/pull/125 to get the compilation to pass.    Build the FW   Firmware will be built as part of the test routines   make -C tests/riscv/virt64/basic   Build Linux to be loaded with Xvisor support   First we will clone and update the configuration using Xvisor scripts   git clone  https://github.com/torvalds/linux.git cp linux/arch/riscv/configs/defconfig linux/arch/riscv/configs/tmp-virt64_defconfig xvisor/tests/common/scripts/update-linux-defconfig.sh -p linux/arch/riscv/configs/tmp-virt64_defconfig -f xvisor/tests/riscv/virt64/linux/linux_extra.config   Then compile Linux   mkdir build-riscv64 export ARCH=riscv export CROSS_COMPILE=riscv64-unknown-linux-gnu- make -C linux O=`pwd`/build-riscv64 tmp-virt64_defconfig make -C linux O=`pwd`/build-riscv64 -j $(nproc) make -C linux O=`pwd`/build-riscv64 ARCH=riscv Image dtbs   Create the FS   Download busybox:   wget https://busybox.net/downloads/busybox-1.33.1.tar.bz2   Configure buildroot to re-use our toolchain and make:   tar -C . -xvf busybox-1.33.1.tar.bz2 export ARCH=riscv export CROSS_COMPILE=riscv64-unknown-linux-gnu- make -C busybox-1.33.1 defconfig make -C busybox-1.33.1 all install -j $(nproc)   Populate the FS and create an image to boot from:   cd busybox-1.33.1/_install; find ./ | cpio -o -H newc &gt; ../../rootfs.img; cd - cd xvisor mkdir -p ./build/disk/tmp mkdir -p ./build/disk/system cp -f ./docs/banner/roman.txt ./build/disk/system/banner.txt cp -f ./docs/logo/xvisor_logo_name.ppm ./build/disk/system/logo.ppm mkdir -p ./build/disk/images/riscv/virt64 dtc -q -I dts -O dtb -o ./build/disk/images/riscv/virt64-guest.dtb ./tests/riscv/virt64/virt64-guest.dts cp -f ./build/tests/riscv/virt64/basic/firmware.bin ./build/disk/images/riscv/virt64/firmware.bin cp -f ./tests/riscv/virt64/linux/nor_flash.list ./build/disk/images/riscv/virt64/nor_flash.list cp -f ./tests/riscv/virt64/linux/cmdlist ./build/disk/images/riscv/virt64/cmdlist cp -f ./tests/riscv/virt64/xscript/one_guest_virt64.xscript ./build/disk/boot.xscript cp -f ~/build-riscv64/arch/riscv/boot/Image ./build/disk/images/riscv/virt64/Image dtc -q -I dts -O dtb -o ./build/disk/images/riscv/virt64/virt64.dtb ./tests/riscv/virt64/linux/virt64.dts cp -f ~/busybox-1.33.1/rootfs.img ./build/disk/images/riscv/virt64/rootfs.img genext2fs -B 1024 -b 32768 -d ./build/disk ./build/disk.img   launch QEMU with Xvisor  ~/qemu/build/qemu-system-riscv64 -cpu rv64,x-h=true -M virt -m 512M -nographic -bios ~/opensbi/build/platform/generic/firmware/fw_jump.bin -kernel ./build/vmm.bin -initrd ./build/disk.img -append 'vmm.bootcmd=\"vfs mount initrd /;vfs run /boot.xscript;vfs cat /system/banner.txt\"'   you now have the xvisor prompt and you can kick start the guest operating systems.   References:   [1] https://people.eecs.berkeley.edu/~krste/papers/EECS-2016-1.pdf   [2] https://arxiv.org/pdf/2103.14951.pdf  ","categories": ["Articles","Tutorial"],
        "tags": ["QEMU","RISC-V","Virtualization"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/exploring_virtualization_in_riscv_machines/",
        "teaser": "https://www.embeddedinn.xyz/images/posts/risc_virt_qemu/image0.png"
      },{
        "title": "Building a barebones web-push server",
        "excerpt":"  Source code for this article is at vppillai/simpleWebPushServer.     Most modern websites offer to “push” a notification to you when there are updates. This is typically enabled using the Notification permission   icon. This article looks at how web push notifications work.      Introduction   Push &amp; notifications are two different operations – a Push and a Notification. During a “push” operation the application server uses a ‘Push API’ to send a notification to a “service-worker” working in the background, in your PC, hosted by your browser. A Notification is an operation where the service worker displays the message coming from the push message on the user’s screen. We will look into the details of these terminologies later. However, an overview of the process is shown in the sequence chart below.    msc {   hscale=\"0.7\", wordwraparcs=1;    user       [linecolor=\"#008800\", textbgcolor=\"#CCFFCC\", arclinecolor=\"#008800\"],   Browser [linecolor=\"#FF0000\", textbgcolor=\"#FFCCCC\", arclinecolor=\"#FF0000\"],   AppServer    [label=\"App Server \", linecolor=\"#0000FF\", textbgcolor=\"#CCCCFF\", arclinecolor=\"#0000FF\"],   PushServer  [label=\"Push Server\",linecolor=\"#FF00FF\", textbgcolor=\"#FFCCFF\", arclinecolor=\"#FF00FF\"];         Browser =&gt;  user [label=\"Allow notifications?\"];   user       =&gt;   Browser [label=\"Allow\"];   Browser =&gt; Browser [label=\"Subscribed; Register Service Worker\"];      Browser =&gt;   AppServer    [label=\"Send Subscription\"];   AppServer =&gt; AppServer [label=\"Store Sub\"];  ---;    AppServer    =&gt;   PushServer[label=\"sub, Notification data\"];   PushServer =&gt; Browser [label=\"Notification data to service worker\"];    ---;   Browser =&gt; Browser [label=\"Service worker shows notification\"] ; }    Voluntary Application Server Identification for Web Push (VAPID)   Since the subscription can be used to send asynchronous notifications to anyone who has subscribed for notifications from a website, it is critical to have a way to establish a secure identity. VAPID is only helpful between the application servers and the push server. In the code below, you can see the VAPID Public Key being passed to registration.pushManager.subscribe(). The full VAPID spec is available here. Key points and excerpts are given below:      There is no basis for an application server to be known to a push service before requesting a push message. Enforcing this places an unwanted constraint on the interactions between user agents and application servers.   VAPID  provides a system whereby an application server can volunteer information about itself to a push service. At a minimum an identity.        Application servers SHOULD generate and maintain a signing key pair usable with elliptic curve digital signature (ECDSA) over the P-256 curve [FIPS186].  Use of this key when sending push messages establishes a continuous identity for the application server.       openssl ecparam -name prime256v1 -genkey -noout -out vapid_private.pem openssl ec -in vapid_private.pem -pubout -out vapid_public.pem                When requesting delivery of to a push server, the application includes a JSON Web Token (JWT) [RFC7519], signed using its signing key. This is as defined in the VAPID spec as a VAPID claim              Note: To know more about JWT, you can refer to my previous article on Java Web Tokens here.                   The VPAID claim can contain multiple items in the body. The 3 main items are given below. But, you can add additional items that are indicative of the application server instance that is triggering the push for additional audit trails and debug:                    sub: The “Subscriber” a mailto link for the administrative contact for this feed. Mainly used by the push server in case there is an issue. E.g: \"sub\": \"mailto:admin@example.com\",           aud: The “Audience” is that indicates the recipient.  Eg. “https://updates.push.services.mozilla.com”           exp: “Expires” this is an integer indicating the date and time that this VAPID header should remain valid until. It is not a validity for the keys. Typically this is the current UTC time + 24 hours.                           Time to get your hands dirty   The full project and setup instructions are available in https://github.com/vppillai/simpleWebPushServer.   codeFlow: Service worker registration   We register the file sw.js as a service worker in index.html Line 12. This file will run in the background and will be invoked when the application server sends a push. In our case, it simply shows a notification using the content of the push.   codeFlow: Creating a subscription   Creating a subscription is straight forward. Take a look at the window.subscribe function  starting at index.html line 34 . In line 42, you can see that the publicVapidKey defined in Line 10 is passed on to registration.pushManager.subscribe(). This is the public key is used to validate the VAPID Claim by the push server. Only the application server has access to the private key corresponding to this key.   codeFlow: Passing the subscription to the application server   In index.html Line 45, we pass this subscription to the CGI post handler in the system. The subscription looks like this:   {     \"endpoint\": \"https://updates.push.services.mozilla.com/wpush/v2/gAAAAABguiX6BAmaIsMsrVpE0qmz19jppZKaYvVIG8I6KVc8zyHHQbncEVgCstSFUMY-cHybm5EJRbdvTvfk1DNjg2vRlD_SqssiUbcLoCbnXG_w0iV4pO1ZlTYo50tT6x7jWttqJ4pIKz90QJq2qQAuJTZZbOQlJJMwFaGeavOvU4Mc8l-OUgM\",     \"keys\": {         \"auth\": \"uTqIX-UBOIznB8SNUbIiPQ\",         \"p256dh\": \"BKWinLri1rAONVNOKCkQVH0aJAALKTQpQtFkFFAFtdyPWN9z5TwUfzljJpSpnjPsJB7OQi00cQry3ZgIRZKeizc\"     } }   The endpoint is unique to this subscription and an authenticated post to the endpoint will result in the data being sent to the service worker.   The test keys were generated with https://vapidkeys.com/   codeFlow: the “application server”   Typical application servers handling webpush are large infrastructures. However, we are using 9 lines of python code to create a local https webserver in this case. An additional 9 lines of code are used to store the subscription and send the 1st notification. We also have an 8 loc helper that can be used to send notifications asynchronously. The code is pretty straightforward. Take a look at server.py , subscription.py &amp; sendNotification.py   Since this is a barebones system, we are not going to setup user identification or even multi user support. We just use the latest subscription in a text file and use it to trigger notifications.  ","categories": ["Articles","Tutorial"],
        "tags": ["webpush","PWA","webdev"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/Building-a-barebones-web-push-server/",
        "teaser": "https://www.embeddedinn.xyz/images/posts/webpush/image2.png"
      },{
        "title": "System Hardware Partitioning Using Device Trees in a Virtualized System",
        "excerpt":"  If you have brought up Linux on an embedded target (or loosely, any non-PC target), you would have at least heard of the term Device Tree. Device trees have been around for a while and have changed over time. This article looks at how device trees can be leveraged to create sliced systems in a virtualized, embedded system.   I strongly suggest that you read through Motivation for virtualization in embedded systems before proceeding. This will help you better appreciate what we are doing in this article and why it is relevant.   For the hand full of followers of articles I put up here in embeddedinn, you might recall that we did some tweaking of the QEMU device tree representation while building the RISC-V butter robot. The initial part of this article expands on the concepts we covered there.   A quick and dirty introduction to device trees.   The fundamental reason that brought about device trees into the Linux world is the need to avoid creating create new system images for boards where the only difference is the addition/removal of a couple of peripherals that has nothing to do with how the kernel operates. These peripherals might be external to the SoC or even internal to the system. For instance, two SoCs of the same device family, one with 2 I2Cs and one with no I2Cs, should boot the same image, compiled once. (This is under the assumption that the I2C is not handling boot-critical functions). But, to make this a reality, we need a standard mechanism by which the bootloader can pass info about the mapping of peripherals to the kernel at boot. Device trees are here to help you with this.   A device tree starts its life as a Device Tree Source (DTS). This is compiled into a flattened device tree, a.k.a a Device Tree Blob (DTB) using the Device Tree Compiler (DTC). Once a DTB is generated the bootloader can pass it on to the kernel at boot. In an embedded system, DTBs are typically stored in the non-volatile memory where bootloaders are stored. Bootloaders like Uboot will read the DTB and feed it to the kernel along with other boot parameters.   Now, what and how exactly is the HW described in the DTS? The general structure of a DTS entry is shown in the image below.                                               (source: Device tree for dummies)         And, this is how an actual entry would look like:                                               (source: Device tree for dummies)         Device trees in a virtualized system   In the case of a Virtualized system, we can pass different device trees to the guest OSes so that they have access to just a subset of the actual peripherals in the system. To see this in action , here is what we will do:           Create a virtualized system using Xvisor on Qemu. This time, we will use Arm A9 emulation.            Pass a full set of peripherals to XVisor. We will modify the default DTS of the vExpress emulation target to include 2 additional UART peripherals to see this in action.              It is not mandatory that we pass the full set of peripherals to the hypervisor since we can pass peripherals not visible even to the hypervisor to the guest OSes. This is ideal for embedded systems that partition a larger SoC into individual discrete “Virtual machines”.                Pass a subset (1 UART less) to the guest OS.       The steps to build the system components from source is given in the appendix section of this article. We will look at the modifications and use the results here.   The DTS we modify comes form the xvisor source tree. The file is arch/arm/board/generic/dts/arm/vexpress-v2p-ca9.dts. The modifications I did to introduce two additional UART instances are highlighted in the image below:                                               Adding extra peripherals to the system         This new dts can now be compiled and used to boot the qemu instance with the Xvisor image. Once xvisor is up, you can pass the chardev list command to see the enumerated devices.                                               Listing the new peripherals in the hypervisor         The DTS passed to the guest OS is at xvisor/tests/arm32/vexpress-a9/vexpress-a9-guest.dts of the Xvisor source tree. The alias is referred to from linux/arch/arm/boot/dts/vexpress-v2p-ca9.dts in the Linux source tree. You can see the three UARTs and an additional virtual UART enumerated by the Linux class layer. (There are some virtualized devices since we are using QEMU based emulation. However, since we are doing this to validate our conceptual understanding, we will have some leeway with what we see here).                                               OS listing our new peripheral         If we remove the reference just from the xvisor tree, it will result in a kernel panic since the DTB we compiled into the kernel is still referring to it.                                               Kernel panic         Once we remove the reference form the kernel DTS as well, we can see that the peripheral that is available in the platform, and visible by the Hypervisor and potentially even other guest OSes is not enumerated by the kernel in this case.                                               Fixing the internal reference         Security concerns.   Securing access to portions of the hardware is the next item to consider. However, it is a very vast topic that needs focused analysis since there are lots of variables involved. The platform capabilities and how the ring levels are configured in the SoC plays a huge role in physically blocking a guest from accessing peripherals. We will plan this in another article and focus on a specific platform while analyzing it.   Sharing Hardware among guests   Considering a simple trust based system, if a Guest wants to access a peripheral that is mapped into another guest’s device tree, we need to build a client server based message posting infrastructure for one guest to request operations to another. This again is a topic for another article.   Overall the DTS based approach is a bit messy since the kernel is flooded with too many device trees. There are also some ongoing initiatives to improve the state of affairs.                                               too many device trees         Appendix A: Build Steps   1. Setting up the development environment   Install dependencies with:    sudo apt install autoconf automake autotools-dev curl libmpc-dev libmpfr-dev libgmp-dev \\                  libusb-1.0-0-dev gawk build-essential bison flex texinfo  libtool\\                  patchutils bc zlib1g-dev device-tree-compiler pkg-config libexpat-dev\\                  libncurses5-dev libncursesw5-dev git gcc-multilib git-all  iasl cgdb xorriso\\                  libncurses5-dev m4 flex bison autoconf expect qemu-system-x86 qemu-utils\\                  qemu-system-arm qemu-user libssl-dev bc python genext2fs -y \t\t\t\t  sudo apt-get install  install gcc-arm-linux-gnueabi   2. Compile Xvisor for the ARM Qemu Target.   At the time of writing this article, Xvisor is in version v0.3.0.   Clone it with :   git clone https://github.com/xvisor/xvisor.git   This version is not compatible out of the box with the latest GCC ARM compiler that we are using. So we need to apply a patch to make it compile.   cd xvisor wget https://github.com/xvisor/xvisor/pull/125.patch git apply 125.patch   Now, compile it with:   export CROSS_COMPILE=arm-linux-gnueabi- make ARCH=arm generic-v7-defconfig make -j  $(nproc)   We also need to build the tests since that gives us the required scripts and patches required for the rest of the operations   make -C tests/arm32/vexpress-a9/basic   3. Compiling the Guest OS (Linux)   To compile the Linux kernel that can run on Qemu Xvisor, we need to make a bit of changes to the default source code and the generated images. XVisor provides the scripts required to do this. However, compatibility of these tools are flakey at best beyond Linux kernel version 5.4. So, we will use v5.4 for the time being.   Clone the kernel with:   git clone https://github.com/torvalds/linux.git cd linux git checkout v5.4   We then use the tooling provided by Xvisor to update the configurations and apply some patches to get the source tree ready for compilation.   cd linux sed -i 's/0xff800000UL/0xff000000UL/' arch/arm/include/asm/pgtable.h cp arch/arm/configs/vexpress_defconfig arch/arm/configs/tmp-vexpress-a9_defconfig ../xvisor/tests/common/scripts/update-linux-defconfig.sh -p arch/arm/configs/tmp-vexpress-a9_defconfig -f ../xvisor/tests/arm32/vexpress-a9/linux/linux_extra.config   Now, make a build with the modified configuration.   make O=../linBuild ARCH=arm tmp-vexpress-a9_defconfig   make O=../linBuild/ ARCH=arm Image dtbs -j $(nproc)   Once compilation is done, we need to patch the kernel image to replace sensitive non-privileged instructions and then repack the image.   export CROSS_COMPILE=arm-linux-gnueabi- ../xvisor/arch/arm/cpu/arm32/elf2cpatch.py -f ../linBuild/vmlinux | ../xvisor/build/tools/cpatch/cpatch32 ../linBuild/vmlinux 0 ${CROSS_COMPILE}objcopy -O binary ../linBuild/vmlinux ../linBuild/arch/arm/boot/Image   4. Create a root FS with BusyBox   Download and un-compress the busybox source.   wget https://busybox.net/downloads/busybox-1.33.1.tar.bz2 tar xvf busybox-1.33.1.tar.bz2   Xvisor provides the BusyBox defconfig for Qemu emulation. However, this is provided for the 1.31.1 version of BusyBox. This version is not compatible with the latest GlibC since the stime function was deprecated since glibc 2.31. This has since been fixed in BusyBox. So, we will use the latest 1.33.1 version of BusyBox. But, the configuration for this is not part of xvisor. I have raised a PR for it. So, for now we will apply it as a patch.   cd xvisor wget https://github.com/xvisor/xvisor/pull/137.patch git apply 137.patch cp tests/common/busybox/busybox-1.33.1_defconfig ../busybox-1.33.1/.config cd ../busybox-1.33.1/ export CROSS_COMPILE=arm-linux-gnueabi- make oldconfig make install -j $(nproc)   Once BusyBox is compiled, we will populate additional items and the device tree components into the filesystem and package it.   mkdir -p ./_install/etc/init.d mkdir -p ./_install/dev mkdir -p ./_install/proc mkdir -p ./_install/sys ln -sf /sbin/init ./_install/init cp -f ../xvisor/tests/common/busybox/fstab ./_install/etc/fstab cp -f ../xvisor/tests/common/busybox/rcS ./_install/etc/init.d/rcS cp -f ../xvisor/tests/common/busybox/motd ./_install/etc/motd cp -f ../xvisor/tests/common/busybox/logo_linux_clut224.ppm ./_install/etc/logo_linux_clut224.ppm cp -f ../xvisor/tests/common/busybox/logo_linux_vga16.ppm ./_install/etc/logo_linux_vga16.ppm  cd ./_install; find ./ | cpio -o -H newc &gt; ../rootfs.img; cd -   5. Create a disk image for QEMU with Xvisor and the guest OS images.   We will not package the Xvisor image, Guest OS and the Guest rootFS into a disk image that QEMU can use.   cd xvisor  mkdir -p ./build/disk/tmp mkdir -p ./build/disk/system cp -f ./docs/banner/roman.txt ./build/disk/system/banner.txt cp -f ./docs/logo/xvisor_logo_name.ppm ./build/disk/system/logo.ppm mkdir -p ./build/disk/images/arm32/vexpress-a9 dtc -q -I dts -O dtb -o ./build/disk/images/arm32/vexpress-a9-guest.dtb ./tests/arm32/vexpress-a9/vexpress-a9-guest.dts cp -f ./build/tests/arm32/vexpress-a9/basic/firmware.bin.patched ./build/disk/images/arm32/vexpress-a9/firmware.bin cp -f ./tests/arm32/vexpress-a9/linux/nor_flash.list ./build/disk/images/arm32/vexpress-a9/nor_flash.list cp -f ./tests/arm32/vexpress-a9/linux/cmdlist ./build/disk/images/arm32/vexpress-a9/cmdlist cp -f ./tests/arm32/vexpress-a9/xscript/one_guest_vexpress-a9.xscript ./build/disk/boot.xscript cp -f ../linBuild/arch/arm/boot/Image ./build/disk/images/arm32/vexpress-a9/Image cp -f ../linBuild/arch/arm/boot/dts/vexpress-v2p-ca9.dtb ./build/disk/images/arm32/vexpress-a9/vexpress-v2p-ca9.dtb cp -f ../busybox-1.33.1/rootfs.img ./build/disk/images/arm32/vexpress-a9/rootfs.img genext2fs -B 1024 -b 32768 -d ./build/disk ./build/disk.img   6. Boot   Boot into Xvisor with   qemu-system-arm -M vexpress-a9 -m 512M -display none -serial stdio -kernel build/vmm.bin -dtb build/arch/arm/board/generic/dts/arm/vexpress-v2p-ca9.dtb -initrd build/disk.img   Boot the guest OS with   guest kick guest0 vserial bind guest0/uart0 autoexec  ","categories": ["Articles","Tutorial"],
        "tags": ["QEMU","ARM","DTS","Virtualization"],
        "url": "https://www.embeddedinn.xyz/articles/tutorial/System_Partitioning_Using_Device_Trees_in_a_Virtualized_System/",
        "teaser": "https://www.embeddedinn.xyz/images/posts/dts_virt/image0.png"
      }]
